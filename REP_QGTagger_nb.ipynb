{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QG-tagger using REP\n",
    "\n",
    "# Folding Strategy\n",
    "REP implements folding strategy as one more metaestimator.\n",
    "\n",
    "When we don't have enough data to split data on train/test, we're stick to k-folding cross-validation scheme. Folding becomes the only way when you use some multi-staged stacking algorithm.\n",
    "\n",
    "Usually we split training data into folds manually, but this is annoying and not reliable. REP has FoldingClassifier and FoldingRegressor, which do this automatically.\n",
    "\n",
    "- https://github.com/yandex/rep/blob/master/howto/04-howto-folding.ipynb\n",
    "- https://github.com/yandex/rep/blob/master/howto/03-howto-gridsearch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import numpy\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from rep.utils import train_test_split\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import from root_pandas library\n",
    "from root_pandas import read_root\n",
    "\n",
    "print sklearn.__version__\n",
    "print pandas.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Upload Monte Carlo Data Set from DESY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Data loading function\n",
    "\n",
    "def load(sig_filename, bkg_filename, category, features):\n",
    "    \"\"\"load fucntion.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sig_filename : array, shape = [n_samples]\n",
    "            true class, intergers in [0, n_classes - 1)\n",
    "    bkg_filename : array, shape = [n_samples, n_classes]\n",
    "    category: string\n",
    "    features: array, shape = [n_features]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Read in ROOT file and produce panda dataframes\n",
    "    signal = read_root([sig_filename], category, \n",
    "                       columns=features+['noexpand:pt_dr_log/pt'], \n",
    "                       where= 'partonId < 4 && axis2 < 8')\n",
    "    #'partonId!=21 && axis2 < 8 && jetIdLevel==3 && matchedJet==1 && nGenJetsInCone==1 && nGenJetsForGenParticle==1 && nJetsForGenParticle==1 && partonId < 4 && balanced==1 && charged_multiplicity >= 3 && charged_multiplicity <= 143'\n",
    "    signal['y']= 1 # add target column for signal\n",
    "\n",
    "    background = read_root([bkg_filename], category, \n",
    "                           columns=features+['noexpand:pt_dr_log/pt'],\n",
    "                           where='partonId==21 && axis2 < 8')\n",
    "    #'partonId==21 && axis2 < 8 && jetIdLevel==3 && matchedJet==1 && nGenJetsInCone==1 && nGenJetsForGenParticle==1 && nJetsForGenParticle==1 && balanced==1 && charged_multiplicity >= 3 && charged_multiplicity <= 143'\n",
    "    background['y']= 0 # add target column for background\n",
    "\n",
    "    data = pd.concat([signal, background])\n",
    "    \n",
    "    return data                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Load input data files\n",
    "\n",
    "# Feature names\n",
    "branch_names = \"\"\"axis1,axis2,ptD,charged_multiplicity,pt,pt_dr_log,partonId,jetIdLevel,matchedJet,nGenJetsInCone,nGenJetsForGenParticle,nJetsForGenParticle,balanced,weight\"\"\".split(\",\")\n",
    "\n",
    "features = [c.strip() for c in branch_names]\n",
    "features = (b.replace(\" \", \"_\") for b in features)\n",
    "features = list(b.replace(\"-\", \"_\") for b in features)\n",
    "\n",
    "# Delcare dataset location\n",
    "signal_sample     = \"QGtagger_training/pt_bin10_eta_bin1.root\"\n",
    "background_sample = \"QGtagger_training/pt_bin10_eta_bin1.root\"\n",
    "tree_category = \"tree\"\n",
    "\n",
    "# Load the data to panda dataframe object\n",
    "data = load(signal_sample, background_sample, tree_category, features)\n",
    "\n",
    "print \"Total number of events: {}\\nNumber of features: {}\".format(len(data.index), len(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "labels = data.y\n",
    "print data.columns\n",
    "variables = list(data.drop(['pt','partonId','jetIdLevel','matchedJet',\n",
    "                           'nGenJetsInCone','nGenJetsForGenParticle',\n",
    "                           'nJetsForGenParticle','nJetsForGenParticle',\n",
    "                           'pt_dr_log','balanced','y'], axis=1, inplace=False).columns)\n",
    "print \"Candidate features\", variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#train_data, test_data, train_labels, test_labels = train_test_split(data.drop([\"weight\",\"y\"], axis=1, inplace=False), labels, train_size=0.5)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data[variables], labels, train_size=0.5)\n",
    "print train_data.shape\n",
    "print test_data.shape\n",
    "print train_labels.shape\n",
    "print test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folding strategy\n",
    "\n",
    "FoldingClassifier implements the same interface as all classifiers, but with some difference:\n",
    "- prediction methods have additional parameter \"vote_function\" (example folder.predict(X, vote_function=None)), which is used to combine all classifiers' predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "from rep.estimators import SklearnClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from rep.metaml import FoldingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define folding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_folds = 4\n",
    "folder = FoldingClassifier(GradientBoostingClassifier(n_estimators=30), \n",
    "                           n_folds=n_folds, features=filter(lambda feature: feature!='weight', variables),\n",
    "                           parallel_profile='threads-4')\n",
    "\n",
    "folder.fit(train_data.drop([\"weight\"], axis=1, inplace=False), train_labels, sample_weight=train_data.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default prediction (predict ith fold by ith classifier)\n",
    "In this case each sample will be predict by estimator that was not using this particular sample in training.\n",
    "\n",
    "When you apply this prediction to some new data (not the same was passed in training), it will predict each sample by random estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "prob = folder.predict_proba(train_data) \n",
    "print 'ROC AUC:', roc_auc_score(train_labels, prob[:, 1], sample_weight=train_data.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "prob = folder.predict_proba(test_data)\n",
    "print 'ROC AUC:', roc_auc_score(test_labels, prob[:, 1], sample_weight=test_data.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting prediction \n",
    "(predict ith fold by all classifiers and take value, which is calculated by vote_function)\n",
    "It makes sense to use all classifier to predict new data, because averaging makes predictions more stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# definition of mean function, which combines all predictions\n",
    "def mean_vote(x):\n",
    "    return numpy.mean(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prob = folder.predict_proba(test_data, vote_function=mean_vote)\n",
    "print 'ROC AUC:', roc_auc_score(test_labels, prob[:, 1], sample_weight=test_data.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of folds\n",
    "Again use ClassificationReport class to compare different results. For folding classifier this report uses only default prediction.\n",
    "\n",
    "## Report training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "from rep.data.storage import LabeledDataStorage\n",
    "from rep.report import ClassificationReport\n",
    "\n",
    "# add folds_column to dataset to use mask\n",
    "train_data[\"FOLDS\"] = folder._get_folds_column(len(train_data))\n",
    "lds_train = LabeledDataStorage(data=train_data.drop([\"weight\"], axis=1, inplace=False), \n",
    "                               target=train_labels, sample_weight=train_data.weight)\n",
    "\n",
    "report = ClassificationReport({'folding': folder}, lds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal distribution for each fold\n",
    "\n",
    "Use mask parameter to plot distribution for the specific fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "%pylab inline\n",
    "for fold_num in range(n_folds):\n",
    "    report.prediction_pdf(mask=\"FOLDS == %d\" % fold_num, labels_dict={1: 'sig fold %d' % fold_num}).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background distribution for each fold¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "%pylab inline\n",
    "for fold_num in range(n_folds):\n",
    "    report.prediction_pdf(mask=\"FOLDS == %d\" % fold_num, labels_dict={0: 'bck fold %d' % fold_num}).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROCs (each fold used as test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "%pylab inline\n",
    "for fold_num in range(n_folds):\n",
    "    report.roc(mask=\"FOLDS == %d\" % fold_num).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report for test dataset\n",
    "*NOTE*: Here vote function is None, so default prediction is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# add folds_column to dataset to use mask\n",
    "lds_test = LabeledDataStorage(data=test_data.drop([\"weight\"], axis=1, inplace=False), target=test_labels, sample_weight=test_data.weight)\n",
    "\n",
    "report = ClassificationReport({'folding': folder}, lds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "%pylab inline\n",
    "report.prediction_pdf().plot(new_plot=True, figsize = (9, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Scatter plot\n",
    "scatter(data.axis2, data.ptD, alpha = 0.1, color='b', label='signal')\n",
    "scatter(data.axis2, data.ptD, alpha = 0.01, color='r', label='background')\n",
    "xlabel('pt_dr_log/pt', fontsize=16)\n",
    "ylabel('ptD', fontsize=16)\n",
    "title('Correlation plot', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "report.roc().plot(xlim=(0., 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "from rep.report.metrics import RocAuc\n",
    "from rep.metaml import GridOptimalSearchCV, FoldingScorer, RandomParameterOptimizer\n",
    "from rep.estimators import SklearnClassifier, TMVAClassifier, XGBoostRegressor\n",
    "print TMVAClassifier.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# define grid parameters\n",
    "grid_param = {}\n",
    "grid_param['learning_rate'] = [0.2, 0.1, 0.05, 0.02, 0.01]\n",
    "grid_param['max_depth'] = [2, 3, 4]\n",
    "grid_param['n_estimators'] = [100, 200]\n",
    "\n",
    "# use random hyperparameter optimization algorithm \n",
    "generator = RandomParameterOptimizer(grid_param, n_evaluations=10)\n",
    "\n",
    "# define folding scorer\n",
    "scorer = FoldingScorer(RocAuc(), folds=3, fold_checks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "estimator = SklearnClassifier(GradientBoostingClassifier(n_estimators=30))\n",
    "grid_finder = GridOptimalSearchCV(estimator, generator, scorer, parallel_profile='threads-4')\n",
    "grid_finder.fit(data[filter(lambda feature: feature!='weight', variables)], labels, sample_weight=data['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "grid_finder.params_generator.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the parameters and threshold\n",
    "\n",
    "In many applications we need to optimize some binary metrics for classification (f1, BER, misclassification error), in which case we need each time after training classifier to find optimal threshold on predicted probabilities (default one is usually bad).\n",
    "\n",
    "In this example:\n",
    "- We are optimizing AMS (binary metric, that was used in Higgs competition at kaggle)\n",
    "- Tuning parameters of TMVA's GBDT\n",
    "- Using Gaussian Processes to make good guesses about next points to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "from rep.metaml import RegressionParameterOptimizer\n",
    "from sklearn.gaussian_process import GaussianProcess, GaussianProcessRegressor\n",
    "from rep.report.metrics import OptimalMetric, OptimalAccuracy, OptimalAMS, OptimalSignificance, ams, OptimalSignificance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# OptimalMetrics is a wrapper which is able to check all possible thresholds\n",
    "# expected number of signal and background events are taken as some arbitrary numbers\n",
    "optimal_ams = OptimalMetric(ams, expected_s=100, expected_b=1000)\n",
    "\n",
    "# define grid parameters\n",
    "grid_param = OrderedDict(\n",
    "    {'NTrees': [1],#[1000], [5, 10, 15, 20, 25], \n",
    "     'MinNodeSize' : [2.5],\n",
    "     'Shrinkage': [0.20], #[0.4, 0.2, 0.1, 0.05, 0.02, 0.01], \n",
    "     'UseBaggedBoost:BaggedSampleFraction': [0.5],\n",
    "     'nCuts': [20],\n",
    "     'MaxDepth': [2],\n",
    "     # you can pass different sets of features to be compared\n",
    "     'features': [variables[:1], variables[:2]]\n",
    "     #'features': [filter(lambda feature: feature!='weight', variables), variables[:1]]\n",
    "     #'features': [variables[:2], variables[:3], variables[:4]],\n",
    "    }\n",
    ")\n",
    "\n",
    "# using GaussianProcesses \n",
    "generator = RegressionParameterOptimizer(grid_param, n_evaluations=1, \n",
    "                                         regressor=GaussianProcessRegressor(), \n",
    "                                         n_attempts=1)\n",
    "\n",
    "# define folding scorer\n",
    "scorer = FoldingScorer(optimal_ams, folds=2, fold_checks=1)\n",
    "#\"Silent=True:V=False:DrawProgressBar=False\"\n",
    "#grid_finder = GridOptimalSearchCV(TMVAClassifier(method='kBDT', BoostType='Grad', \n",
    "#                                                 factory_options=\"!V:!Silent:Color:DrawProgressBar:Transformations=I;D;P;G,D:AnalysisType=Classification\"),\n",
    "grid_finder = GridOptimalSearchCV(TMVAClassifier(method='kBDT', BoostType='Grad', \n",
    "                                                 factory_options=\"!V=False:!Silent=True:DrawProgressBar=False:AnalysisType=Classification\"),\n",
    "                                                                    \n",
    "                                  generator, scorer, parallel_profile='threads-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_finder.fit(data, labels, sample_weight=data.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_finder.fit(data[filter(lambda feature: feature!='weight', variables)], labels, sample_weight=data.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_finder.generator.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see dynamics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plot(grid_finder.generator.grid_scores_.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing complex models + using custom scorer\n",
    "REP supports sklearn-way of combining classifiers and getting/setting their parameters.\n",
    "So you can tune complex models using the same approach.\n",
    "\n",
    "Let's optimize\n",
    "- BaggingRegressor over XGBoost regressor, we will select appropriate parameters for both\n",
    "- we will roll new scorer, which test everything on special part of dataset\n",
    "- we use the same data, which will be once split into train and test (this scenario of testing is sometimes needed)\n",
    "- optimizing MAE (mean absolute error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from rep.estimators import XGBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "class MyMAEScorer(object):\n",
    "    def __init__(self, test_data, test_labels):\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        \n",
    "    def __call__(self, base_estimator, params, X, y, sample_weight=None):\n",
    "        cl = clone(base_estimator)\n",
    "        cl.set_params(**params)\n",
    "        cl.fit(X, y)\n",
    "        # Returning with minus, because we maximize metric\n",
    "        return -mean_absolute_error(self.test_labels, cl.predict(self.test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define grid parameters\n",
    "grid_param = OrderedDict(\n",
    "    {\n",
    "    # parameters of sklearn Bagging\n",
    "    'n_estimators': [1, 2], #[1, 3, 5, 7],\n",
    "    'max_samples': [0.1], #[0.2, 0.4, 0.6, 0.8],\n",
    "    # parameters of base (XGBoost)\n",
    "    'base_estimator__n_estimators': [1, 2], #[10, 20, 40], \n",
    "    'base_estimator__eta': [0.1] #[0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "    }\n",
    ")\n",
    "\n",
    "# using Gaussian Processes \n",
    "generator = RegressionParameterOptimizer(grid_param, n_evaluations=4, \n",
    "                                         regressor=GaussianProcessRegressor(), \n",
    "                                         n_attempts=10)\n",
    "\n",
    "estimator = BaggingRegressor(XGBoostRegressor(), n_estimators=10)\n",
    "\n",
    "scorer = MyMAEScorer(test_data, test_labels)\n",
    "##scorer(sample_weight=data[\"weight\"]) #does not work!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_finder = GridOptimalSearchCV(estimator, generator, scorer, parallel_profile='threads-4')\n",
    "grid_finder.fit(data[filter(lambda feature: feature!='weight', variables)], labels, sample_weight=data['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "grid_finder.generator.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following two secions below are still in developement. Neeed to figure out if ipyton can handle threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "\n",
    "## Standard nested k-fold cross-validation\n",
    "def nestedGridSearchCV(Classifier,\n",
    "                       generator,\n",
    "                       X, y,\n",
    "                       outer_cv, \n",
    "                       param_grid, \n",
    "                       scorer, \n",
    "                       parallel_profile):\n",
    "#def nestedGridSearchCV(Classifier, X, y, outer_cv, inner_cv, \n",
    "#                       parameter_grid, scoring=\"accuracy\"):\n",
    "    \"\"\"Nested k-fold crossvalidation.\"\"\"\n",
    "    \n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    ----------\n",
    "    Classifier : array, shape = [n_samples]\n",
    "            true class, intergers in [0, n_classes - 1)\n",
    "    X : array, shape = [n_samples, n_classes]\n",
    "    y : array, shape = [n_samples, n_classes]\n",
    "    outer_cv:  shape = [n_samples, n_classes]\n",
    "    inner_cv:  shape = [n_samples, n_classes]\n",
    "    parameter_grid: shape = [n_samples, n_classes]\n",
    "    scoring:   shape = [n_samples, n_classes]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Grid classifier: classifier re-fitted to full dataset\n",
    "    \"\"\"    \n",
    "    \n",
    "    \n",
    "    outer_scores = []\n",
    "    \n",
    "    for training_samples, test_samples in outer_cv.split(X, y):\n",
    "\n",
    "        # Training datasets\n",
    "        x_training_temp = pd.DataFrame(X.iloc[training_samples], columns=features)\n",
    "\n",
    "        x_training = x_training_temp.drop('weight', axis=1, inplace=False)\n",
    "        y_training = pd.Series(y.iloc[training_samples])\n",
    "\n",
    "        # Extract sample weight\n",
    "        weights_training = x_training_temp[\"weight\"].values\n",
    "\n",
    "        # Testing datasets\n",
    "        x_testing_temp = pd.DataFrame(X.iloc[test_samples], columns=features)\n",
    "\n",
    "        x_testing = x_testing_temp.drop('weight', axis=1, inplace=False)\n",
    "        y_testing = pd.Series(y.iloc[test_samples])\n",
    "\n",
    "        # set up grid search configuration\n",
    "        #cv = GridSearchCV(estimator=Classifier, param_grid=param_grid,\n",
    "        #                  cv=inner_cv, scoring=\"accuracy\", \n",
    "        #                  n_jobs=-1,\n",
    "        #                  fit_params={\"classifier__sample_weight\": weights_training})\n",
    "        cv = GridOptimalSearchCV(Classifier, generator, scorer, parallel_profile=None)\n",
    "                         \n",
    "        # train on the training set\n",
    "        cv.fit(x_training, y_training)\n",
    "        \n",
    "        # evaluate\n",
    "        #outer_scores.append(cv.score(x_testing, y_testing))\n",
    "        print cv.generator.grid_scores_.values()\n",
    "        outer_scores.append(cv.generator.grid_scores_.values())\n",
    "\n",
    "    # Print final model evaluation (i.e. mean cross-validation scores)\n",
    "    #print \"Final model evaluation (mean cross-val scores):\\n\", np.array(outer_scores).mean()\n",
    "    \n",
    "    # note: the scoring is being done without the weights associated with X\n",
    "    # fit model to entire training dataset (i.e tuning & validation dataset)\n",
    "    #cv.best_estimator_.fit(X.drop('weight', axis=1, inplace=False), y,\n",
    "    #                       **{\"classifier__sample_weight\": X[\"weight\"].values})\n",
    "    \n",
    "    \n",
    "    #return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels)\n",
    "\n",
    "grid_param = {\n",
    "    # parameters of sklearn Bagging\n",
    "    'n_estimators': [1, 2], \n",
    "    'max_samples': [0.1],\n",
    "    # parameters of base (XGBoost)\n",
    "    'base_estimator__n_estimators': [1, 2], \n",
    "    'base_estimator__eta': [0.1]\n",
    "}\n",
    "\n",
    "k_fold=5\n",
    "outer_kfold_cv = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "\n",
    "# using Gaussian Processes \n",
    "generator = RegressionParameterOptimizer(grid_param, n_evaluations=4, \n",
    "                                         regressor=GaussianProcessRegressor(), \n",
    "                                         n_attempts=10)\n",
    "\n",
    "estimator = BaggingRegressor(XGBoostRegressor(), n_estimators=10)\n",
    "\n",
    "scorer = MyMAEScorer(test_data, test_labels)\n",
    "##scorer(sample_weight=data[\"globalTimesEventWeight\"]) #does not work!!\n",
    "\n",
    "grid_finder = nestedGridSearchCV(Classifier=estimator,\n",
    "                                 generator=generator,\n",
    "                                 X=train_data, y=train_labels,\n",
    "                                 outer_cv=outer_kfold_cv, \n",
    "                                 param_grid=grid_param, \n",
    "                                 scorer=scorer, \n",
    "                                 parallel_profile='threads-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-ML: Factories\n",
    "\n",
    "- http://www.programcreek.com/python/example/86675/sklearn.metrics.roc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from rep.metaml import ClassifiersFactory\n",
    "from rep.estimators import TMVAClassifier, SklearnClassifier, XGBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "grid_param = OrderedDict({\"MaxDepth\": [4, 5], \"NTrees\": [10, 20]})\n",
    "\n",
    "# Define classifiers\n",
    "factory = ClassifiersFactory()\n",
    "\n",
    "#There are different ways to add classifiers to Factory\n",
    "factory.add_classifier('tmva', TMVAClassifier(NTrees=15, features=filter(lambda feature: feature!='weight', variables), Shrinkage=0.1, factory_options=\"Silent=True:V=False:DrawProgressBar=False\"))\n",
    "factory.add_classifier('ada', AdaBoostClassifier(n_estimators=10))\n",
    "\n",
    "#tmva = TMVAClassifier(method='kBDT', NTrees=15, Shrinkage=0.1, nCuts=-1, BoostType='Grad', features=filter(lambda feature: feature!='weight', variables))\n",
    "#ada = AdaBoostClassifier(n_estimators=100)\n",
    "#factory.add_classifier('tmva', tmva) \n",
    "#factory.add_classifier('ada', ada)\n",
    "\n",
    "factory['xgb'] = XGBoostClassifier(features=filter(lambda feature: feature!='weight', variables)) # training\n",
    "\n",
    "#factory.fit(train_data, train_labels, features=variables, parallel_profile='IPC')\n",
    "factory.fit(train_data, train_labels, features=filter(lambda feature: feature!='weight', variables), parallel_profile='threads-4')\n",
    "\n",
    "# predict\n",
    "#factory.predict_proba(some_data, parallel_profile='IPC')\n",
    "factory.predict_proba(test_data, parallel_profile='threads-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric functions\n",
    "def significance(s, b):\n",
    "    br = 0.01\n",
    "    radicand = s/numpy.sqrt(b+br)\n",
    "    return radicand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rep.report.metrics import significance\n",
    "metrics = report.metrics_vs_cut(significance, metric_label='significance')\n",
    "metrics.plot(new_plot=True, figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user define metric\n",
    "def AMS(s,b):\n",
    "    b_reg = 0.01\n",
    "    radicand = 2*((s+b+b_reg) * numpy.log(1.0 + s/(b+b_reg)) -s)\n",
    "    return numpy.sqrt(radicand)\n",
    "\n",
    "metrics =report.metrics_vs_cut(AMS, metric_label='ams')\n",
    "metrics.plot(new_plot=True, figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = factory.test_on(test_data, test_labels) \n",
    "learning_curve = report.learning_curve(RocAuc(), metric_label='ROC AUC', steps=10) \n",
    "learning_curve.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot data information: features correlation matrix\n",
    "report.features_correlation_matrix_by_class(features=filter(lambda feature: feature!='weight', variables)).plot(new_plot=True, show_legend=False, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions for each feature\n",
    "# use just common features for all classifiers\n",
    "report.features_pdf().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "report.feature_importance().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features importance using shuffling method \n",
    "# (apply random permutation to one particular column)\n",
    "report.feature_importance_shuffling().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = factory.test_on(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc curves\n",
    "report.roc().plot(xlim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "estimator = Pipeline([('feature_scaling', None), \n",
    "                 ('feature_selection', None), \n",
    "                 ('classifier', DummyClassifier())]\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# feature selection\n",
    "select = VarianceThreshold()\n",
    "\n",
    "# create classifier for use in scikit-learn\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# preprocessing using 0-1 scaling by removing the mean and scaling to unit variance \n",
    "scaler = RobustScaler()\n",
    "\n",
    "# prepare models: create a mapping of ML classifier name to algorithm\n",
    "param_grid = [\n",
    "    {'classifier': [model],\n",
    "     'classifier__n_estimators': [10, 20, 30, 40],\n",
    "     'classifier__max_depth': [3, 4, 5],\n",
    "     'classifier__learning_rate': [0.01, 0.1, 0.4],\n",
    "     'feature_selection': [select],\n",
    "     'feature_selection__threshold': [0.5],\n",
    "     'feature_scaling': [scaler]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "print estimator.get_params().keys()\n",
    "\n",
    "# use random hyperparameter optimization algorithm \n",
    "generator = RandomParameterOptimizer(grid_param, n_evaluations=3)\n",
    "\n",
    "# define folding scorer\n",
    "scorer = FoldingScorer(RocAuc(), folds=3, fold_checks=3)\n",
    "\n",
    "grid_finder = GridOptimalSearchCV(estimator, generator, scorer, parallel_profile='threads-4')\n",
    "grid_finder.fit(data[filter(lambda feature: feature!='weight', variables)], labels, sample_weight=data.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Grid search in REP extends sklearn grid search, uses optimization techniques to avoid complete search of estimator parameters.\n",
    "\n",
    "REP has predefined scorers, metric functions, optimization techniques. Each component is replaceable and you can optimize complex models and pipelines (Folders/Bagging/Boosting and so on).\n",
    "\n",
    "## Structure together\n",
    "- ParameterOptimizer is responsible for generating new set of parameters which will be checked\n",
    " - RandomParameterOptimizer\n",
    " - AnnealingParameterOptimizer\n",
    " - SubgridParameterOptimizer\n",
    " - RegressionParameterOptimizer (this one can use any regression model, like GaussianProcesses)\n",
    "- Scorer is responsible for training and evaluating metrics\n",
    " - Folding scorer (uses metrics with REP interface), uses averaging quality after kFolding\n",
    "- GridOptimalSearchCV makes all of this work together and sends tasks to cluster or separate threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
