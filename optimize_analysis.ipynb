{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DesyFellow/Library/Python/2.7/lib/python/site-packages/root_numpy/__init__.py:46: RuntimeWarning: numpy 1.12.1 is currently installed but you installed root_numpy against numpy 1.12.0. Please consider reinstalling root_numpy for this numpy version.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016, 12:39:47) \n",
      "[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\n",
      "Sklearn version 0.18.1\n",
      "Root_numpy version 4.7.2\n",
      "Numpy version 1.12.1\n",
      "Scipy version 0.18.1\n",
      "Pandas version 0.19.0+515.gd0a281f\n",
      "Matplotlib version 2.0.0\n",
      "Seaborn version 0.8.dev\n",
      "Imblance version 0.2.1\n",
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "## Import common python libraries\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Import from root_numpy library\n",
    "import root_numpy\n",
    "from root_numpy import root2array, rec2array\n",
    "\n",
    "# Import panda library\n",
    "from pandas.tools import plotting\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from pandas.core.index import Index\n",
    "import pandas.core.common as com\n",
    "\n",
    "# Import scipy\n",
    "import scipy\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy as sp\n",
    "\n",
    "# Import itertools\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "\n",
    "# Import Jupyter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Import scikit-learn\n",
    "import sklearn\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import (confusion_matrix, roc_auc_score, roc_curve, \n",
    "                             auc, average_precision_score, precision_score, \n",
    "                             brier_score_loss, recall_score, f1_score, log_loss, \n",
    "                             classification_report, precision_recall_curve, accuracy_score)\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import feature_selection\n",
    "\n",
    "# Import imblearn\n",
    "import imblearn\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Optimize \n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "# python utilities\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# python regular-expression\n",
    "import re\n",
    "\n",
    "# Sciki-kit learn graph \n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Check the versions of libraries/packages\n",
    "print(\"Python version \" + sys.version)\n",
    "print(\"Sklearn version \" + sklearn.__version__)\n",
    "print(\"Root_numpy version \" + root_numpy.__version__)\n",
    "print(\"Numpy version \" + np.__version__)\n",
    "print(\"Scipy version \" + scipy.__version__)\n",
    "print(\"Pandas version \" + pd.__version__)\n",
    "print(\"Matplotlib version \" + matplotlib.__version__)\n",
    "print(\"Seaborn version \" + sns.__version__)\n",
    "print(\"Imblance version \" +imblearn.__version__)\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Specifying which nodes should be run interactively\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Data loading function\n",
    "\n",
    "def load(sig_filename, bkg_filename, category, features):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    https://www.kaggle.com/wiki/MultiClassLogLoss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sig_filename : array, shape = [n_samples]\n",
    "            true class, intergers in [0, n_classes - 1)\n",
    "    bkg_filename : array, shape = [n_samples, n_classes]\n",
    "    category: string\n",
    "    features: array, shape = [n_features]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    signal = root2array(sig_filename, category, features)\n",
    "    signal = rec2array(signal)\n",
    "\n",
    "    backgr = root2array(bkg_filename, category, features)\n",
    "    backgr = rec2array(backgr)\n",
    "\n",
    "    # for sklearn data is usually organised\n",
    "    # into one 2D array of shape (n_samples x n_features)\n",
    "    # containing all the data and one array of categories\n",
    "    # of length n_samples\n",
    "    X = np.concatenate((signal, backgr))\n",
    "    y = np.concatenate((np.ones(signal.shape[0]), np.zeros(backgr.shape[0])))\n",
    "\n",
    "    # convert to numpy ndarray into pandas dataframe\n",
    "    dataframe_X = pd.DataFrame(data=X, columns=features)\n",
    "    dataframe_y = pd.DataFrame(data=y, columns=['y'])\n",
    "\n",
    "    data = pd.concat([dataframe_X, dataframe_y], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events: 11502\n",
      "Number of features: 14\n",
      "\n",
      "Wall time to read in file input:  0.19686794281\n",
      "CPU time to read in file input:  0.183924\n"
     ]
    }
   ],
   "source": [
    "## Load input data files\n",
    "\n",
    "# Feature names\n",
    "branch_names = \"\"\"mass_tag_tag_min_deltaR,median_mass_jet_jet,\n",
    "    maxDeltaEta_tag_tag,mass_higgsLikeDijet,HT_tags,\n",
    "    btagDiscriminatorAverage_tagged,mass_jet_tag_min_deltaR,\n",
    "    mass_jet_jet_min_deltaR,mass_tag_tag_max_mass,maxDeltaEta_jet_jet,\n",
    "    centrality_jets_leps,centrality_tags,globalTimesEventWeight\"\"\".split(\",\")\n",
    "\n",
    "features = [c.strip() for c in branch_names]\n",
    "features = (b.replace(\" \", \"_\") for b in features)\n",
    "features = list(b.replace(\"-\", \"_\") for b in features)\n",
    "\n",
    "wall = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "# Load dataset\n",
    "signal_sample = \"combined/signalMC.root\"\n",
    "background_sample = \"combined/backgroundMC.root\"\n",
    "tree_category = \"event_mvaVariables_step7_cate4\"\n",
    "\n",
    "data = load(signal_sample, background_sample, tree_category, features)\n",
    "\n",
    "print \"Total number of events: {}\\nNumber of features: {}\".format(data.shape[0], data.shape[1])\n",
    "\n",
    "# Store a copy for later use\n",
    "df_archived = data.copy(deep=True)\n",
    "\n",
    "print \"\\nWall time to read in file input: \", time.time()-wall\n",
    "print \"CPU time to read in file input: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Function to extract class label counts and percentage\n",
    "\n",
    "def class_info(classes):\n",
    "    # Store the number of signal and background events\n",
    "    class_count = {}\n",
    "    counts = Counter(classes)\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    for cls in counts.keys():\n",
    "        class_count[class_label[cls]] = counts[cls]\n",
    "        print(\"%10s: %7d  =  % 5.1f%%\" % (class_label[cls], counts[cls], float(counts[cls])/float((total))*100.0))\n",
    "\n",
    "    return (class_count[\"signal\"], class_count[\"background\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background:    6777  =   58.9%\n",
      "    signal:    4725  =   41.1%\n"
     ]
    }
   ],
   "source": [
    "## Determine class label counts and percentages\n",
    "\n",
    "class_label = {0.0: \"background\", 1.0: \"signal\"}\n",
    "class_info(data.y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Create features dataframe and target array\n",
    "\n",
    "df_X = data.drop(\"y\", axis=1, inplace=False)\n",
    "df_y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot AUC for ROC curve for several classifiers out-of-the-box\n",
    "\n",
    "# Set feature scaling type\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# prepare models: create a mapping of ML classifier name to algorithm\n",
    "pipe_classifiers = {\n",
    "    'SVC':  make_pipeline(scaler, SVC(class_weight=\"balanced\")), # sample_weight included\n",
    "    'LogisticRegression'    : make_pipeline(scaler, LogisticRegression(class_weight=\"balanced\")),\n",
    "    'AdaBoostClassifier'    : make_pipeline(None,   AdaBoostClassifier()),\n",
    "    'RandomForestClassifier': make_pipeline(None,   RandomForestClassifier(min_samples_leaf=10)),\n",
    "    'DecisionTreeClassifier': make_pipeline(None,   DecisionTreeClassifier(min_samples_leaf=10,\n",
    "                                                                           class_weight=\"balanced\")),\n",
    "    'GradientBoostingClassifier': make_pipeline(None,   GradientBoostingClassifier(min_samples_leaf=10)),\n",
    "    'BaggingClassifier': make_pipeline(None,   BaggingClassifier(n_estimators=1000)),\n",
    "    'ExtraTreesClassifier' :  make_pipeline(None, ExtraTreesClassifier(min_samples_leaf=10)),#,\n",
    "    #'LinearDiscriminantAnalysis':  make_pipeline(scaler, LinearDiscriminantAnalysis()),\n",
    "    #'KNeighborsClassifier':  make_pipeline(scaler, KNeighborsClassifier()),\n",
    "    #'GaussianNB' :  make_pipeline(scaler, GaussianNB()), \n",
    "    #'MLPClassifier':  make_pipeline(scaler, MLPClassifier()), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Early stopping plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Validation curve definition\n",
    "\n",
    "def early_stopping_curve(clf, X, y):\n",
    "    \"\"\"Early stopping curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : array, shape = [n_samples]\n",
    "            true class, intergers in [0, n_classes - 1)\n",
    "    X : array, shape = [n_samples, n_classes]\n",
    "    y : array, shape = [n_samples, n_classes]\n",
    "    Returns\n",
    "    -------\n",
    "    plt : matplotlib\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split data into a development and evaluation set\n",
    "    X_dev,X_eval, y_dev, y_eval = train_test_split(X, y, test_size=.33,\n",
    "                                                   random_state=seed)\n",
    "    # Split development set into a train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.33,\n",
    "                                                        random_state=seed+31415)\n",
    "    \n",
    "    sample_weight_train = X_train[\"globalTimesEventWeight\"].values\n",
    "    \n",
    "    sample_weight_test = X_test[\"globalTimesEventWeight\"].values\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train = X_train.drop('globalTimesEventWeight', axis=1, inplace=False)\n",
    "    X_test = X_test.drop('globalTimesEventWeight', axis=1, inplace=False)\n",
    "    \n",
    "    clf.fit(X_train, y_train, **{'sample_weight': sample_weight_train}) \n",
    "\n",
    "    # Customize the major grid\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid(which='major', linestyle='-', linewidth='0.2', color='gray')\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    test_score = np.empty(len(clf.estimators_))\n",
    "    train_score = np.empty(len(clf.estimators_))\n",
    "\n",
    "    for i, pred in enumerate(clf.staged_predict_proba(X_test)):\n",
    "        test_score[i] = 1-roc_auc_score(y_test, pred[:,1], sample_weight=sample_weight_test)\n",
    "\n",
    "    for i, pred in enumerate(clf.staged_predict_proba(X_train)):\n",
    "        train_score[i] = 1-roc_auc_score(y_train, pred[:,1], sample_weight=sample_weight_train)\n",
    "\n",
    "    best_iter = np.argmin(test_score)\n",
    "    learn = clf.get_params()['learning_rate']\n",
    "    depth = clf.get_params()['max_depth']\n",
    "        \n",
    "    test_line = plt.plot(test_score, label='test (1-roc_auc=%.3f)'%(test_score[best_iter]))\n",
    "\n",
    "    colour = test_line[-1].get_color()\n",
    "    plt.plot(train_score, '--', color=colour, \n",
    "             label='train (1-roc_auc=%.3f)\\nlearn=%.1f depth=%i'\n",
    "             %(train_score[best_iter],learn,depth))\n",
    "\n",
    "    plt.title(\"Early stopping curve\")\n",
    "    plt.xlabel(\"Number of boosting iterations\")\n",
    "    plt.ylabel(\"1 - area under ROC\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.axvline(x=best_iter, color=colour)\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##  plot the early stopping curve for the fitted classifier\n",
    "# and check with the test set at which number of n_estimators we reach the minimum test error.\n",
    "\n",
    "wall = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "# Set of hyper-parameter selected\n",
    "opts = dict(max_depth=3, \n",
    "            learning_rate=0.01, \n",
    "            n_estimators=1200)\n",
    "\n",
    "clf = GradientBoostingClassifier(**opts)\n",
    "\n",
    "early_stopping_curve(clf, df_X, df_y)\n",
    "\n",
    "print \"\\nWall time to generate early stopping plots: \", time.time()-wall\n",
    "print \"CPU time to generate early stopping plots: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature ranking and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Feature ranking \n",
    "\n",
    "def feature_ranking_plot(X, importances, std, indices, title):\n",
    "\n",
    "    # Customize the major grid\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid(which='major', linestyle='-', linewidth='0.2', color='gray')\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    X_tmp = X.drop('globalTimesEventWeight', axis=1, inplace=False)\n",
    "\n",
    "    print(title)\n",
    "    for i in xrange(X_tmp.shape[1]):\n",
    "        print(\"%d. %s (%f)\" % (i + 1, X_tmp.columns[indices[i]], importances[indices[i]]))\n",
    "\n",
    "    # Plot the feature importances of the model\n",
    "    plt.title(title)\n",
    "    plt.bar(range(X_tmp.shape[1]), importances[indices],\n",
    "            color=\"r\", yerr=std, align=\"center\") \n",
    "    plt.xticks(range(X_tmp.shape[1]), X_tmp.columns[indices], rotation='vertical')\n",
    "    plt.xlim([-1, X_tmp.shape[1]])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Extract feature selection\n",
    "\n",
    "def extract_feature_selected(clf, X, y):\n",
    "    \n",
    "    # Split data into a development and evaluation set\n",
    "    X_dev,X_eval, y_dev, y_eval = train_test_split(df_X, df_y, test_size=.33, \n",
    "                                                   random_state=seed)\n",
    "    # Split development set into a train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.33,\n",
    "                                                        random_state=seed+31415)\n",
    "\n",
    "    sample_weight = X_train[\"globalTimesEventWeight\"].values\n",
    "    \n",
    "    X_train = X_train.drop('globalTimesEventWeight', axis=1, inplace=False)\n",
    "    X_test = X_test.drop('globalTimesEventWeight', axis=1, inplace=False)\n",
    "    \n",
    "    clf.fit(X_train,y_train, **{clf.steps[1][0].lower()+'__sample_weight': sample_weight}) \n",
    "    \n",
    "    select_indices = clf.named_steps['SELECT'].transform(\n",
    "    np.arange(len(X_train.columns)).reshape(1, -1))\n",
    "\n",
    "    feature_names = X_train.columns[select_indices]\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Feature selection\n",
    "\n",
    "def features_selection_model_performance(clf, X, y, parameter_set):\n",
    "\n",
    "    # Customize the major grid\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid(which='major', linestyle='-', linewidth='0.2', color='gray')\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    this_scores = list()\n",
    "    score_means = list()\n",
    "    score_stds = list()\n",
    "\n",
    "    params = {'SELECT__k': 'top k features', \n",
    "              'SELECT__threshold': 'feature threshold',\n",
    "              'SELECT__n_features_to_select': 'n features to select',\n",
    "              'SELECT__percentile': 'percentile',\n",
    "              'SELECT__cv': 'k-fold',\n",
    "              'SELECT__selection_threshold':'selection threshold'}\n",
    "\n",
    "    label = [keyname for keyname in clf.get_params().keys() if keyname in params.keys()][0]\n",
    "    \n",
    "    sample_weights = X[\"globalTimesEventWeight\"].values\n",
    "    \n",
    "    for k in parameter_set:\n",
    "\n",
    "        param = {label: k}\n",
    "        clf.set_params(**param) \n",
    "        \n",
    "        # Compute cross-validation score using 1 CPU\n",
    "        this_scores = cross_val_score(clf, \n",
    "                                      X.drop('globalTimesEventWeight', axis=1, inplace=False), y,\n",
    "                                      cv=3, n_jobs=1, \n",
    "                                      fit_params={'classifier__sample_weight': sample_weights})\n",
    "        score_means.append(this_scores.mean())\n",
    "        score_stds.append(this_scores.std())\n",
    "\n",
    "    plt.errorbar(parameter_set, score_means, np.array(score_stds))\n",
    "\n",
    "    model = clf.steps[1][0]\n",
    "\n",
    "    title = 'Performance of the {}-{} varying for features selected'.format(model,\n",
    "                                                                            clf.get_params().keys()[1])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(params[label])\n",
    "    plt.ylabel('Prediction rate')\n",
    "\n",
    "    print  extract_feature_selected(clf, X, y).values[0]\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Univariate Statistics: variance threshold\n",
    "\n",
    "process = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "# Sample weights\n",
    "sample_weights = df_X[\"globalTimesEventWeight\"].values\n",
    "\n",
    "# Removing features with low variance\n",
    "select = VarianceThreshold(threshold=0.0)\n",
    "\n",
    "# Configure pipeline \n",
    "pipe = Pipeline([('SELECT', select), ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Features selected by variance threshold\n",
    "_=pipe.fit(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y,\n",
    "         **{'classifier__sample_weight': sample_weights})\n",
    "\n",
    "# Extract feature importances, standard deviations, and indices\n",
    "importances = pipe.named_steps[\"SELECT\"].variances_\n",
    "std = np.std([importance for importance in importances], axis=0)*np.ones(len(importances))\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature ranking\n",
    "feature_ranking_plot(df_X, importances, std, indices, \"Feature importances based on variance ranking\")\n",
    "\n",
    "# Variance thresholds\n",
    "threshold = [0.0, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "# Plot feature selection\n",
    "features_selection_model_performance(pipe, df_X, df_y, threshold)\n",
    "\n",
    "print \"Wall time to produce feature ranking plots: \", time.time()-wall\n",
    "print \"CPU time to prodduce feature ranking plots: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Univariate Statistics: percentile (of the highest feature scores)\n",
    "\n",
    "process = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "# Sample weights\n",
    "sample_weights = df_X[\"globalTimesEventWeight\"].values\n",
    "\n",
    "# Univariate feature selection according to a percentile of the highest scores\n",
    "select = feature_selection.SelectPercentile(f_classif)\n",
    "\n",
    "# Configure pipeline\n",
    "pipe = Pipeline([('SELECT', select), ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Features selected based on k-highest scores\n",
    "_=pipe.fit(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y,\n",
    "         **{'classifier__sample_weight': sample_weights})\n",
    "\n",
    "# Extract feature importances, standard deviations, and indices\n",
    "importances = pipe.named_steps[\"SELECT\"].scores_\n",
    "std = np.std([score for score in importances], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature ranking\n",
    "feature_ranking_plot(df_X, importances, std, indices, \"Feature importances based percentile of the highest feature scores classification ranking\")\n",
    "\n",
    "# Percentile selections\n",
    "percentiles = [6, 10, 15, 20, 30, 40, 60, 80, 100]\n",
    "\n",
    "# Plot feature selection\n",
    "features_selection_model_performance(pipe, df_X, df_y, percentiles)\n",
    "\n",
    "print \"Wall time to produce feature ranking plots: \", time.time()-wall\n",
    "print \"CPU time to prodduce feature ranking plots: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Univariate Statistics: K-best features\n",
    "\n",
    "process = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "# Sample weights\n",
    "sample_weights = df_X[\"globalTimesEventWeight\"].values\n",
    "\n",
    "#  Univariate feature selection based on the k highest scores\n",
    "select = SelectKBest(score_func=f_classif, k=6)\n",
    "\n",
    "# Configure pipeline \n",
    "pipe = Pipeline([('SELECT', select), ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Features selected based on k-highest scores\n",
    "_=pipe.fit(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y,\n",
    "         **{'classifier__sample_weight': sample_weights})\n",
    "\n",
    "# Extract feature importances, standard devations, and indices\n",
    "importances = pipe.named_steps[\"SELECT\"].scores_\n",
    "std = np.std([importance for importance in importances], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature ranking\n",
    "feature_ranking_plot(df_X, importances, std, indices, \"Feature importances based k-best features classification ranking\")\n",
    "\n",
    "# K-best features\n",
    "k = [2, 4, 6, 8, 10, 12]\n",
    "\n",
    "# Plot feature selection\n",
    "features_selection_model_performance(pipe, df_X, df_y, k)\n",
    "\n",
    "\n",
    "print \"Wall time to produce feature ranking plots: \", time.time()-wall\n",
    "print \"CPU time to prodduce feature ranking plots: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Model-Based Feature Selection: SelectFromModel (RandomForrestClassifier)\n",
    "\n",
    "# The SelectFromModel class selects all features that have an importance measure \n",
    "# of the feature (as provided by the supervised model) greater than the provided \n",
    "# threshold. \n",
    "\n",
    "process = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "# Sample weights\n",
    "sample_weights = df_X[\"globalTimesEventWeight\"].values\n",
    "\n",
    "# Selection using SelectFromModel based on RandomForestClassifier  \n",
    "select = SelectFromModel(RandomForestClassifier(n_estimators=10, random_state=seed), threshold=\"median\")\n",
    "\n",
    "# Configure pipeline\n",
    "pipe = Pipeline([('SELECT', select), ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Features selected by SelectFromModel using the RandomForestClassifier\n",
    "_=pipe.fit(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y,\n",
    "         **{'classifier__sample_weight': sample_weights})\n",
    "\n",
    "# Extract feature importances, standard deviations, and indices\n",
    "importances = pipe.named_steps[\"SELECT\"].estimator_.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in pipe.named_steps[\"SELECT\"].estimator_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature ranking\n",
    "feature_ranking_plot(df_X, importances, std, indices, \"RandomForestClassifier model-based feature importances ranking\")\n",
    "\n",
    "# Select n features\n",
    "#p = [\"mean\", \"median\"]\n",
    "p = [0.0001, 0.0005, 0.001, 0.01, 0.1]\n",
    "\n",
    "# Plot feature selection\n",
    "features_selection_model_performance(pipe, df_X, df_y, p)\n",
    "\n",
    "\n",
    "print \"Wall time to produce feature ranking plots: \", time.time()-wall\n",
    "print \"CPU time to prodduce feature ranking plots: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Model-Based Feature Selection: SelectFromModel (ExtraTreesClassifier)\n",
    "\n",
    "process = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "# Sample weights\n",
    "sample_weights = df_X[\"globalTimesEventWeight\"].values\n",
    "\n",
    "# Selection using select from model based on extra trees classifier\n",
    "select = SelectFromModel(ExtraTreesClassifier(random_state=seed), threshold=\"median\")\n",
    "\n",
    "# Configure pipeline\n",
    "pipe = Pipeline([('SELECT', select), ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Features selected by SelectFromModel using the ExtraTreesClassifier\n",
    "_=pipe.fit(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y,\n",
    "         **{'classifier__sample_weight': sample_weights})\n",
    "\n",
    "# Extract feature importances, standard deviations, and indices\n",
    "importances = pipe.named_steps[\"SELECT\"].estimator_.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in pipe.named_steps[\"SELECT\"].estimator_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature ranking\n",
    "feature_ranking_plot(df_X, importances, std, indices, \"ExtraTree Model-based feature importances ranking\")\n",
    "\n",
    "# Selection based on threshold\n",
    "threshold = [0.001, 0.01, 0.017, 0.019] \n",
    "\n",
    "# Plot feature selection\n",
    "features_selection_model_performance(pipe, df_X, df_y, threshold)\n",
    "\n",
    "\n",
    "print \"\\nWall time to generate features selection performance plots: \", time.time()-wall\n",
    "print \"CPU time to generate features selection performance plots: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Iterative Feature Selection: Recursive feature elimination (without cross-validation)\n",
    "\n",
    "process = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "# Sample weights\n",
    "sample_weights = df_X[\"globalTimesEventWeight\"].values\n",
    "\n",
    "# Recursive feature elimination \n",
    "select = RFE(estimator=RandomForestClassifier(), step=1)\n",
    "\n",
    "# Configure pipeline\n",
    "pipe = Pipeline([('SELECT', select), ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Features selected based on RFE ranking\n",
    "_=pipe.fit(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y,\n",
    "         **{'classifier__sample_weight': sample_weights})\n",
    "\n",
    "# Extract feature ranking, standard deviations, and indices\n",
    "importances = pipe.named_steps[\"SELECT\"].ranking_\n",
    "std = np.std([importance for importance in importances], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "             \n",
    "# n features to select\n",
    "n_features_to_select = [2, 3, 5, 7]\n",
    "\n",
    "# Plot feature ranking\n",
    "feature_ranking_plot(df_X, importances, std, indices, \"RFE model-based feature importances ranking\")\n",
    "\n",
    "# Plot feature selection\n",
    "features_selection_model_performance(pipe, df_X, df_y, n_features_to_select)\n",
    "\n",
    "print \"Wall time to produce feature ranking plots: \", time.time()-wall\n",
    "print \"CPU time to prodduce feature ranking plots: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Iterative Feature Selection: Recursive feature elimination with cross-validation\n",
    "\n",
    "process = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "# Sample weights\n",
    "sample_weights = df_X[\"globalTimesEventWeight\"].values\n",
    "\n",
    "# Recursive feature elimination (RFE) with cross-validation \n",
    "select = RFECV(estimator=RandomForestClassifier(), step=1, \n",
    "               cv=StratifiedKFold(3), scoring='accuracy')\n",
    "\n",
    "# Configure pipeline\n",
    "pipe = Pipeline([('SELECT', select), ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Features selected based on k-highest scores\n",
    "_=pipe.fit(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y,\n",
    "         **{'classifier__sample_weight': sample_weights})\n",
    "\n",
    "# Extract feature importances, standard deviations, and indices\n",
    "importances = pipe.named_steps[\"SELECT\"].ranking_\n",
    "std = np.std([importance for importance in importances], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature ranking\n",
    "feature_ranking_plot(df_X, importances, std, indices, \"RFECV model-based feature importances ranking\")\n",
    "\n",
    "# Extract feature selection\n",
    "pipe.fit(df_X, df_y)\n",
    "\n",
    "kfold = [3, 5]\n",
    "\n",
    "# Plot feature selection\n",
    "features_selection_model_performance(pipe, df_X, df_y, kfold)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % pipe.named_steps[\"SELECT\"].n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "_=plt.figure()\n",
    "_=plt.xlabel(\"Number of features selected\")\n",
    "_=plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "_=plt.plot(range(1, len(pipe.named_steps[\"SELECT\"].grid_scores_) + 1), pipe.named_steps[\"SELECT\"].grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print \"Wall time to produce feature ranking plots: \", time.time()-wall\n",
    "print \"CPU time to prodduce feature ranking plots: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model building: feature scaling, selection, hyper-parameter optimization, and final model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Standard nested k-fold cross-validation\n",
    "\n",
    "def nestedGridSearchCV(Classifier, X, y, outer_cv, inner_cv, \n",
    "                       parameter_grid, scoring=\"accuracy\"):\n",
    "    \"\"\"Nested k-fold crossvalidation.\"\"\"\n",
    "    \n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    ----------\n",
    "    Classifier : array, shape = [n_samples]\n",
    "            true class, intergers in [0, n_classes - 1)\n",
    "    X : array, shape = [n_samples, n_classes]\n",
    "    y : array, shape = [n_samples, n_classes]\n",
    "    outer_cv:  shape = [n_samples, n_classes]\n",
    "    inner_cv:  shape = [n_samples, n_classes]\n",
    "    parameter_grid: shape = [n_samples, n_classes]\n",
    "    scoring:   shape = [n_samples, n_classes]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Grid classifier: classifier re-fitted to full dataset\n",
    "    \"\"\"    \n",
    "    \n",
    "    \n",
    "    outer_scores = []\n",
    "    \n",
    "    for training_samples, test_samples in outer_cv.split(X, y):\n",
    "\n",
    "        # Training datasets\n",
    "        x_training_temp = pd.DataFrame(X.iloc[training_samples], columns=features)\n",
    "\n",
    "        x_training = x_training_temp.drop('globalTimesEventWeight', axis=1, inplace=False)\n",
    "        y_training = pd.Series(y.iloc[training_samples])\n",
    "\n",
    "        # Extract sample weight\n",
    "        weights_training = x_training_temp[\"globalTimesEventWeight\"].values\n",
    "\n",
    "        # Testing datasets\n",
    "        x_testing_temp = pd.DataFrame(X.iloc[test_samples], columns=features)\n",
    "\n",
    "        x_testing = x_testing_temp.drop('globalTimesEventWeight', axis=1, inplace=False)\n",
    "        y_testing = pd.Series(y.iloc[test_samples])\n",
    "\n",
    "        # set up grid search configuration\n",
    "        cv = GridSearchCV(estimator=Classifier, param_grid=parameter_grid,\n",
    "                          cv=inner_cv, scoring=\"accuracy\", \n",
    "                          n_jobs=-1,\n",
    "                          fit_params={\"classifier__sample_weight\": weights_training})\n",
    "                         \n",
    "        # train on the training set\n",
    "        cv.fit(x_training, y_training)\n",
    "        \n",
    "        # evaluate\n",
    "        outer_scores.append(cv.score(x_testing, y_testing))\n",
    "\n",
    "    # Print final model evaluation (i.e. mean cross-validation scores)\n",
    "    print \"Final model evaluation (mean cross-val scores):\\n\", np.array(outer_scores).mean()\n",
    "    \n",
    "    # note: the scoring is being done without the weights associated with X\n",
    "    # fit model to entire training dataset (i.e tuning & validation dataset)\n",
    "    cv.best_estimator_.fit(X.drop('globalTimesEventWeight', axis=1, inplace=False), y,\n",
    "                           **{\"classifier__sample_weight\": X[\"globalTimesEventWeight\"].values})\n",
    "    \n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Setup common configuration: cross-validation type, feature scaling, and model pipeline \n",
    "\n",
    "# Standard K-Fold cross-validation\n",
    "k_fold=5\n",
    "outer_kfold_cv = KFold(n_splits=k_fold, shuffle=True, random_state=seed)\n",
    "inner_kfold_cv = KFold(n_splits=k_fold-1, shuffle=True, random_state=seed)\n",
    "\n",
    "# preprocessing using 0-1 scaling by removing the mean and scaling to unit variance \n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Default pipeline setup with dummy place holder steps\n",
    "pipe = Pipeline([('feature_scaling', None), \n",
    "                 ('feature_selection', None), \n",
    "                 ('classifier', DummyClassifier())]\n",
    "               )\n",
    "\n",
    "# Split data into a development and evaluation set\n",
    "X_dev,X_eval, y_dev,y_eval = train_test_split(df_X, df_y,\n",
    "                                              test_size=0.33, random_state=seed)\n",
    "# Split development set into a train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.33,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "select = VarianceThreshold()\n",
    "\n",
    "# create classifier for use in scikit-learn\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# prepare models: create a mapping of ML classifier name to algorithm\n",
    "param_grid = [\n",
    "    {'classifier': [model],\n",
    "     'classifier__n_estimators': [100],\n",
    "     'classifier__learning_rate': [0.1],\n",
    "     'feature_selection': [select],\n",
    "     'feature_selection__threshold': [0.4],\n",
    "     'feature_scaling': [scaler]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = nestedGridSearchCV(Classifier=pipe,\n",
    "                          X=X_train, y=y_train, \n",
    "                          outer_cv=outer_kfold_cv, inner_cv=inner_kfold_cv, \n",
    "                          parameter_grid=param_grid, \n",
    "                          scoring=\"roc_auc\")\n",
    "\n",
    "y_pred = grid.predict(X_test.drop('globalTimesEventWeight', axis=1, inplace=False))\n",
    "report = classification_report( y_test, y_pred )\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate best selected model\n",
    "\n",
    "print(\"Pipeline steps:\\n{}\".format(grid.best_estimator_.steps))\n",
    "\n",
    "# extract the first step \n",
    "components = grid.best_estimator_.named_steps[\"feature_scaling\"]\n",
    "print(\"components: {}\".format(components))\n",
    "classifier = grid.best_estimator_.named_steps[\"classifier\"]\n",
    "print(\"GradientBoostingClassifier classifier step:\\n{}\".format(classifier))\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Test set score: {:.2f}\".format(grid.best_estimator_.score(df_X.drop('globalTimesEventWeight', \n",
    "                                                                           axis=1, inplace=False), df_y))) \n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "select = SelectKBest(k=8)\n",
    "\n",
    "# create classifier for use in scikit-learn\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# prepare models: create a mapping of ML classifier name to algorithm\n",
    "param_grid = [\n",
    "    {'classifier': [model],\n",
    "     'classifier__n_estimators': [100],\n",
    "     'classifier__learning_rate': [0.1],\n",
    "     'feature_selection': [select],\n",
    "     'feature_selection__k': [10],\n",
    "     'feature_scaling': [scaler]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = nestedGridSearchCV(Classifier=pipe,\n",
    "                          X=X_train, y=y_train, \n",
    "                          outer_cv=outer_kfold_cv, inner_cv=inner_kfold_cv, \n",
    "                          parameter_grid=param_grid, \n",
    "                          scoring=\"roc_auc\")\n",
    "\n",
    "\n",
    "y_pred = grid.predict(X_test.drop('globalTimesEventWeight', axis=1, inplace=False))\n",
    "report = classification_report( y_test, y_pred )\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate best selected model\n",
    "\n",
    "print(\"Pipeline steps:\\n{}\".format(grid.best_estimator_.steps))\n",
    "\n",
    "# extract the first step \n",
    "components = grid.best_estimator_.named_steps[\"feature_scaling\"]\n",
    "print(\"components: {}\".format(components))\n",
    "classifier = grid.best_estimator_.named_steps[\"classifier\"]\n",
    "print(\"GradientBoostingClassifier classifier step:\\n{}\".format(classifier))\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Test set score: {:.2f}\".format(grid.best_estimator_.score(df_X.drop('globalTimesEventWeight', \n",
    "                                                                           axis=1, inplace=False), df_y))) \n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "select = feature_selection.SelectPercentile(f_classif)\n",
    "\n",
    "# create classifier for use in scikit-learn\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# prepare models: create a mapping of ML classifier name to algorithm\n",
    "param_grid = [\n",
    "    {'classifier': [model],\n",
    "     'classifier__n_estimators': [100],\n",
    "     'classifier__learning_rate': [0.1],\n",
    "     'feature_selection': [select],\n",
    "     'feature_selection__percentile': [90],\n",
    "     'feature_scaling': [scaler]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = nestedGridSearchCV(Classifier=pipe,\n",
    "                          X=X_train, y=y_train, \n",
    "                          outer_cv=outer_kfold_cv, inner_cv=inner_kfold_cv, \n",
    "                          parameter_grid=param_grid, \n",
    "                          scoring=\"roc_auc\")\n",
    "\n",
    "y_pred = grid.predict(X_test.drop('globalTimesEventWeight', axis=1, inplace=False))\n",
    "report = classification_report( y_test, y_pred )\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate best selected model\n",
    "\n",
    "print(\"Pipeline steps:\\n{}\".format(grid.best_estimator_.steps))\n",
    "\n",
    "# extract the first step \n",
    "components = grid.best_estimator_.named_steps[\"feature_scaling\"]\n",
    "print(\"components: {}\".format(components))\n",
    "classifier = grid.best_estimator_.named_steps[\"classifier\"]\n",
    "print(\"GradientBoostingClassifier classifier step:\\n{}\".format(classifier))\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Test set score: {:.2f}\".format(grid.best_estimator_.score(df_X.drop('globalTimesEventWeight', \n",
    "                                                                           axis=1, inplace=False), df_y))) \n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "select = RFE(estimator=RandomForestClassifier(), n_features_to_select=8, step=1)\n",
    "\n",
    "# create classifier for use in scikit-learn\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# prepare models: create a mapping of ML classifier name to algorithm\n",
    "param_grid = [\n",
    "    {'classifier': [model],\n",
    "     'classifier__n_estimators': [100],\n",
    "     'classifier__learning_rate': [0.1],\n",
    "     'feature_selection': [select],\n",
    "     'feature_selection__n_features_to_select': [10],\n",
    "     'feature_scaling': [scaler]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = nestedGridSearchCV(Classifier=pipe,\n",
    "                          X=X_train, y=y_train, \n",
    "                          outer_cv=outer_kfold_cv, inner_cv=inner_kfold_cv, \n",
    "                          parameter_grid=param_grid, \n",
    "                          scoring=\"roc_auc\")\n",
    "\n",
    "y_pred = grid.predict(X_test.drop('globalTimesEventWeight', axis=1, inplace=False))\n",
    "report = classification_report( y_test, y_pred )\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate best selected model\n",
    "\n",
    "print(\"Pipeline steps:\\n{}\".format(grid.best_estimator_.steps))\n",
    "\n",
    "# extract the first step \n",
    "components = grid.best_estimator_.named_steps[\"feature_scaling\"]\n",
    "print(\"components: {}\".format(components))\n",
    "classifier = grid.best_estimator_.named_steps[\"classifier\"]\n",
    "print(\"GradientBoostingClassifier classifier step:\\n{}\".format(classifier))\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Test set score: {:.2f}\".format(grid.best_estimator_.score(df_X.drop('globalTimesEventWeight',\n",
    "                                                                           axis=1, inplace=False), df_y))) \n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "select = RFECV(estimator=RandomForestClassifier(), step=1,  \n",
    "               cv=StratifiedKFold(3), scoring='accuracy')\n",
    "\n",
    "# create classifier for use in scikit-learn\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# prepare models: create a mapping of ML classifier name to algorithm\n",
    "param_grid = [\n",
    "    {'classifier': [model],\n",
    "     'classifier__n_estimators': [100],\n",
    "     'classifier__learning_rate': [0.1],\n",
    "     'feature_selection': [select],\n",
    "     'feature_selection__cv': [7],\n",
    "     'feature_scaling': [scaler]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = nestedGridSearchCV(Classifier=pipe,\n",
    "                          X=X_train, y=y_train, \n",
    "                          outer_cv=outer_kfold_cv, inner_cv=inner_kfold_cv, \n",
    "                          parameter_grid=param_grid, \n",
    "                          scoring=\"roc_auc\")\n",
    "\n",
    "y_pred = grid.predict(X_test.drop('globalTimesEventWeight', axis=1, inplace=False))\n",
    "report = classification_report( y_test, y_pred )\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate best selected model\n",
    "\n",
    "print(\"Pipeline steps:\\n{}\".format(grid.best_estimator_.steps))\n",
    "\n",
    "# extract the first step \n",
    "components = grid.best_estimator_.named_steps[\"feature_scaling\"]\n",
    "print(\"components: {}\".format(components))\n",
    "classifier = grid.best_estimator_.named_steps[\"classifier\"]\n",
    "print(\"GradientBoostingClassifier classifier step:\\n{}\".format(classifier))\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Test set score: {:.2f}\".format(grid.best_estimator_.score(df_X.drop('globalTimesEventWeight', \n",
    "                                                                           axis=1, inplace=False), df_y))) \n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implementing a Weighted Majority Rule Ensemble Classifier\n",
    "\n",
    "In order to trained a set of equally well performing models and balance out their individual weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Esemble classifier\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)]\n",
    "voting='hard' #'soft'\n",
    "\n",
    "eclf = VotingClassifier(estimators=estimators, voting=voting, weights=[1,1,1])\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression',\n",
    "                                                 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "\n",
    "    scores = cross_val_score(clf, df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y, \n",
    "                             cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Handling unblanaced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply the random over-sampling\n",
    "ros = RandomOverSampler()\n",
    "X_overresampled, y_overresampled = ros.fit_sample(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y)\n",
    "\n",
    "# Apply the random under-sampling\n",
    "rus = RandomUnderSampler()\n",
    "X_underresampled, y_underresampled = rus.fit_sample(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y)\n",
    "\n",
    "# Apply SMOTE SVM\n",
    "sm = SMOTE(kind='svm')\n",
    "X_resampled, y_resampled = sm.fit_sample(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression',\n",
    "                                                 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "\n",
    "    scores = cross_val_score(clf, X_overresampled, y_overresampled, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression',\n",
    "                                                 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "\n",
    "    scores = cross_val_score(clf, X_underresampled, y_underresampled, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression',\n",
    "                                                 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "\n",
    "    scores = cross_val_score(clf, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyper-parameter tuning:  Bayesian optimization using Gaussian Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define objective loss function to minimize/maximize\n",
    "\n",
    "def objective(params):    \n",
    "    # Split development set into a train and test set\n",
    "    X_train, X_test, y_train, y_test= train_test_split(df_X.drop('globalTimesEventWeight', axis=1, inplace=False), df_y, test_size=0.33, random_state=42)\n",
    "\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    \n",
    "    max_depth, learning_rate, max_features, min_samples_split, min_samples_leaf = params\n",
    "\n",
    "    clf.set_params(max_depth=max_depth,\n",
    "                   learning_rate=learning_rate,\n",
    "                   max_features=max_features,\n",
    "                   min_samples_split=min_samples_split, \n",
    "                   min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    #return -np.mean(cross_val_score(clf, X, y, cv=5, n_jobs=-1,\n",
    "    #                                scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "    #return np.mean(cross_val_score(clf, X, y, cv=5, n_jobs=-1, scoring=\"roc_auc\"))\n",
    "    shuffle = KFold(n_splits=5, shuffle=True)\n",
    "    score = cross_val_score(clf, X_train, y_train, cv=shuffle, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    #return 1-score.mean()\n",
    "\n",
    "    return score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_features = df_X.drop('globalTimesEventWeight', axis=1, inplace=False).shape[1]\n",
    "\n",
    "parameters  = [(1, 4),                           # max_depth\n",
    "               (10**-2, 10**0, \"log-uniform\"),   # learning_rate  (sample from \"log-uniform\" distribution)\n",
    "               (9, n_features),                  # max_features\n",
    "               (2, 100),                         # min_samples_split\n",
    "               (1, 100)]                         # min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optimize hyper-parameters of classifier\n",
    "\n",
    "process = time.time()\n",
    "process = time.clock()\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=50, random_state=0)\n",
    "\n",
    "clf_gp = gp_minimize(objective, parameters, n_calls=30, random_state=0)\n",
    "\n",
    "\"Best score=%.4f\" % clf_gp.fun\n",
    "\n",
    "print \"\\nWall time to generate classifier models: \", time.time()-wall\n",
    "print \"CPU time to generate classifier models: \", time.clock()-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"\"\"Best parameters:\n",
    "- max_depth=%d\n",
    "- learning_rate=%.6f\n",
    "- max_features=%d\n",
    "- min_samples_split=%d\n",
    "- min_samples_leaf=%d\"\"\" % (clf_gp.x[0], clf_gp.x[1], \n",
    "                            clf_gp.x[2], clf_gp.x[3], \n",
    "                            clf_gp.x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_convergence(clf_gp);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into a development and evaluation set\n",
    "X_dev,X_eval, y_dev,y_eval = train_test_split(df_X, df_y,\n",
    "                                              test_size=0.33, random_state=seed)\n",
    "# Split development set into a train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.33,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "space4rf = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('max_features', range(1,5)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(1,20)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "    #'scale': hp.choice('scale', [0, 1]),\n",
    "    #'normalize': hp.choice('normalize', [0, 1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "def hyperopt_train_test(params):\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    \n",
    "    return cross_val_score(clf, X_train.drop('globalTimesEventWeight', axis=1, inplace=False), y_train).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best = 0\n",
    "\n",
    "def f(params):\n",
    "    global best\n",
    "    acc = hyperopt_train_test(params)\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "    print 'new best:', best, params\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best: 0.660861754781 {'max_features': 4, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 11}\n",
      "new best: 0.660861754781 {'max_features': 4, 'n_estimators': 6, 'criterion': 'entropy', 'max_depth': 2}\n",
      "new best: 0.660861754781 {'max_features': 3, 'n_estimators': 1, 'criterion': 'entropy', 'max_depth': 14}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 13, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 1}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 9, 'criterion': 'entropy', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 2, 'criterion': 'entropy', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 8, 'criterion': 'gini', 'max_depth': 5}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 15, 'criterion': 'gini', 'max_depth': 1}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 18}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 3, 'criterion': 'entropy', 'max_depth': 18}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 7, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 15, 'criterion': 'gini', 'max_depth': 12}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 6, 'criterion': 'gini', 'max_depth': 1}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 6, 'criterion': 'entropy', 'max_depth': 18}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 8, 'criterion': 'entropy', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 17, 'criterion': 'gini', 'max_depth': 15}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 6, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 18, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 18, 'criterion': 'gini', 'max_depth': 3}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 18, 'criterion': 'gini', 'max_depth': 7}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 4, 'criterion': 'gini', 'max_depth': 19}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 16, 'criterion': 'gini', 'max_depth': 17}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 10, 'criterion': 'gini', 'max_depth': 13}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 14, 'criterion': 'gini', 'max_depth': 10}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 12, 'criterion': 'gini', 'max_depth': 16}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 18, 'criterion': 'gini', 'max_depth': 11}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 1, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 13, 'criterion': 'gini', 'max_depth': 2}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 18, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 9, 'criterion': 'gini', 'max_depth': 14}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 2, 'criterion': 'gini', 'max_depth': 5}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 5, 'criterion': 'gini', 'max_depth': 17}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 3, 'criterion': 'gini', 'max_depth': 13}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 16, 'criterion': 'gini', 'max_depth': 16}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 10, 'criterion': 'gini', 'max_depth': 15}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 14, 'criterion': 'gini', 'max_depth': 12}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 19}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 7, 'criterion': 'gini', 'max_depth': 3}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 12, 'criterion': 'entropy', 'max_depth': 7}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 4, 'criterion': 'gini', 'max_depth': 10}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 15, 'criterion': 'entropy', 'max_depth': 11}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 1, 'criterion': 'gini', 'max_depth': 2}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 8, 'criterion': 'gini', 'max_depth': 6}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 1}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 13, 'criterion': 'gini', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 9, 'criterion': 'gini', 'max_depth': 14}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 2, 'criterion': 'gini', 'max_depth': 4}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 18, 'criterion': 'gini', 'max_depth': 18}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 3, 'criterion': 'entropy', 'max_depth': 5}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 5, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 7, 'criterion': 'gini', 'max_depth': 12}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 15}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 6, 'criterion': 'gini', 'max_depth': 6}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 18, 'criterion': 'gini', 'max_depth': 3}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 15, 'criterion': 'gini', 'max_depth': 7}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 17}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 16, 'criterion': 'gini', 'max_depth': 13}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 8, 'criterion': 'gini', 'max_depth': 16}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 10, 'criterion': 'gini', 'max_depth': 19}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 14, 'criterion': 'gini', 'max_depth': 1}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 4, 'criterion': 'gini', 'max_depth': 10}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 18, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 12, 'criterion': 'gini', 'max_depth': 2}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 13, 'criterion': 'gini', 'max_depth': 11}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 18}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 1, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 2, 'criterion': 'gini', 'max_depth': 14}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 9, 'criterion': 'gini', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 5}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 12}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 3, 'criterion': 'gini', 'max_depth': 15}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 6, 'criterion': 'gini', 'max_depth': 17}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 7, 'criterion': 'gini', 'max_depth': 13}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 3}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 16, 'criterion': 'gini', 'max_depth': 19}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 15, 'criterion': 'gini', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 4, 'criterion': 'gini', 'max_depth': 7}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 16}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 14, 'criterion': 'gini', 'max_depth': 10}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 10, 'criterion': 'gini', 'max_depth': 11}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 8, 'criterion': 'gini', 'max_depth': 2}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 12, 'criterion': 'gini', 'max_depth': 1}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 1, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 9, 'criterion': 'gini', 'max_depth': 18}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 13, 'criterion': 'gini', 'max_depth': 14}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 2, 'criterion': 'gini', 'max_depth': 5}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 5, 'criterion': 'gini', 'max_depth': 12}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 3, 'criterion': 'gini', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 6, 'criterion': 'gini', 'max_depth': 15}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 7, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'entropy', 'max_depth': 17}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 17, 'criterion': 'gini', 'max_depth': 6}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 15, 'criterion': 'gini', 'max_depth': 7}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 16, 'criterion': 'gini', 'max_depth': 13}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 3}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 19}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 8, 'criterion': 'gini', 'max_depth': 1}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 10, 'criterion': 'gini', 'max_depth': 16}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 4, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 12, 'criterion': 'gini', 'max_depth': 10}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 19, 'criterion': 'entropy', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 14, 'criterion': 'gini', 'max_depth': 11}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 13, 'criterion': 'gini', 'max_depth': 2}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 1, 'criterion': 'gini', 'max_depth': 4}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 18}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 9, 'criterion': 'entropy', 'max_depth': 14}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 2, 'criterion': 'gini', 'max_depth': 8}\n",
      "new best: 0.673448299035 {'max_features': 3, 'n_estimators': 5, 'criterion': 'gini', 'max_depth': 5}\n",
      "new best: 0.673448299035 {'max_features': 2, 'n_estimators': 3, 'criterion': 'gini', 'max_depth': 9}\n",
      "new best: 0.673448299035 {'max_features': 4, 'n_estimators': 6, 'criterion': 'gini', 'max_depth': 12}\n",
      "new best: 0.673448299035 {'max_features': 1, 'n_estimators': 19, 'criterion': 'gini', 'max_depth': 15}\n",
      "new best: 0.676158326266 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.676158326266 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 17}\n",
      "new best: 0.676158326266 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 3}\n",
      "new best: 0.676158326266 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 7}\n",
      "new best: 0.676158326266 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.676158326266 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.676158326266 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.676158326266 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677128220686 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677128220686 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677128220686 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677128220686 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677128220686 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677128220686 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677128220686 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677128220686 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 13}\n",
      "new best: 0.677128220686 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 7, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 18, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 15, 'criterion': 'entropy', 'max_depth': 19}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 16, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 4, 'criterion': 'entropy', 'max_depth': 16}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 10}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 10, 'criterion': 'entropy', 'max_depth': 11}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 8, 'criterion': 'entropy', 'max_depth': 1}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 12, 'criterion': 'entropy', 'max_depth': 2}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 14, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 18}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 1, 'criterion': 'entropy', 'max_depth': 14}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 13, 'criterion': 'entropy', 'max_depth': 12}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 2, 'criterion': 'entropy', 'max_depth': 5}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 9, 'criterion': 'entropy', 'max_depth': 15}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 6, 'criterion': 'entropy', 'max_depth': 17}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 3, 'criterion': 'entropy', 'max_depth': 7}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 13}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 18, 'criterion': 'entropy', 'max_depth': 19}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 7, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 3}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 15, 'criterion': 'entropy', 'max_depth': 1}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 10, 'criterion': 'entropy', 'max_depth': 16}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 16, 'criterion': 'entropy', 'max_depth': 11}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 10}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 8, 'criterion': 'entropy', 'max_depth': 2}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 4, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 14, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 14}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 12, 'criterion': 'entropy', 'max_depth': 18}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 5}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 1, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 13, 'criterion': 'entropy', 'max_depth': 12}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 9, 'criterion': 'entropy', 'max_depth': 15}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 17}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 2, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 6, 'criterion': 'entropy', 'max_depth': 3}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 3, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 7}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 19}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 7, 'criterion': 'entropy', 'max_depth': 13}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 10}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 18, 'criterion': 'entropy', 'max_depth': 16}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 15, 'criterion': 'entropy', 'max_depth': 11}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 16, 'criterion': 'entropy', 'max_depth': 1}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 2}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 10, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 8, 'criterion': 'entropy', 'max_depth': 18}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 14, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 4, 'criterion': 'entropy', 'max_depth': 5}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 14}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 12, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 13, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 1, 'criterion': 'entropy', 'max_depth': 15}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 12}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 9, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 7}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 3, 'criterion': 'entropy', 'max_depth': 17}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 2, 'criterion': 'entropy', 'max_depth': 13}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 6, 'criterion': 'entropy', 'max_depth': 3}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 7, 'criterion': 'entropy', 'max_depth': 19}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 16}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 18, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 16, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 15, 'criterion': 'entropy', 'max_depth': 10}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 11}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 10, 'criterion': 'entropy', 'max_depth': 1}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 8, 'criterion': 'entropy', 'max_depth': 2}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 9}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 12, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 4, 'criterion': 'entropy', 'max_depth': 18}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 14}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 14, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 1, 'criterion': 'entropy', 'max_depth': 5}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 13, 'criterion': 'entropy', 'max_depth': 12}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 2, 'criterion': 'entropy', 'max_depth': 15}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 9, 'criterion': 'entropy', 'max_depth': 3}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 17}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 3, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 6, 'criterion': 'entropy', 'max_depth': 7}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 13}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 7, 'criterion': 'entropy', 'max_depth': 19}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 15, 'criterion': 'entropy', 'max_depth': 16}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 18, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 1}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 16, 'criterion': 'entropy', 'max_depth': 10}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 10, 'criterion': 'entropy', 'max_depth': 11}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 14, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 4, 'criterion': 'entropy', 'max_depth': 2}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 8, 'criterion': 'entropy', 'max_depth': 9}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 18}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 12, 'criterion': 'entropy', 'max_depth': 14}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 1, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 13, 'criterion': 'entropy', 'max_depth': 12}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 15}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 2, 'criterion': 'entropy', 'max_depth': 5}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 9, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 17}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 3}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 7}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 3, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 6, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 18, 'criterion': 'entropy', 'max_depth': 19}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 7, 'criterion': 'entropy', 'max_depth': 13}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 16, 'criterion': 'entropy', 'max_depth': 16}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 15, 'criterion': 'entropy', 'max_depth': 1}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 10, 'criterion': 'entropy', 'max_depth': 4}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 10}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 4, 'criterion': 'entropy', 'max_depth': 11}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 8, 'criterion': 'entropy', 'max_depth': 2}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 12, 'criterion': 'entropy', 'max_depth': 9}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 1, 'criterion': 'entropy', 'max_depth': 18}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 14, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 14}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 9, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 13, 'criterion': 'entropy', 'max_depth': 5}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 12}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 3, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 2, 'criterion': 'entropy', 'max_depth': 15}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 6, 'criterion': 'entropy', 'max_depth': 17}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 7, 'criterion': 'entropy', 'max_depth': 3}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 19}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 18, 'criterion': 'entropy', 'max_depth': 7}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 15, 'criterion': 'entropy', 'max_depth': 13}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 1, 'n_estimators': 16, 'criterion': 'entropy', 'max_depth': 16}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 10}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 10, 'criterion': 'entropy', 'max_depth': 8}\n",
      "new best: 0.677512891215 {'max_features': 2, 'n_estimators': 17, 'criterion': 'entropy', 'max_depth': 1}\n",
      "new best: 0.677512891215 {'max_features': 4, 'n_estimators': 8, 'criterion': 'entropy', 'max_depth': 6}\n",
      "new best: 0.677512891215 {'max_features': 3, 'n_estimators': 14, 'criterion': 'entropy', 'max_depth': 11}\n",
      "best:\n",
      "{'max_features': 3, 'n_estimators': 16, 'criterion': 1, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(f, space4rf, algo=tpe.suggest, max_evals=300, trials=trials)\n",
    "\n",
    "print 'best:'\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 n_estimators\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11c8efa90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11b766e50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 max_depth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10513a6d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11947d2d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 max_features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x119967890>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11b81b910>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 criterion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11c911350>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11bf72dd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAJMCAYAAACyzuTPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsvXmcXFWd9/++91ZVr9Xp7qSzdneChEgSlkDCYkgCCMwM\nUUTGB42EGUZwYB59eImisggDimIQl3FGEXSYvAZFJYoyDhP9KY4mJJEkxiRmgywm6TXdnV6qeqvt\n3vP7o1KVdNNdt+5NVXVV9/ftq21O6t4659y6fep87nfTlFIKQRAEQRAEQRAEoeDQx3oAgiAIgiAI\ngiAIgjtE0AmCIAiCIAiCIBQoIugEQRAEQRAEQRAKFBF0giAIgiAIgiAIBYoIOkEQBEEQBEEQhAJF\nBJ0gCIIgCIIgCEKBIoJOEARBEIbR1dXFO9/5zrN6j29961u89tprADz44IM8//zzmRiaIAiCIAxB\nBJ2Qce688066uroA+Md//EcOHz6ckff985//zD//8z9n5L0EQRCyzdatW4nFYmM9DEEQCoSf/exn\nXHPNNdx1112u3+PMPZgwcfCM9QCE8cfmzZuT//29730vY+97+PBh2traMvZ+giDkL1u3buXrX/86\nU6dO5dChQ5SUlHDvvffy/e9/n6NHj/JXf/VXPPjggzz55JPs3r2b/v5+lFJ88Ytf5JJLLuEjH/kI\nCxcu5LOf/SxbtmzhwQcf5Gc/+xlTpkwZtc9f//rXfOMb36CkpIQLLrhgyGs/+clP+NGPfoRlWVRW\nVvLoo49y7rnn8uCDD6JpGkeOHKGrq4urrrqKRx55hHXr1rF3716+8pWvYBgGADt37mTVqlWcPHmS\n8847j6997WuUlpZm9ToKglA4vPLKK3zyk5/k5ptvdv0eZ+7BhImDCLoJxNatW/nGN75BXV0dhw4d\nIhKJ8M///M9ceeWVo55z5MgRvvSlL9HT04Npmvzd3/0d/+f//B/6+/t56KGHOH78OLqus3DhQr7w\nhS/wuc99DoA77riD7373u6xevZpvfvObDAwM2G7OHn74YSzLGnGDNnPmTP71X/+V3t5eHnroIb78\n5S/z0ksv8f3vfx9d15kyZQqPPvoo55xzDg8++CA9PT00NjZyzTXXcO2117JmzRosywLgnnvu4a//\n+q9zcs0FQXDPnj17+OlPf8qCBQv46Ec/yne/+11eeOEF+vr6WLFiBX/9139Ne3s7L730Erqu893v\nfpfvfe97PPvsszz99NPccsstXHrppTzxxBN87WtfSynmTp48ycMPP8yPf/xj5s6dy3PPPZd8bdu2\nbbzyyiu8+OKLlJSUsGnTJu69917Wr18PwJtvvskPfvADvF4vd955Jy+99BK33347v/rVr1i9ejU3\n3HADv/3tb2lra+OFF17A5/Nx66238utf/5r3v//9Wb+OgiA4J9cPlZ588kn27NlDU1MT3d3d3Hbb\nbXz1q19l+/btmKbJggULeOSRRygvL+d3v/sdzz33HJFIhK6uLt7//vdz33338dBDDwFv34NdeOGF\nALz73e/mm9/8JlVVVaxevZpzzz2X5uZmvv/979PU1MRXv/pVBgcH0TSNe++9l2uvvZaOjg4eeOAB\nuru7Abj66qu57777cvMhCOmjhAnDG2+8oebPn6/279+vlFLq+eefV6tXrx71+Gg0qlauXKn27t2r\nlFIqGAyqG2+8Ue3cuVP9/Oc/V3feeadSSqlYLKY+97nPqWPHjimllJo3b57q7OxUSil17bXXqj//\n+c/Jvvft26eUUuquu+5SH/rQh1Q4HFadnZ1q4cKF6sSJE+pPf/qTuvfee5VpmkoppZ577jl1zz33\nKKWUevnll9Xdd9+tlFJqy5Yt6vrrr0/28/LLL6sbb7xRWZalHnjgAXXHHXck5/H3f//36tVXX1VK\nKXXgwAH1+OOPn/3FFAQhq7zxxhvqhhtuSLY///nPq69//evJ9uWXX67eeustdeTIEfXiiy+qNWvW\nqFtuuUXdfvvtyWN++9vfqne+853qW9/6lm1/v/rVr9SHP/zhZLu7u1vNmzdPKaXUU089pa666ir1\nvve9L/mzdOlS1d3drR544AH1n//5n8nz1q1bl1yzbr/9dvXLX/5SKaXUAw88oJ599tnkcZ/97GfV\n888/7/SyCIKQI9LZt/zxj38cdc/S1tamli5dqn7zm9+oFStWqG3bttn2eeaa8W//9m9qzZo1yrIs\npZRSX/va19Rjjz2mLMtSt99+uzp69KhSSqkTJ06o+fPnJ/dDI+3BEiTajY2Nat68eWr79u1KKaV6\nenrUX/3VX6nGxsbke65YsUI1Nzerb33rW+rRRx9VSinV39+v7rvvPhUMBt1fWCEriIVugjFz5kzm\nz58PwIIFC/j5z38+6rHHjh2joaGBhx9+OPlvoVCI/fv3s3z5cr7xjW/wd3/3dyxdupQ77riD2bNn\np+y7traWBQsWAFBfX4/f78fn81FdXU1ZWRmBQIBLLrmESZMm8eMf/5jGxka2bt1KWVnZ297r9ddf\nZ+XKlVRXVwPwt3/7t3zpS1+iqakJgMWLFyePvfHGG/nCF77A//7v/7J06VI+9alPpXm1BEEYS3w+\n35C2xzP0K+sPf/gDP/jBD/jIRz7Cddddxzve8Q5+8YtfJF8/fPgwU6ZMYc+ePbZ9aZqGUmrEvizL\n4uabb+Yzn/lMst3e3s6kSZMAki6VAEopdH3k8PQz33N4f4Ig5B92+xa/389999034p5l6tSpPPHE\nE3zsYx/j3nvv5bLLLnPU9+9//3t6e3vZsmULANFolMmTJ6NpGs8++yy///3vefXVVzly5AhKKQYH\nBx29v8fjYdGiRQDs2rWLjo4OPv7xjydf1zSNt956i+XLl3P33XfT2trK0qVLuf/++/H7/Y76ErKP\nJEWZYBQXFyf/225DYZomFRUV/Nd//VfyZ926dXzgAx+grq6O3/zmN9x999309fXxkY98hF/96lcp\n+7bbnEF8AbvnnnsAuO666/jwhz884nuNNG6lVDIBwZlxKatWreIXv/gFV111FZs2beJ973sfvb29\nKccqCEL+87vf/Y5rr72W2267jQsvvJDXXnsN0zSBeBKlF154gZdffplgMMh//ud/pnyvJUuWcPjw\nYd58800gnpwgwVVXXcX//M//0N7eDsCPfvQj7rjjjuTrv/zlL4lEIoTDYX7+859z7bXXAnGhJ0lR\nBKFwSeehUqo9i5OHSsOxLIuHH344uf/6yU9+kgxhueWWW9i3bx8LFizgs5/9LB6PZ9T93Jn/HolE\nhswtMR/TNDn33HOH7Pdeeuklli1bxkUXXcRvf/tbPvShD9Hc3Mytt97Kn/70J8fzEbKLCDphVM45\n5xyKior4r//6LwBaW1t573vfy969e/nhD3/IQw89xLJly/jMZz7DsmXLOHToEHB2m5jNmzePukE7\n832XLVvG+vXrk5mcXn75ZSorK0e0Eq5atYoDBw7wt3/7tzzxxBMEg0ECgYCr8QmCkD88/PDDbN++\nnZtuuokPfehD1NXV0dTURG9vL5/61Kd45JFHmDZtGmvWrOGZZ55h//79o75XdXU1X/3qV/n0pz/N\nLbfckrT2Ayxfvpx//Md/5M477+Smm27i1Vdf5Vvf+haapgHxB2W33XYbN910E0uWLOEDH/gAANde\ney1PPfVUSk8IQRAKl0w+VBrOsmXLePHFF4lEIliWxaOPPsrXv/51jh8/Tl9fH/fddx/vfve72bZt\nW/IYGLpXqq6uZu/evcBpK9xILFq0iOPHj7N9+3YADhw4kIxR/upXv8ozzzzD9ddfz+c+9znmzp3L\nsWPH3FwuIYuIy6UwKj6fj2eeeYYvfelL/Pu//zuxWIxPfOITLF68mPnz57Nt2zZWrlxJSUkJM2fO\n5O///u8BuOGGG7jtttt45plnHPe5atUqPv3pT3PTTTdhGAZLlizh17/+NZZlcckll/Av//IvfPzj\nH+fb3/42//AP/8Add9yBZVlUV1fz3HPPjejq9OlPf5onn3ySf/mXf0HXdf7f//t/1NbWnvX1EQQh\ne1xxxRW8+uqryfbwkiVbt24FeJtYeuSRRwCS9d8g7iqVOD4VV199NVdffXWyfaa7+erVq1m9evWI\n573rXe8aMc34HXfckbTk3XLLLUNeW7Nmje14BEHIbx5++GEeeOCBt+1ZRnqodOutt3LZZZclXTjt\n+NjHPsZTTz3FLbfcgmmazJ8/nwcffJDS0lKuueYabrzxRioqKqivr2fu3LkcP36c+vr6IXuwT3/6\n0zz++OO89NJLLFy4kIULF47YV3V1Nf/6r//KV77yFcLhMEopvvKVrzBr1izuuOMOHnzwQd773vfi\n8/l45zvfyXvf+95MXkYhA2hKnPgFQRCECcC///u/89///d8jvnbXXXfxvve9z/F7Pvjgg5x33nln\nVTdKEARBEM4GEXQTnGxscARBEARBEIShyJ5LyBYi6ARBEARBEARBEAoUSYoiCIIgCIIgCIJQoIig\nEwRBEARBEARBKFDyJstlR0f6dcGqqkrp7h7I4mikb+lb+s5E3zU1hV98VNam/O9f+pa+nTIe1iYo\nnPVpNPJxTJCf45Ixpcd4GJOb9akgLXQejyF9S9/S9zjtu5CZyJ/ZRJ279D2x+i5k8vG65eOYID/H\nJWNKj4k6poIUdIIgCIIgCIIgCIIIOkEQBEEQBEEQhIJFBJ0gCIIgCIIgCEKBIoJOEARBEARBEASh\nQBFBJwiCIAiCIAiCUKCIoBMEQRAEQRAEQShQRNAJgiAIgiAIgiAUKCLoBEEQBEEQBEEQChQRdIIg\nCIIgCIIgCAWKCDpBEARBEARBEIQCRQSdIAiCIAiCIAhCgSKCThAEQRAEQRAEoUARQScIgiAIgiAI\nglCgiKATBEEQBEEQBEEoUETQCYIgCIIgCIIgFCgi6ARBEARBEARBEAoUEXSCIAiCIAiCIAgFigg6\nQRAEQRAEQRCEAkUEnSAIgiAIgiAIQoHisTvAsiwef/xx3nrrLXw+H1/84heZPXt28vUNGzbw7W9/\nG6UUCxcu5LHHHqOvr49PfvKTDAwM4PP5ePrpp6mpqcnqRARBmFjI2iQIQj4ia5MgCLnG1kL32muv\nEYlEeOmll7j//vtZs2ZN8rW+vj6efvppnn32WX7yk58wa9Ysuru7+dnPfsa8efP44Q9/yMqVK3n+\n+eezOglBECYesjYJgpCPyNokCEKusRV0O3bsYPny5QAsWrSIvXv3Jl/buXMn8+bN46mnnuK2225j\nypQpVFdXM2/ePPr7+4H44uXx2BoCBUEQHCFrkyAI+YisTYIg5BrbFaOvr4/y8vJk2zAMYrEYHo+H\n7u5utm7dyiuvvEJpaSmrV69m0aJFVFVVsXnzZlauXEkgEODFF1+0HUhVVSkej5H2wGtq/Gkfm2mk\nb+lb+h57ZG3Kr77Hun/pW/rOF3K1NkFhrU+jkY9jgvwcl4wpPSbimGwFXXl5efKpEcR9wxNPjior\nK7nwwguTft5LlizhwIEDrF+/no9+9KOsWrWKN998k3vvvZf//u//TtlPd/dA2oOuqfHT0dGb9vGZ\nRPqWvqVvZ++TLWRtyp++x7p/6Vv6dvMe2SJXaxMUzvo0Gvk4JsjPccmY0mM8jMnN+mTrcnnppZey\nceNGAHbt2sW8efOSry1cuJCDBw/S1dVFLBZj9+7dzJ07l4qKCvz++GAmT548ZGETBEHIBLI2CYKQ\nj8jaJAhCrrG10N1www1s3ryZVatWoZTiySefZO3atdTX13Pddddx//3389GPfhSAv/mbv2HevHl8\n4hOf4JFHHuGHP/whsViMJ554IusTEYSJTCQSY+3a3Rw+3MXcudXcddfF4z4GQ9am/CASg7X7vDSH\nYFaxl7sWRhnnt54gpETWJkEQAGLE2FO8kxC9FBf7uTB0CR576eUKTSmlsvLODnFqiixkVw/pW/rO\ndN/PPbeD9esPJ9srV87lnnsW56Rvu/cpdGRtSs1zu72sP+rB5/UQicZYeU6Mey6O5nQME/G6S9/5\n63KZSwplfRqNfBwT5Oe4ZEzpkU9j2lm8nb8UHcTrM4hGTN4Rnscloctsz3OzPslzVEEYBxw61Ekw\nGCYcNikqMjh0qHOshyRMEA536ynbZ4tpwdZWg+Y+jVnliitnmOiZ7UIQBEEQMk63cZKwHiKCQuka\n3cbJrPUlgk4QxgEej04gEAYgFIrh8ciOV8gNc6ssDvboQ9qZZGurwaaWeBa/o8H4vy2dZWa0D0EQ\nBEHINBo6YS2MgYapKTT71CWuEUEnCOOAiy+eytGjPXR2DjJ5cgkXXzx1rIckTBDuWhh3r2wOeZhV\nHEu2M0Vzn5ayLQiCIAj5SE1sGgGji6gRwRvzUROblrW+RNAJwjhg9uwqFi+eOaQtCLnA44F7Lo5S\nU1NMR0fmY+dmlaukZS7RFgRBEIR8p9KqZkasltKiIgZiYSqt6qz1JX5ZgjAOuPTSaXR1DfDGG010\ndQ2wZEn2ngIJwpmYFmxpNvjBn+O/rcx6XHLZNJNJPkVHv8Ykn+LyaeJuKQiCIOQ/U6PTGdAGOMZR\nBrQBpkanZ60vsdAJwjjgP/5jN2+80Uw4bNLVNcjzz+/m//5f+0xKTjBNi61bm2lu7mXWLD9XXjkL\nXbJTTHi2NBusO+ghooFPebAsWFaXOdH1xgmDvScNeiMQMg3eOGGxrFZEnSAIgpDf/Ll4B8d9R7Aw\n6fR14jcrWBJ6V1b6EkEnCOOA118/RlNTkFjMwuPR2bjxWMYF3datzWza1AjA0aM9ACxdWpfRPoTC\nY0NjXHCZGhjKYFqJyqig29BgsOekRtjUKDIU00oMEXSCIAhC3nPQd4A+vQ+FhabrHPQdEEEnCOmQ\nbStSvlqpTpzop68vilKgaSYnTvRnvI+jR7vZsaMlmXilrq5cBJ1AQ1CjsVfDVGBoGo3BzCYtOd6j\ncaxHJ2ppeHXFcb+9T6eUOhAEwQ0WFi3eJlqIYnm9zIzWokt00tuQ65QefUYQpcW/s5Rm0WcEbc5w\njwg6YVyRbStSvlqpolELTeOUoIu3U+FGmO7efYI9e9oxTUVLSy+zZ1fw4Q9flMlpCAVIV0gjZmko\nDZSl0RnKrKA7EtQJWxoWYFkafwnabxoSbqC9EQ2/T2XcDVQQhPFJi7eJZu9xSiliwBsvBVQbrR/j\nUeUfTd4G3iraA1hQpGNhUR+dM9bDyjtiWixlO5OIoBPGFc3NvSnb+f7+bikv92IYOpqm0HWN8nJv\nyuPdCNOWlj40Lb5Z1zSNlpa+DIxcKHSKPVDqVSg0NBTFGf5W6YtqaFo8g5cG9EbtBePrzQYNvXHh\n1x3W2NhsZFzQJayAva3gtwyxAgrCOKBPD6ZsC3GavMcIGkG8GEQNkybvMRF0I6CUStnOJCLohLwl\nYUXq7Y3h93vSsiLNmuVPCpREO5Nk+/3B3bznz5/Mnj0dWBZommL+/Mkpj29oCNDYGKC3N4Lf76Ox\nsQJILejKy4uGtP3+olGOFCYSl9WYbD2hEzahyFBcXpNZ4TSjxKKhT8ciLupmlGQ4jaZLEgXPy0qh\nfyBe+FwKngtCYVNuVRAwuoe0hbdjYtKrB7Aw0XWDqWQve2MhoyktZTuTiKAT8paEFamsrIj+/rjr\ng50V6corZwEMcSXMJG7e36l7o5t5DwyY+HweolELr1dnYCD1xrKra5A9e9oJh02KigwWLJhiO48L\nLpjC5s2NxGIxfD4vF1xgf44w/mka0IgpHaVBTOk0DaT+worEYO0+L4e7deZWWdy1MIonxTdRqUel\nbI/EilqT1n6d3gj4ffF2ppGC54Iw/pgZrQVAJ0pV1JtsC0OJatFT7oMKS1NEtczXIB0PePBiEhvS\nzl5fgpCnuHFv1HU9qzFtbqzlTt0b3cz7+PEA0aiFZSmiUYuGhkDK47u7Q2ialvzp6grZ9tHVNUhx\nsQeloLjYQ2fnoO05QmHhJpnIjjYDS8WtZ5aCP7YZKY9/fo+XdQe9hM34scqCf7pk9M3AiZCOV+dU\n0pV4246lM010jSHzyDRS8FwQxh86OrXRemrw0xHNj5CKfMTCxKu8mJgYysBCvBNGImwMpmxnEhF0\nQt6SC/dGp7iJPXMq0KZPL2fjxgYiEQufT2fpUvsnhJGIiWlaWFbcRzscTr24appGRUXRkLYdDQ0B\nOjr6iUYt+vp0GhtTi0ah8Ei4EQJJsZKOG2HEBEU8xs3uTtrSYhCIxI8KmbC5xUgp6FAQseLvb6p4\n2w5dz777Y0Ik9urgrzSzIhoFQRDyE41BfRBQoKez8k9QEl+MZ7azhAg6IW9JuDOeGUs21rixnjkV\nppZl0t7eRzAYoaLCh2XZbxQNQyWth0rF26lYtqyW3bvb6OoapLq6hOXL7UXjsWMBIhELpSASsTh6\nVATdeMONG6HfozhTZ5XbuESWexXhGMQUeLR4OxXKOv3e6lQ707ixTCZEY00NdHSImBtLpEyFIOSW\nkDaIiRmvr4ZOSBOPnREZ/hWaRd0rgk5wRS7qsSXcJ2tq/HR0pOf6kO1xubEaOo2727y5iVDIpLy8\niFAoxqZNTaxYcU7Kc0pLfWgaWFZ8o1lW5kt5vK5rTJ1aSkmJB7/fl5aFLlG0XCmFpmnEYvmRnELI\nHG7cCEs9UKwrYmh4UJTafKssrLbY1KyImRo+j2Jhder7yAK8p37rp35nGilzUNi4tSwLguCOXj3I\nmY/aeiUb6MhYgDGsnSVE0AmuyNd6bE7H5VQAukmK4jyuz/kjnWAwjHlq/2KaEAiEUx7f2tpPXd2k\nIW07rriilo6OQWIxhcejccUVEiw+3ki4DTqJPdMNRURpWAosTbO1DvdENCaXKspPZcXsiaS+v+v8\nFoeDRtIKWJ9GYXGn5KLMgZA9JEGNIOQWpVlYWnxVVhrJ4tnC2CGCTnBFLuqxuUnf73RcTgVgtpOu\nAKxYUU9ra18yhm7FCvuipo2NwZTt4bixND7++DJOnOjj8OFu5s6t4vOfX2Z7jlBYuIk96wzpydg5\nDThpk7RE12FSESSe7toZ0Iv0+EPNxHbBlwVXOqUgGNZOlV5wl/wo04gbYfpIghpByC2aiq/7ydhp\nJYvTiIjLpZDv5CJhiZv0/U7H5aYeW7ZZurQWXdccxQ4Gg2bK9nDcWBr/+Mc2KiqKOO+8avx+H3/8\nYxvLls22PU8Y3wRC8Xg4RTzLZcAmYarTkgK7TsY3CvqwdiqclkaYXKxStseC8eJGmAth6sayLAiC\nezx4QGloGqC0eFt4O8O/SiQpipBvZLveW+K9U7UzMa5AIExDQ3y31N0doqcntatiLnATO+iUWMxi\nz552Dh/uYu7cai6/fIat9XPDhmNDatdNm1Yqgm6c4WbzHTWHJi2J2uylnZYUMHQt6W6pnWrbsXaf\nl/VH419vB3viE7jn4tEzaVYVKy6YYiZj6KryQNDlqxth4h7pbQW/ZdjeI7kQprnIalrIWFi0eJto\nIYrljddW0xGLiuCeMqscj/LEk6IonTKrfKyHlJ8M/zPL4p+dCLoCwI3rYbbJhUuSGyugU5fIqqpi\n6usn0dsbxu8voqqq2NVYU5GLBDIeD8RiQ9upeP75Xaxbt59w2OSPf2xFKcU//dOSlOc0NfXS3j6A\naVoYhk5Tk9ToGW+42Xz7i8EXjq8JmhZvp8Lp0jG11KKpX08mRZlaah+rsb9N53A3DMZ0SjwW+0tT\n/73VVSjq+k7n6qyrsB+lU2HjlHx1I0zcI2Wl0D8Qv1dS3SP5KkwnEi3eJpq9xymliAFv/KFlbdTe\nlX8iIuI3PfzWJIpVMRYmujLwW5PsT5qISNkC4UzcuB7makyQflIUtwlIslm2oLa2grq6iiHtTLNl\nSyPr1u1PunValpVxy9bcuZN4883AkLbdmBKJU0KhGJs3N9oKOtO06O0NJ7NdWpYEQY833Gy+l0wz\naerTk1kuL5uWWgA6FY3nlCv2dypCFhTr8I40hM3hoEZ32EApCJkGh4Opz3HjspfIjBnRwKc8Gc+M\nedk0kz0n9aTb6OU21zVXOL1H8lWYTiT6hmUgHN4WTiPiNz2KrWJMLUYME4+mKLYy/zB8XCAxdMKZ\nuHE9dCqeIpEYa9fuTrrg3XXXxXhSmHncjMmpCMyFFdCpi6Yba9uGDQ3s3duRdFWcPr0hpaBzY5G9\n6aa5vPXWjqSV5Oab56Y8vqzMQygUS1rbysrsl4K//KUb04w/bjJNxZEj3bbnCIWFm833smkmP3zT\nS5R4eYFrbIRHY69GY/B0DF2TTdbKnihElIYGRBR0p6hBnqA/osGpuD5NnWqnwM1Sk8iM6fNCJKpn\nPDPm9rZ4AfaaMkUgorGtzbC1luYiXs3pPZKL+DZJIJOacquCgNE9pC2MjIjf9GjztBLSwigsYppO\nm6d1rIeUn4iFTjgTN66HTsXT2rW7Wb/+MAAHD3YBcM89izM6JrcZKLNpmXQqGt1YJpubg0OsYU1N\nqb8gtmxpYt26/cksl5alWLYs9RPCH/1o/5DC4j/84X7uv//qUY+/6KJp7NjRxuBgjJISDxddNC3l\n+wNEoyaaRrIOXdQuWEoYc5xudN1svh/e4iN66hsrisZntvhYuWD0IrNdgxp7T+rJjJILq1OLrZMh\nPX7fEX9YYZdFE8BjxOOq9DPaqchFnJfTz8Kp8IXczCNxT/Tq4K80be+RXMS3jZcEMtliZjReYkYn\nSlXUm2wLb6fUKqfRe5Qg8aKuM6JjX44pHzlptGNp8b8xpZmcNNrHeER5iljohDNx43roVDwdPtyV\nsj3amJwkRZk+vYyNG48nXQ+XLk19TmNjkMbGYFLY2AkhcG5BcyrQjh7tZseOFjo7B5k8uYS6uvK0\nMm9OmlSUtNDZid948pE2TBMMg1PJR1ILupaWoRvo5ubRN9QAkyYVU1VVjGUNUlVVTGWlvbvEnDlV\nHD/elxR0c+ZU2Z4jjC1OC2a72Xx3RfWU7eF0DmooFIr475ODNtYzderByymLWzpPOK+abnIkoBM2\nNYoMxVXTU8/JjavpVTNNdrcbBCNQYcCymZl1NXUqfAEaghqNQS35eTeWa5BhT/XEPVJTAx0d9vdK\nLqxnuZgZRqcPAAAgAElEQVS3MDHQADQNDQ2ladncfxc0g/pgyraQe0TQFQBush46taDNnVudtMwl\n2umMyQmalqhUBaerVo1OT0+IhoYAPp+HSCRGd3eNbR9OBdqxYz1DBFp9vT/l8Xv2tHPoUNx1pasr\nxO7d7Xz4w6nHtHx5PXv2tCf7sKsr19LSSzAYwTB0TNOipcX+Mx9uabSzPO7ceYKjR3uIRi2CwTC7\ndp3gttsuSnnOjTeey4kT/cl53HjjubbjEjKHm41xLgpmezWFqbQh7VQ4rUNXU2JhKSOeFEXBlBJ7\nS1UIKPWCz6PwaPF2KqaXKjY2nRYES6fbq0Yt+X/x33Ybv2MBjR2tBp0hmFwM9WUWqZ5pORW+AIGI\nNuTz7omMfZxrLqxn+TjvfKLBe4xdJVuJEsFb4mMRVzAn+o6xHlZe0qf3pmwLcYZLXZG+Y48IunGK\nUwvaXXddDDAkhi7TtLb2DUlA0tral/L4ysoi6usrkha6ysoi2z4SVr1E1ko7q97u3W3s3dtBLKZo\nbe1jzpxKVq26cNTjYzFriLUtFrPfOBiGxtSpZZSUePH7feg2addnzCjDMDTC4RhFRQYzZpTZ9lFU\nBKHQ0HYq9u/vIBKxUEoRiVjs3dth28fs2VVcf/07hrSF3JGvbmXXzzJZ33Q6C+X1NmNyWoeuY2Bo\n4fKOAXvzjmlpTC1VSeuWadlbAdsHdLpCGtXFKi1X7NebDdoHNEwNQpG4WF5eP/pc/nxS51AgPvau\ncLye3qoU7+9U+AJUFSnq/Sp5bauKxj4BSS6yXObjvPOJN4v20O3pxkCjz9PPm0V7RNCNwqA2QIfR\nBigwNCZr9g+SJyJFFBElPKQtjC0i6MaAXKSxd2pB83g8KWPmMoFTq+HMmUNfTydOL2HVg3hdOTur\nXnNz8NTmTaGURnNzagF47rnV7NhxItm2s2QCb0vvb+f+WlNTTnm5j5IShWFo1NTY13eZNaucI0f6\nhrRToWkaHo8+pG1HLmoPCqPjZmPsVDy5sQL+w0Ux+pVG0PRSYcT4h4tiKY93Woeu3xwaD9efhoY9\nr9pKiqdEOxW/O26w76TGYEyj1QP/ezy1OIP4+AMRDUOPC0a7zyMS0/BoioGYRqlHEYmlPt7pZwdQ\n61fUVVhD2mNNLrJc5uO884mwFiaejzBu8Q1rY19vNV8Z0PtP5bGIX6sBvX+sh5SX1EXmcKj4AEqL\n16Gri8wZ6yFNeETQjQFuEmtkG6ci040odSoITrtonvmTmoRVLxGnZ2fVq6goprjYM6SdivPPr8Lr\n1QgEopSXezj/fHsrldPi5dXVJVxwwdSkZbK6usS2j6VL62lpeZNYTOHxaCxdmtqt8z3vmUt7+5+T\nSVHe857UWTHBnZutkDncbIyvmG6yr/N06vsrbWLJ3FgBL59hsv64wbE+mFNuZTxJxsJqixMDOjEL\nPHq8bcddC+OpMBPzTrRH4/UWg55IfP0KRzReb7HJogLMLFNU+CxMdAwsZpal/jx8HkVMafgM4r89\nqY93KnwhNxklnZKLMeXjvPMJv1VBhxZ/EKlp8bYwMjoGxaoYLwZRFf/rFt7OOZG5NHsbiOghfGYx\n50Ts9xATEguG3EJZ9AYXQTcGuEn575RsJwdxI0qdZpRsbAzS3t5PMBihosKXVlKUurpJ1NUFh7RT\nsWrVAjo7B5JxYatWLUh5/B/+0EJJiY+6Oh8AW7a0cM01qWPJnBYvT9TGS2T3TKc23uLFM9mz52Ry\nHosXz0x5/N13X4phGFl1sRUyi5tN6xutBntP6vRGNEKmzhutqWPo3FgB1+7zsrXVg6lBW6+H/9in\n+KdL0qgtkCar58foiWh0hjQmFytWz09tAQTweOCei9MfgxX3rkpmmLbSWKtW1Jrs6TQIxqDCA1fb\nWNAWTbE42mMlY+gWTUn9ze4mQU0+2qVykeUyH+edT0yLzSAY7SFWFMET9TEtNmOsh5S31EVn02/0\nomGhLJ26aGZrxo4X2rwtlCs/XiqJKpM2bwvnRueN9bDyj3zKcmlZFo8//jhvvfUWPp+PL37xi8ye\nffoG37BhA9/+9rdRSrFw4UIee+wxLMviy1/+Mnv37iUSiXDvvfdy7bXXZm8WBYablP9OcSq4nIrM\nXNSh2737BIcOdWEYOm1tfezadYJVqy5I2YdTK+C73lXLgQOdSWGzdKldOmfnf51Oi5dfdtkM9uxp\np7m5j1mzyrn8cvsv30AgQjRqUVTkOZXoJJLy+Fy42GabibY2udkYO02K4sYKuKXFGOJ6uLnFyKig\nW1Zn4vFk1/py2XSTtkGdmAKPFm/boWswtdSiUgOfsrDzWq6fpFh8xtjrJ2VehmxpMVj3ljfppmkp\nWJaGq6YTEm65va3gt4ysZK106vqbb/Gl+bY2+a1JlFllQAlYOn4r9YPOicysaD0aOnpxFCskJR5G\nJ4dKRUgLW0H32muvEYlEeOmll9i1axdr1qzhO9/5DgB9fX08/fTTvPDCC1RXV/O9732P7u5ufv/7\n3xOLxfjxj39MW1sbv/zlL7M+kUIiF7FITgWXU5GZizp00ahJRYUvmb4/nbpnTt0C33ijmb172+nt\njRAKxXjjjeaURb+XLatl9+42uroGqa4uYfly+8Xe6ee9fXsrgUCYadPKCQTCbNvWajunzs4BlFLJ\nn5MnB2zHVejI2pR53FgB/T6Vsn225MLC88TSCJoOB7sM5lWbfOHK1A9EAJocWjMvm2ay5+Rp99fL\nbQqwu+H1JoOG3vg4usOwscnIuKBLiKeyUugfiIuoTH8+TgVaLhKvOCHf1iZJxZ8+FhYdRhsheik2\n/EyPzkQnw08sxgFiyUyTfLLQ7dixg+XLlwOwaNEi9u7dm3xt586dzJs3j6eeeorGxkZuvfVWqqur\n2bRpE+eddx533303SikeffTR7M2gAHEqOhLuk2fWobOLV3MquJyKjksvncYvfvEWBw92MW9eNUuW\n2Bemnj69nI0bG5Kuh3bWsHnzpnD4cE+ybMG8eVNs+3DK6683DIlv27ixIaWgU6cKYg39nRqnn7cb\n66dlKfr7owwOxojFPGmNyym5SObjBFmb7HGaWMONeHr/OTE2N+t0h3QqvBYfOMfeJTLbOLXw+Hyw\nZoW9iDuTrpDGnpM6JmCgs8Amtm97W9ySWVOmCEQ0trUZY56lNBKLu8yeGWvosdkV5EI8Oe3DTdmJ\nbJJva1NA76HTOEmEED6jmIDeg0RDj8ye4p38pehgPIauqBWAS0KXjfGo8o8Z0VpOGh2EinspjvqZ\nIZbMkbE0MNTQdpawFXR9fX2Ul5/OmGcYBrFYDI/HQ3d3N1u3buWVV16htLSU1atXs2jRIrq7u2lo\naOC5555j+/btPPTQQ7z44osp+6mqKsXjST/4tKYm826K+dr3hg3H2LmzPdmurCzl6qvnpDznppvO\np7KylMbGIHV1FSxfXm+7+b755tRuGGfO+5vffIMjRwIYhsGRIwF++tNDfOITV6Y8v6qqhOJiD5GI\nSXGxh8rKkpTX8jOfWUpb2wD793ewePFMPvvZpfh8vpR9OKWsrAifzzOkPXxMZ7Z37erAsjSqqkqx\nLNi5s4MPfCB1/TannH/+VLZta+XIkR4qKopYuXKe7T2nFAwOxjBNxeBgDMs6+/t0+PmvvXaE73zn\nT5w8OcCUKaX4/UW8+91jV4tO1ib7vm+eDNVV0BiEugpYXu/NuHvcH7bFv6OKvWBpHjZ1eVg1Bpm+\nz5z7hmOw89TzrPYeqKyEVEumacGmhjOvk32ZgLABXg9YJngNDyHDQ03N6PGxva1QVnpGW4eaDFyn\nM+f9voXQbUIwDBVF8L6FHmpqRk8M9fU/wE8PQ9iEXZ1QVlbMJ9+Vur/z+2DbSTjSCxVFRayclZl5\njNRHYh4j9XHmvKv6oLgYIhoUF0FllS/jY3JCrtYmSG99auEYPXSiUAx4+2nxHuNalp/1PDPFWK6Z\nwxkgQIwIYWIYPg8DvgA1/vwZX75cq6McxYOinHLwK0L+Hs7hnLEeVpJ8uU5+yunl1EN5Dfye8qyN\nzVbQlZeX099/Om2rZVl4Tj3Cq6ys5MILL6Tm1Mq5ZMkSDhw4QGVlJddccw2apnH55Zdz7Ngx24F0\nd6fvIuakwHamyUTfTi0d+/a10d8fTibJ2LevjQULJtv2s2DB5ORxnZ1nl3p3+Lx37mwhEokNadtd\nlwMHOqipKaWmpjTZvuCC0b91t2xppLjYYPny2fT3h/nlLw9nPMviJZdMZcuWxqQL5aWXTh0yj+Hz\n7u+PDJl3f38k4/diINBPKBQBFKFQhJ6efts+AoEQpaUeBgZilJZ6CARCZzWuke7zZ5/dzr598QcL\nbW19PPPMdi68cKrt+2QLWZvS63tBWfwHoLMz8/3ubinBq+sU6zqmZbG7xaKjY3DU492URrBj+Nz3\nNXnoP6Ne3b4miwVlo1sOtzSfdvHbDfT0mLbWs4F+H7GYBzSDWMxkoD9GR8foVj6/ZSRdFAH8lSYd\nHWdnoRs+7wVlcMuc09d2QZlJR4oSk79+q5jOU2PqA/6/t0xun5u6DHug2yAU8oDmi69P3TE6yjNc\nKPxUH+GIRkipt/Ux0ucdPx5CCvY3xbigPLWleDysTZDe+tQ+qQPTY6HpcW+O9lgHHYH8KJg9lmvm\nSITLYvQU96B0C83SqQnNoKM/P8aXT9eqqegEA0aY0rIiBvrDNJknKA9n3ovKDfl0nUJVkXiWSw1Q\nEDIjdHTbj83N+mT7NXrppZeyceNGAHbt2sW8eaez2CxcuJCDBw/S1dVFLBZj9+7dzJ07l8WLF7Nh\nwwYA3nzzTWbMkIxKZ5JIDnL0aA+bNjXyxhvNKY8f7i6ZjSQqThlefy2demxO55GLbKC6rjF1aimz\nZvmZOrXUth7bihX11NdPSmauXLEidXkAgEgkxnPP7eAzn/kNzz23g1gs9UajtbWfurpJXHTRdOrq\nJtHamo4Yt2hr66e7e5C2tn40LfO5cbu7B+nuHqS9Pd5PIDD6pj0XTLS1KRKD53Z7+czvi3hutxeb\n2yhnzKs2U7aHk4iPOhrU2dRi8EZr5tOCD0/mYpfcxY0b4eRilbI9nMummUzyKTr6NSb5lG0MnWnF\nheZP3vKwpdnASuNP2qmjoZv4x+b+M0vIaLT0p75WbubhtI+esEZDr0b3Gb/Hknxbm0pUKdoZ/ytR\npfYnTVAGtAHMM/43oI3/eHQ3lFnlBI0AbZwgaAQos+zr5U5E1LBVeXg7k9ha6G644QY2b97MqlWr\nUErx5JNPsnbtWurr67nuuuu4//77+ehHPwrA3/zN3zBv3jzmzJnDY489xgc/+EGUUnz+85/P2gQK\nEadCJRHPdmYMXaZxajVMpLl3kvY+kb0xcY5d9sZcZANNiKcz26lYurQWXdccJbRZu3Y369cfBuDg\nwS6AlBkm3cz7L3/pIRIxsSywLJMjR3psz3FKMBgiFIqhVPx+6elJ/SQ/20y0tWntPi/rj8aX7IM9\n8b9NJ6n5s0UigcixviLmlEdtE4rkIgbLaQISN9k9K3yKqSUWQdOgwrCYZCOGnMbQucnc6PScW8+L\ncbhb52RIY0qx4oPn2T8lSIgnnxciUXvx5GYeiT4gntzFro9Kn6LebyVj6CoznJjHKfm2NtVF5tBl\nnMTSTXTLkCLQKejXg2gaaOigxdvC2zGx6Nd6iRLBq/kws1lgrYCJEU3ZziS2gk7Xdb7whS8M+bdz\nzz0dN/Oe97yH97znPUNe9/l8fPnLX87QEMcfTjfsWchv8TaclhRwk/beaUZJN0LWqTB1+lm4Ka59\n8OBJAoEQ4bBJUZHBwYMnUx7vZt4tLf0Yho5hnG5nGq/XoKzMmyxe7vWObcHVibY2He7WU7ZHIhvu\njcMxPPC+c016dfBbpm1SDTfiySlOxZOb7J49EY22QQ0TGESjJ5JadDgVsg1Bjcbg6UQfjeUa2CwF\nTvvweuCdkxUzIwq/D9vPDk6Lp4gGvmLLVjy5EfBOBVpdhaKuL56oKtEeS/JtbSpTfmrM6cQ8YTxm\nEWVq7L188hUDD5ZSaFp872VIueYRafE2YGoWRRQR1UxavA3Mib5jrIeVf2gqdTuDyJ06BjjNKJkQ\nW4kYOrAv4u2UXLg3Os0omRBPTvyhnQrTXJSQ8HqNZF24cNi0FUJu5l1b66e1tQ/TVBiGRl1d5r+w\n58+vobGxb0hbyB1zq6ykZS7RtiMX9bmcprF3I56c4lREuMnu2R065cCmgaY0ukKp+3AqZAMRbUgN\nwZ6I/efttI/Wfo26CmtI246EeIp/3spWPLnJQOlUoOXinipkQtog/XofGhZhPUpIG1t3+XxmWmwG\n/UYvpm5iWIYUYR8VqUOXDroysDCHtLOFCLqzxE0q91yksXdKLtwbc7EAOL1WbixuTlm0aDpHjwaS\niVcWLZqe8T6uv34Ohw51MzgYo6TEw3XXzUl5fCQSY+3a3UNcZj02j+e/8IUVAMlSFYm2kBv+YX6U\nowEtWSvtI/PtXTecChs3Fr1ciCen5MIKqGlQUaROuR4q28LiTkVHVZGi3q+SJSeqiuzn4LQPN9cp\n8Z69ejyxi10f8etyOh4unWXf6TzG1h6X/wzqA6cue7wG3aAucWGjMTt6LiF9ADwWRHVmR8cuk3M+\nI3Xo0qPcqiBodA9pZwsRdGeJU4uQG3IhtpzGt7lhxYp6Wlv7knXo7BKK5KL+Xi6or5/E4sUzhrQz\nzeTJZVxzzWx6eyP4/T4mTy5LebzTuD6IuwStWXN9ZgYsOGZHh0F1CVx5Sgz9scO+jtn0MsXGptN1\n6JbabIy3tBise8ubPN5S2BamzoV4ckouLDaJGn9x10OV8Rp/tX41xHpW60+n5qWzPtxcp0QfNTWk\nlaXTjRXQ6TxyYYkuZHR0ilRxvLaaMqVQdgpqo/Xo6OglUaywl5lSX21EZkXr0dDRi6NYIblOozN8\n3RaXy7wlF9azXCRF2b69lUAgTE1NGYFAmG3bWlMKUzeWSacJRbZsaWTduv1EIhY+n45lWSldNCE3\nLpROycWYZs50lkH08OGulG0h/3ATixR311ckcybbfJe83mQMSUaxscmwFXROLTa5IBdWwKUzTXQN\nenUvfiua8XnnQpS62VokrLi9rfFSDHZWXKcPFdyQi0Q7hcysaB3tnlYiRPAoH7OiUlZcODt0dGqj\n9dTgpyOaHyUC8pGwHk7ZziQi6M6SXFiE3MRUOcWpMHVjmXTq3piIufP5PEQiMduYOzd95IJcjMmy\nFO3t/XR2DjJ5cgmWlXqrNnduddIyl2gL+Y0bS1jrgHYq/kgl25nGqcVmvJDteedClJ5NJs10Yyad\nPlQA566/+Wglzid0dMpUedw9TulioUtBi7eJZu9xSiliwBvffNdG7csTTTQsLFq8TbQQxfLGLXRy\nX70d41TMnIaGQiXb2UAE3VmSjxYhNzgVprmwTE7UoFs3rqabNzcSCpmUlfkIhUw2bWpkxYo5ox7v\npuyEMLa4sdg43eiumGXS2n86gcUKcVsb17ixbDk9x2lNOXAuNCUpSmr69T4qzElxkWKG6df77E+a\noPQNK1MwvC3EEeGbHjWx6TQYf0FpFprSqYllPodCAhF0Z4kb64sbd8Vs41SYurFMOp13IuYu4XKZ\nThHvfLy2TnGX1dSZ+NU0nQsvnEp1dQmzZvkL7hpNRNxYbJxudJfOiltCsrkxzkUphfGAm+uUC8uW\n03Oc1pQDaOzVaAyedtNs8qfO8JkLa2YhU2qV0+g9ShALdJ0Z4nI5KnKt0qNXDxA0AvRioQydCjMw\n1kPKS2bEZtHha8UkhqE8zIhlz+gjgm4M2LKliXXr9ieTg1iWYtmysX2y4VSYurFMJmLiEok77GLi\nEjF3TmIHc5GkJtu4sX5edVUtu3efSLpcLluWOkB5PFyniYabDb7Tja6bjbHTmCpJYJEeuSgs7say\nlSja3twHs4qVbdF2N0W/3YhAYXTiiUbjZTaUpk0QXxd3yLVKj5AWIqgH4ol2dJOQFhrrIeUlvXqA\nqIpgYmIpi149e8JXBN0YsGHDMfbsaUsWmp42rXTMBZ1T3Fgmc1GHLjeuoNnFjfXTMHSmTi2npMSL\n319ka3EbD9dpopGvQmhLs8G6g554tkflwbJgWd3o45IEFumRC3dINwI+UbR9WjkEBuyLtrsp+u1G\nBAqjIy6X6SPXKj28youhdMKE8SofXuUd6yHlJSe8zZiGiYaGaZic8DZnrS8RdMPIhcteS0vvkELT\nLS0TZTOd/Zi4fCxb4PSeSpSQaG7uY9as8rRKSLS29lFXVzGknYp8vE5CavJVCL3ebNDQq5+qx6az\nsdlIKeicuuxNVBfNXLhDuiHhDhnpBZ/Sbd0h3VgBnYrAiXqPpEu5VUEgR7WwCh25VukR0cIMGANo\nKKJGjIiWveyNhYyJiYUCLEDDJHsPYUXQDcOpK5obAThrlp9Jk4qSFrpC3Ey7mXcu6tDlop6eU5ze\nU4kSEtOmladVQgKcC7TxksxnIjFeMvklXPYOd+vMrbJsXfby1TKZbdwIIafnuBFCXYMae0/qmBoY\nSmdhdeatgHKPZJbp0Zl0GG300Uux5Wd6dOZYDylvSdRT04lSFZX6aqMxqA+cTlqrpFj9aHiUl9Op\nfdWpdpb6yto7FyiNjUEaG4NJ0dHUlDrDkZtYpKuvnsOJEwPJPq6+ek7K490Im2zjZt5XXDGTffs6\nkmLryitTf6m4SQ7itJ5eLnDq3ujGHdKpQMvH8g5CavI1k9+V001+32DQ2g+VXlg6PfW4Ei57NWWK\nQMTeZS9fLZPZxo0QcnqOm0LynYPx9NtKgUJxcjDzn4fcI5ml1dvESW8bGhZ93gFazSbqonPGelh5\nidRXSw8NbUixeok2HJnhLrvZdOEVQTeMnp4QDQ3xoMXu7hDd3TUpj29oCNDYGEgm+mhsrABSb5Sd\nFth2l/Uwu7gRHU7Flps+sh0b5tYi68R65sYdUgTa+OdsEpZk0xXtYI9OVGkUeyGqNN7s0bkmhVuJ\n0833eLFM5qNboJtC8roOk4o45WJLVuYwUe+RbNHoPT4kgUWj97gIOuGsqI3OoV/vBSxKTJ1auZ9G\nJKpFUrYziQi6YVRWFlFfX5EUaJWVRSmPDwTCQxJ99PTY+xE73XznYwILN6LD6Tzc9DF9ejkbNzYk\nrZ9Ll2bWXcKNZdKp9SzxupPsnoIwErlwRTvSo1NRpE5t8BVHemwecDjcfOerZdIp48UtcNlMk90d\nBsEIVHhg+czMz2Gi3iPZY/j1E8ErnB210Xp0dPSSKFZYXFNHY7jlMpuWTBF0w5g5s4Izi6DOmpU6\nILaqqpj6+klJAVFVVZzxMbkRNpFIjLVrdw8pHO3xZO7jdhOD5TbOy4mw0bTTgfTx35n94nIjrp0K\neDfZPQVhJJzW83JjRZpbZXHwDBE3tyqzSTLGS40xp1anXFj03BSS1zWYWmpRqYFPWWhZ2J9M1Hsk\nW8yK1tPuOUGECB7lY5YUgBbOEnFNTQ+fVUzUiAxpZwsRdMNwKghqayuGZBesrc18RiQ3wmbt2t2s\nX38YgIMHuwC4557Fox7v1JXQjYuf2zgvJ8KmtbWfurpJQ9qZZCJnhxwPRdsLGTcbfKf1vNxYke5a\nGAWgOeRhVnEs2R6Nibr5dmp1yoVFz00h+aYcxKtN1HskW2jolFllQAlYOhqybo+GhUWLt4kWolje\nuOVJl+v1NuQ6pYcHI2U7s30JQ2hqcmaByUW2QDfC5uDBkwQCoWQmzYMHT6Y8PheFpnMR5+VUcLkt\nKZBPWTRzhRQjH1vcbPD9HkWxrugMaUwuVlR4UouIhqBGY/C0xaaxXAObJU3T4cIpFnN08FvWmMeF\n5StOrU5uPgunuBFOgYg2pExFTyS1RVYYe4J6D/16H1EieHUfQb3H/qQJSou3iWbv8XgdOm88hKZW\nLJpvQ65TepRZfoIqgNIsNKVTZmXPCCCCbhhOY+LyNRmF12sMqXXn9aZ+KuAmuUs+4lRguy0pkE9Z\nNHNFPsZyTiTcZPLrjWmELI0yH4QsjWAs9TmJzTrErXnpbNYTQrOsFPoH4uuMWFfejlPx5OazyAVV\nRYp6v4oXki9WVBVlPh4rHxPIFDLtnhN0eDrQUCiPRoVZBVI2bET69GDKthBHrlN6ePFiaDqggxZv\nZwsRdMPIRUxcLlzXLrpoKn/60wk6OweZPLmEiy+emvJ4N8ld8pFsJ5yZyKJmIrub5gNuMvklNt+J\nGDq7zbfT4yH7KeMjMVi7z5usSXbXwigZDAfOW9x8Frmg1q+oq7BOCXiLWn/mxzVeEsjkC716EEsz\nUVhomk6vbL5Hpcwqp9F3jF4slKEzMzoxHtg6RQqwp0epVUapWYblMdFNg1KrLGt9TYCvRWfkIiYu\nF65rvb1RSkq81NbGnwYEg6njWpwK2fEST5WLkgLjBSlGPra4yeSX2Hyf2c7k8ZD9lPFr93lZfzT+\nVZVIvnLPxanXs/GAm88iF5atxH3Xq4O/0sxKRkmpK5dZTC2GQqGjY6EwtdhYDylviRfKVijUqd/C\nSEix+vSYbNYQsLrjJUMsk8lm6lJoZ4MIumHkYtOaCyuP0/ILToXseImncltSYCKKmnx1L54ouIl3\ncioC3YjGbG/wD3frKdvjFTefRS4sW4n7sKYGOjqyYzWTunKZZUpsGkGjB0s30S2DKbFpYz2kvGVA\n76PCqozHhllhBrJYCLqQOeFtIaKHKKecAT3ECW+LxNCNwIWhSwAI+XopDvuT7Wwggm4Y+Zi4ww1O\nyy84FSqNjUEaG4NJi15Tk70LRz5a9dyWFBCEQsCpCHQjGrO9wXdaFmG84EbCOC1Tka84FbMSc5ea\nOdFzCesD4LEgqjMneu5YDylvEVfC9AjoPbR6WogRxuMpotycRC0i6Iajo1NjTkOnGsv0ZjUTqAi6\ns8SNSMmFlceyLNrb+5IxdJaV+ovdqVDp6QnR0BAA4jF33d32ZuTxYtUTBCF3JMognBlDl2lyIQic\n9iDzHVMAACAASURBVOHG2ua0TEW+4vTBgsTcpUaKQKdP4troRKmKyrUajXZPK92eTgw0TE8f7WYr\nC8MXjfWw8o4G7zF2lWyNZ5gt8bGIK5gTfUdW+iooQZcQT2fWYxtrC48bkaJy4D2yaVMj7e0DhMMm\npjnA6683smLFnIy9v1OXTpjYCUUE4WzJR9GRCzye7MfM5UIQbGk2WHfQkyxDYFmwrG70PtzEkVX6\nFPV+K9lHpW9iuCpKzF1qpAh0+si1Sg8TE59VhGZYGJaOiTxAGYk9RX+i2dMUT0jk0TGKvCLo4LR4\nKisror8/noVxrC08bkSKUxHoRsg2N/cSCMSvUSgUy7h4qqubRF1dcEjbjomcUEQQzpZciI6JaunI\nhSB4vdkYUoZgY7ORUtC5iSOrq1DU9SkSDpt1FYUp6Jw+WJCYOyFTSMHs9Jhs1hDwnEr2obKb7KOQ\nafeewNJNNDQs3aTdeyJrfRWUoMtHC48bkeJ0Hm6E7MyZZVRU+JKFxWfOzGyqVDduoxM5oYggnC25\nEB0T1dKRj4LgbBLUODknH3H6YGG8zFsYe6RgdnrkMtlHIWNgoJ3KZ6GhYZC6JvTZUFCCzql4ikRi\nrF27m8OHu5g7t5q77roYT4YLGLkRKU7nkSj6HYlY+Hx6WkW/r7lmDm1tA0mXyGuumWM7Lie4SQ4i\nCUUEwT25EB1TSxQ/PWjQFdKoLla8a9rE2BjnQhAsm2myu+P0tV0+M3UfZ5OgptBx+mBhvMxbGHuk\nYHZ65DLZRyEzK1LPweL9KM1CUzqzItl7OFBQgi4hls50PUzF2rW7Wb/+MAAHD3YBcM89izM6Jjci\nxakITBT99vk8RCKxtIp+X3HFLPbtO5kUs2INE4TC5rJpJntO6snkIJdnQWzt79RpG4CwCVEL9nbq\nLK8f/xvlXAgCXYOppRYlnnh8mzYxjJ+uyEeLqTAxkCyX6SGWzPTwmxXxGENdYVg6fjN791NBCbqE\neKqp8dPRYe9ueehQJ8FgOOl2eOhQp+05uUit7zQpSqLod8JCZ1f0G2D79lYCgTA1NWUEAmG2bWsV\n65ggFDDb2wwCEY2aMkUgorGtzci4CDkS0NFOOYhoaBwJZP6paz4mXskFrQPaqZg2lWwLI5OLhxeC\nMBJSMDs9xJKZHm3eFjQ97mqp6Rpt3hYIZaevghJ0TtE0aGwMEIspPB6NJUum256Ti9T6TvuYMaOc\n+CYg/jNzZrltH9mON8zHmnKCMJ5xU2PMqXjy6HGxCBAy4+10++htBb9lZCUd/3ggF1an8SKWc/Hw\nYiIhiT7SRwpmp4dYMtMjqAeJaJF4Q4u3s4WtoLMsi8cff5y33noLn8/HF7/4RWbPnp18fcOGDXz7\n299GKcXChQt57LHH0E75khw5coQPfvCDbNmyhaIi+7T2mSYYDBEvrq0A7VQ7NU6FkBth47SP+PUc\n/pOabGeUlJpywlhTyGuTG9zUGHMqni6ebHG0x6QzpDG5WHHxZHvRmOijrBT6BwzbPpzGR40XkZKL\nOL3xIpYLPTlPvq1N4h6XPmJ5Sg+xZKZHWAulbGcSW0H32muvEYlEeOmll9i1axdr1qzhO9/5DgB9\nfX08/fTTvPDCC1RXV/O9732P7u5uqqur6evr46mnnsLn82Vt8HYMDJhUVBQNadvhVAht2dLIunX7\nk8lHLMti2bLZKc9x2sdwwdfSYm9ty3ZGyXzMOCpMLAp5bXKDmxpjTjfGsysVi2dYQ9qZ7sOppWq8\niJRcxOnlQgg5tci6EeSFHkOXb2tTUO+J/2CBruPXJwEi6EZCLE/pIZbM9NCHBUsPb2cSW0G3Y8cO\nli9fDsCiRYvYu3dv8rWdO3cyb948nnrqKRobG7n11luprq5GKcWjjz7Kpz71KT72sY9lbfB2XHVV\nHU1NvckYuquusrcgXXbZDPbsaU8mE7n88hkpj3/99QYaGuLfPN3dITZubLAVdE7FVk9PiIaGQDIp\nSne3fb2PbGeUlJpywlhTyGuTG9zUGHO6MXZjRcp2Hw1BjcaglhSyjeUaSI6nEcmFEHJqkXUjyAu9\nDEG+rU1hLUzQCMZrhhkmU2L2idUmKjOjtQDoRKmKepNtYShiyUyPqtgU+o2+Ie1sYSvo+vr6KC8/\nHbNlGAaxWAyPx0N3dzdbt27llVdeobS0lNWrV7No0SJeffVVrr76as4///y0B1JVVYrHk359hpoa\newHx0EPLqago4c03T3L++VP4+MeX2JYt2LDhGLEYzJlTTSwGBw8GuPrqOaP2XVZWhM/nGdJOZ2w3\n32xfiDvBrFkVVFQUcfLkAFOmlFJbW5FWH04wTYtNmxpobAxSV1fB8uX1b3MdPbPPm246n8rK0pTH\nZ5JMz3e89Z3O55etvseKQl6bIG652NQAjUGoq4Dl9aS0XKysgmMhePMknD8F3nMBDF/Ohvd902So\nrEy/D4Cbp6U1/JH7mFOU8T6sI3DilJdKfwhMH9SM8EyrEP5Os923m8/bKb2tUFYa/++y0iJ69ZE/\njwQ9zdARgWAYKoogQOrjE9jdI/m8PuVqbYL01qfJVNBKEQMMUOorZbKvghp//ly/fPosTUwGKCVI\ngMrKUmrw51W8Yb5cqz6mE2UAgNKyImqZTg35MTbIn+tUywxO0kaUCF7NR23xTGqKszM2W0FXXl5O\nf39/sm1ZVlIUVVZWcuGFF1JzanVesmQJBw4c4Be/+AXTp0/n5ZdfpqOjgzvvvJMXX3wxZT/d3QNp\nDzrdLJcAt9228Iw+Bm2P37evLVm8O9FesGDyqH0vXjydw4e76O0N4/cXsXjx9LTHli7NzUGCwTDl\n5UUEg2GamoK2fTiN7duypTEZE7d7dys9PQNDLHwjXfMFCyYnr01nZz/ZwsnnPVH7tvv8stm33ftk\ni0Jfm7Y0n7Zc7AZ6esyUlostzQZNXQblOjR1wf/sHXr8aH0vKIv/AHTaJ/p1xYIyuHpOvP9M9+GJ\nephe7Ekmg/FEY3R0xIYcUyh/p7noO9uft98y6B8wKCston8gjL/SpKNj9Pu28aSXgx3xv8sTwNzy\nGB0d0bMaw/B5u3HrHA9rE6S3PnUWBxksCuPz+RiMhOkMB+kI5UeYxFj+/YxEk7chHm9YVkRjfws9\n0YG8cSXMp2tVSjVV3gH0yihWj5fSaDUd5MfY8uk6nSzvAZ+Gz/ChTI2TkW46+uzH5mZ9shV0l156\nKb/73e9YuXIlu3btYt68ecnXFi5cyMGDB+nq6qKiooLdu3fzwQ9+kN/85jfJY9797nfzH//xH44H\nNlY4dSVcurQWXdeyFqsGMGlSEcXFHoLBCBUVPiZNsg+Udpq0RGLiCpuJ+PkV+trkNN5poroe1voV\ndRXW/8/evUdHVd77H//svWcmt0kIaOQSEqRqqlIV8VLKEWxLUY/Y2taKUFuPFU+pq6e2lh/eVhVU\nirKsbdex3ltpa7XeVrWnLtex9WhBQNGioCgXUa4hQoTcJiHJzOz9+2PISBBmMkNm9t6T92stVniy\nZ/J898zkyf7u59arDPf0DH9sM6Xyynja4ZCDixzVljvJhHxwUe6GgUremGfptbap2ClWhT1Ihmw5\ntqliJ/3WRwNVm9miVqtFbbLlWKYq4i1uh+RJpkyNjNaqSuVqjBb+9Ub2nOQyhsa+cq6kTeimTJmi\nZcuWafr06XIcRwsWLNCiRYtUW1uryZMna/bs2bryyislSeedd16vhsuPMp3fls1ctUx7z1pautTZ\nGVM4HFJnZ0wtLenHv2d6ge/FOXE9r9P+G8mzNcLBefH9yzW/t03DSh0t2f5JgjZhWOqGvqXb0Na2\nxOe/qctQc3f6FSgLgd/nU/UolNU6exZ3qapSyp65HvlIyL22KqbX2qZye5Aq4oMSq1zGu1Ru933K\nx0DTYXRoR2C7bMVkBgIaEu3D+GDgEMrtQSqxy2RYiZspufzdS5vQmaapW2+9tdf3jjnmmOT/p06d\nqqlTpx7y+S+99NJhhJd/uV5MRMq896yyski1tRXJjcUrK9P30A0bVqYlS7YkV9+cMCF1YprrVTGz\n0fM6lZUVJYfBsjXCwXnx/cs1v7dNicWu9rt3l+YaNB89HV6Uj9Uh88FrvUj5ko+EPNObI7nmtbaJ\nhT76riFQr71Guxw5MgxDDYF6t0OCj42KjtZeq11G0JYTNzUqOjpndRX0xuJelWnv2YgRFdp/D7rq\n6vTL6H6yd53Ul73r8pHIZmogDiPMlhffP6R2YC/CjjS9Cgw99Dev9SLlSz4S8kxvjgw0DI/ru3az\nTTIMWTJlG06iDGRpeHSkPrYa1VncpuJouYbn8GYKCZ0LMh0eZxg9S5Xv/y+1hoaIamoqepX7UzYb\nqmdqIA4jxMCR6UbhhTL0cKDy+95q+ZTp8NSGdqPXzY6GdjK6/dmytSO4XTsUlR1M9NB5aeVGL6mw\nK9XqtMiQI8MxVGFXuh0SfCyf+/WR0B2mbBKbTIfHNTS0q6ZmUHLoYUND+hUlc50MZTpsVMr8tep5\nXfafQ9ff8pGYAgdTHnBUbDra3WnoiGJHFYHUF/iFMvSwUGS6wXahJOSZnnc2Mh2eSrKc2tbgZq0q\nWZFYOr0kpLH6vI6OfsbtsDzpxK6T1WV2KmZ1KRAr0oldJ7sdEnys1WxO/JMtmabKzUGSSOg8KZvE\nJtPhcdkkZ7meU7V1a4u2bWtJztHbtq1CUupzyvS16nmdcrkEbTbvH9Af2mKGOm1DZSGp0zbUGkvd\nq1Aoi2oUikw32C6UhDzT885GpsNTCyVZzpW1RW+rMdAoQ46cgKG1RW+T0B1CTXSULFkyi6Ky9zLf\nEIeny+hSq9WqoCxFrbiOjKVf1DBbJHSHKR/zvLLpqcr1nKqWli5t3Zq4JdrU1Knm5v5feTMfvBgT\nBoZMFzkZqItqeNVAnROXj/POtMetUJLlXGm1WhU3YjJlyDYctVqt6Z80QDHfEP0pn1uGkNAdpnzM\n83I8OHpk8OBi1dYOSm6oPnhw+g+pF+fEeTEmDAyZLnIyUBMIrxqow/wyPe9sepbpcetfFfEKNQd2\ny5YtQ6Yq4ukXVgNw+PK5ZQgJ3WE644zheuedXdq4cY+OPXaIzjxzeL/XkY/l+zOdSzZyZEWvRVdG\njkz/ByIfS+tnO09vIC33D28YVxXX/3xgacMeS3VD4jq9KvVF60BNILwq0w22C0Wm551Nz3KmPW4M\nR06tKjZUO4M7ZJtxmbalqthQt0OCz7HQTt8cFR2m90NrtUs7FDYqdVR0WM7qKuiErrs7pkWLVieT\nrZkzT1EgkPqUM00I3nijQS0tXaqqKlNLS5def72h35OtfAwLzHQuWTaJkBf3+GO5f7jlD2uDWrsn\ncaG7do+lRWuDmnVK9JCPz0evhRcvjLOJKR/nkekG24Ui0/POR88yw5FTK3XKVB2tkQK2FDVV6pS5\nHZJnkaj0zfbgVq0vekeSLRWZsmWrNnq022F5zprit9QQ3C7JUVswojXFb2lc5+dzUldBJ3SLFq3W\n889vlCRt2LBHkjRr1mkpn5NpQpDN4iCZysewwEyTRq8mQsyJg19sbDJTlg+Uj3lCXrwwziamTJ/j\nxUS2UOSjZ5nhyKmV2xWSYciQIccwEmUc1I7gdtUHtySGyAUTI6Jytcy8n20Pbu612Mf24GYSuoOo\nD25Vl9klS4bipqP64NacJXQF/Sdr48Y9KcsHk2lC0LM4SFNTp7Zube3T4iCZOuOM4Ro0qEg7d0Y0\naFBRToZ1Hpgk+nUuWaGcBwrfsYPtlGU3ePHCOJuYMn1OTwK4qdXU0h2WXmuwMg/Uh+K2tLze0lPr\nA1peb8nOwUdw/PC4zhoR1+gKW2eNyM3Q1AOTRIYj9+ZIkuPIkbPvKw4lYramLKPHgW2q+38rvCjo\nBBVXTN3qVlwxBZ1gzuoq6B66Y48dkuyZ6ymnk2lvWDaLg2SqZ1jn0KHhnA3rLJS5ZIVyHih8M8ck\nhldubDJ17GA7WXaTF+fpZRNTps/xYiKbD/nokc1HzzKLqKTWYUZUYVcmep3sLnWYEbdD8qywXaEW\nq6lXGZ9WEx2ldqstuXpjTXSU2yF50pGxofoouEO24jIdS0fmcP5qQSd0M2eeIkm95tClk2lCkM3i\nIJnO08vHMEKvDqHMVKGcBwpfIKCUc+bc4MUL42xiyvQ5+UhkvTiss1ASWbYtSI0kpe+GRUeo0dqp\niNpUbJdrWHSE2yF5UnW0VoZMmcVR2Z3s13co+Zy/WtAJXSAQSDtn7kCZJgTZ9AhlOk+PpfVzJ9Pk\nGugvXrzA9+KFcTYxZfqcfCSyXpyf6MUeWfS/nottU1ENjnLxncpHwR3qNjsVVlgdZqc+Cu5gDt1B\nsF9f3+Rz/mpBJ3T5kE2PUKY9btlsLJ6pgZrYZJpcA/1leb2lJzcE1NZtqDzkyLals2q8lUwNFPlI\nZL3YG+bFHln0Py6++445dOhPcdlqN9oUVbeCRkhx5W6uPAmdCzLtcetJGquqytXYmJvGuFASGy8O\nZwUOZvE2S2s+ttQVl4osaWiJQ0JXwLzYG+bFHlnATaV2WNuCm9QqWzJNDY/67zoI3rEjuFVxw1aR\nihQ14toR3Kqjo5/JSV0kdC7w4sIdhZLYMJwVflHfbqilO9FL0xlPlFG46A0DvM+Qeg2Ro1XG4cnf\naqAkdC7w4sIdhZLYZDuc1UvJNQaGkWFH60JOsodupAd6bJA79IYB3tduRlQRH5RYETTepXZWBMVh\nyOdqoCR0LvDifLVCSWyyHc4K5NvZI+P6qMNIzqE7eyQX+yg8Xlz8BzgUVgTtG1u2dgS3a4eisoOJ\nhXbMwt7aOiv5XA2UhM4FXpyvViiJTaEkpih8E6oTF7YMwUMh8+LqnsChsCJo3+wIbld9cEuiJzPY\nJUmsBnoQ+VyQiITOBV6cr+bFXsNsFEpiisLHAEsMBF5c3RPA4WE1UO/xVULXk3Tsv3x/fycd+Uhs\nvDhfzYu9hkAhK5Sei54hdW0NUrlteWJIHcP8vMOLq3sCh1If3Kp1xWsSc56KTTmyVRM92u2wPIeh\nqX2Tz6GpvkroepKOsrIitbcnunhTJR3ZJGf5SGy8OCzQi72GQCErlJ6LnsS0rFRq70gkqG4npoWQ\nLBdKUsrqnvCTbcEtajVbFJSlqBnXtuAWErqDYGhq3+TzBoGvErpMk45skrN8JDZeHBboxV5DoJAV\nSs+FFxNTL8aUqUJISiVW94TfHNgO+7NdzjU2q++bfN4g8FVCl2nSkU1ylmkd3d0xLVq0Whs37tGx\nxw7RzJmnKBDw1csqyZu9hkAhO2NoXO98bGpjk6ljB9s6c6g/L3q9mJh6MaZMFUJSmq1C6Z30ClYk\n7LsR0VrtCjSoS10KOiGNYKEPHJb83SDwVebRk2TsP4culWx6nTJNbBYtWq3nn98oSdqwYY8kadas\n09LW4zVe7DUECtkbOy21dBuqKnPU0m3o9Z2WL3syeobQtZlSeWXcE0PqCmGYXyEkpdkqlN5Jr2BF\nwr6zZKrMKU8MkXNMWSS+OAwjo0er3WyTZKskbmpkDofv+iqh60k6qqrK1diYvrftjDOG6513diV7\nz848c3if6+irjRv3pCwDwMEUSg9Mz5C6qiqpsdEbF92FkPoUQlKarUL53fCKNrNFrVaL2mTLsUxV\nxFvcDsmz2Fgc/WlktFamTJklUdlduZ1rWNC3Ht54o0EtLV2qqipTS0uXXn+9Ie1z4nFby5dv01NP\nvafly7fJtu2Ujz/22CEpywBwMAf2uAykHphc6+nh2dRqaukOS681WG6HlLGB/Gngd6N/dRqdajVb\n1KEOtZot6jQ63Q7Js8rssFqtFu3UR2q1WlRmh90OCT7WM9fwFI1NJne54qseukxlM4cu04VUZs48\nRZJ6zaEDgHQGcg9MrhVCD89AHnbI70b/KnKKVBGvUM+wryKnyO2QPMuRJMeRI2ffV8AfCjqhy2YO\nXaZJYCAQ8OWcOQDuYvW/3CmE+WeFkJRmi9+N/lVhV6rNbkkMI7S7VGFXuh2SZ3WYEVXYlcnXqoMh\nl/CJgk7oslm5MR/L9+dj83IAGKgKoYenEJJSiRUrvWBYdIQarZ2KqE3FdrmGRUe4HZJnsWE2/Cpt\nQmfbtubNm6f169crFApp/vz5GjVqVPL44sWLdc8998hxHI0ZM0Zz585VJBLRnDlzFIlEFI1Gdf31\n1+vUU0897GB7EqH9V7lMlQhls3JjPpbvz8fm5UCh81LbNJD1XLC3NUjltuWJC/ZC6OEphKRUGphD\nR73WNn0U3KFus1NhhdVhduqj4A5WuTwEkl/4VdqE7sUXX1R3d7eeeOIJrVq1SnfccYfuu+8+SVIk\nEtGdd96pP/7xjxoyZIgeeughNTU16U9/+pPGjx+vyy+/XB9++KFmz56tZ5555rCD7UmEysqK1N6e\nWHq3vxOhfCzfn4/Ny4FC56W2aSDruWAvK5XaOxIX7oV+wZ4PhZCUSgNz6KjX2qaI2ZqyjE+Q/MKv\n0iZ0K1eu1MSJEyVJY8eO1Zo1a5LH3nrrLdXV1WnhwoXatm2bLr74Yg0ZMkSXX365QqGQJCkej6uo\nqH8m4BZKIpSPYZ1AofNS2zSQDcQLdvRdoQwdzYTX2iaGEfYdyS/8Km1CF4lEFA5/smyrZVmKxWIK\nBAJqamrSihUr9Oyzz6q0tFSXXnqpxo4dq9GjR0uSGhsbNWfOHN144439EmyhJEL5GNYJFDovtU0D\n2UC8YEffFcrQ0Ux4rW3q2fvKVFSDo7ndC8vvSH7hV2kTunA4rPb29mTZtm0FAomnVVZW6qSTTlJV\nVZUk6fTTT9fatWs1evRorV+/Xj/96U917bXX6swzz0wbyODBpQoEUu8V9NWvHq/KylJt29aqmpoK\nTZxYm3IOXTxua+nSrX1+fF9VVR1+InnhhYNcqztb1E3dXuKltml/A+09++oRUmWltK1Vqjm6SBNr\n5cocuoH2uh+s7rgtLd26772oUM7fi76e94VD3avbDflqm6S+t09Dte+aw4MLXHrpvTxCJ6hSpWpV\ni0aXDdIojcrp3mGZ8tJr1YOYDi2uuLZqq3boA1VU5fbzlDahGzdunF5++WWdf/75WrVqlerq6pLH\nxowZow0bNmjPnj2qqKjQ6tWrNW3aNG3cuFE//vGP9etf/1rHH398nwJpauro0+NOPPEInX320Wps\nbNPu3e0pH7t8+bbk4iOrVzeoubnjsOfHVVWVq7HRnaGe1E3dfqs7l42q19omqTDes2ycWCadfXT5\nvnY5//UP1Nf9wLqX13+yAMlqSc3N8ZzNw/PSeWf7M3IlX22T5J/26VC8GFNYR2p01ehEe6bU15n5\n5MXXiphS2x7cqvrgFpWWFWlb+w41Rzv6NCczm/YpbUI3ZcoULVu2TNOnT5fjOFqwYIEWLVqk2tpa\nTZ48WbNnz9aVV14pSTrvvPNUV1enq666St3d3fr5z38uKXG3qmdCcD4Vypw7AJ/m57YJyAXmM3oD\nbRMAKb9zMg3HcTwx4SGTbLqv2fcrr2zRk0++p7a2bpWXh3TJJSfqrLNGpX1ef9SdC9RN3X6r2yvD\nHg5HLtqmXHD7ruRAPXcv1b1/D50knTUidQ/d4ewR56XzzvZnFAK/tE+H4sWYJG/GRUx946WYtgY3\na33RO1KJLe019dmuk1QbPTrt83LSQ+dnhmFI6rlDuf//AQAoLJkuQDIQ94gDgHwxJMkwZMiQYxg5\nzUIKOqFraIiopqaiVxkAgEKU6d51DNEEgNxpNyOqiA9SqYrUEe9Su5m7PMQ7S/fkwIHbGvRlm4N4\n3Nby5dv01FPvafnybbJtO1fhAQDgmmFljra1mnrvY1PbWk0NL/PEDAwAKAhldlitVot26iO1Wi0q\ns8Ppn5Slgu6hy2a/t+XLt/Wad2fb9mHPuwMAwGsMR5IcJQYGOYn/9rPDmacHAH4Wl612o01RdSto\nhBRX7jqJfJXQxeO2VqyoV1tbTOXlAY0fX51yX7lslnt55ZWt2ro1MZmgqalTS5ZsJaED4EtcTCOV\n7XkYcsk8PQAD1bbgZjVbTbIVl2lZ2hbcrKOjn8lJXb5K6FasqNfSpdtUVlak9vYuSUq5r1zP4yVp\n06bmtI9POPAPGnMKAPgTF9NIpaXb0Na2RIbf1GWoubv/7x4zTw/AQNUY+EidZqdMGbLNqBoDH+Ws\nLl/dq810X7ls9qE766yRKi4OqKMjquLigCZOHJl5oADgAVxMI5XBRY5qy51eX/tbddhJWT6YuJ3Y\nguGp9QEtr7fEVHYAfmQ5lgwZsmXLkCHLsdI/KUu+6qGrri5P9rT1lPvz8ZJkmoaOOqpUJSUBlZeH\n9m19AAD+Ux12kj1zPWWgx8hyRzUVdq9yf8t0KwWJnmUAhaHcHqRdzk4ZMuU4hsrtQTmry1cJXc+i\nJvvPoevL4zNZFKWhoV01NYN6lQHAj7K5mMbAkY/PR6ZbKfTEk6oMAH5wVGyYWq0mRa1uBWMhHRUb\nlrO6fJXQmaapCRNq+rwLfM/jM5FNrx4AeFE2F9MYOLz6+aBnGUAhqLSHaHhspEqLitQR61KlPSRn\ndfkqocuHbHr1AABA/6BnGUAhGBFNrMNhKqrB0WCynAskdAfIplcPAAD0D6/2HAJAJkyZGhmtVZXK\n1RhNP7Lw8OoCAAAAAPgSCR0AAAAA+BQJHQAAAAD4FAkdAAAAAPgUCR0AAAAA+BQJHQAAAAD4FAkd\nAAAAAPgUCR0AAAAA+BQJHQAAAAD4FAkdAAAAAPgUCR0AAAAA+FTA7QCAQheP21qxol719W2qri7X\n+PHVMk3upRSSuC2taLDU1iCV25bGD4+LtxgAgIHLlq0dwe3aoajsYFAjoiNl5qgvzVcJXc+FcVtb\nTOXlAS6M4QsrVtRr6dJtkqRNm5olSRMm1LgZEvrZigZLS3dYKiuV2jssSdKE6rjLURWGnmS5PmKo\nOuyQLAMAfKE+uFXritfIkC2n2JQjWzXRo3NSl68Sup4L47KyIrW3d0niwhjeV1/flrIM/6uPbNjo\n6QAAIABJREFUGCnLyF5PsixJm1oT3yNZBgB43bbgFrWaLQrKUtSMa1twS84SOl/d5+TCGH5UXV2e\nsgz/qw47KcvIHskyAMCfDrwWyN21ga966IYNC2vJkq3q7rYVCpmaMGGk2yEBaY0fXy1JvebQobCM\nH57oMWozpfLKeLKMwzeszNGS7abauqXykDSB1xYA4APV0VrtCnykbnUr4IRUHa3NWV2+SugMw1Ei\nu93/H+BtpmkyNLjAmWZiGGBVldTYSMLRnwxHSrT1RuIrzT4AwAcMmSqzyySVSLYpI4cDI32V0DU0\ntKumZlByDl1DQ7vbIQEAcqihw1BNxSc38Bo6GHIJAPC+DjOiCrtSpSpSh92lDjOSs7rSJnS2bWve\nvHlav369QqGQ5s+fr1GjRiWPL168WPfcc48cx9GYMWM0d+5cdXV1ac6cOdq9e7fKysq0cOFCDRky\n5LCDHTasTEuWbNlvyCVD14CBykttUzZYvbFvqsNOcjGUnjLgdX5vnwAcvlI7rG3BTWqVLZmmhkdz\nN1or7eXDiy++qO7ubj3xxBOaPXu27rjjjuSxSCSiO++8U/fff7+eeuopVVdXq6mpSX/+859VV1en\nxx57TF//+td177339kuwhmEoMexm/38ABiIvtU3Z6Fm9cVOrqaU7LL3WYLkWi5eNHx7XWSPiGl1h\n66wRzE+EP/i9fQJw+AxJMgwZMvZ9zZ20Cd3KlSs1ceJESdLYsWO1Zs2a5LG33npLdXV1Wrhwob79\n7W/ryCOP1JAhQ3o9Z9KkSXr11Vf7JdiGhohqaip08slDVVNToYaG3HVdAvA2L7VN2WD1xr7pmZ94\n8WdjmlBNLyb8we/tE4DD125GVBEfpKEapor4ILW7OeQyEokoHA4ny5ZlKRaLKRAIqKmpSStWrNCz\nzz6r0tJSXXrppRo7dqwikYjKyxNLs5eVlamtrX+2F6iuLk9uzNxTBjAwealtygZDCYHC5ff2CcDh\nC9sVarGaepVzJW1CFw6H1d7+yeIjtm0rEEg8rbKyUieddJKqqqokSaeffrrWrl3b6znt7e2qqEh/\nAoMHlyoQSD3k6KtfPV6VlaXatq1VNTUVmjixVqYLt2urqtxLJKmbugdC3X3hpbZpf3193b56hFRZ\nKW1rlWoqpIm1OuzeJ7ffs4H6eaXugVV3X/i9fconL8YkeTMuYuobr8R0hE5QpUrVqhaNLhukURol\nM0crXaZN6MaNG6eXX35Z559/vlatWqW6urrksTFjxmjDhg3as2ePKioqtHr1ak2bNk3jxo3T4sWL\ndfLJJ2vJkiU67bTT0gbS1NTRp4BPPPEInX320WpsbNPu3flf5bKqqlyNje7cNaNu6vZb3blsVL3W\nNkmZv24nliX+SdLu3X1+Wr/U3d8K4fNK3QOn7lxf8BVC+5QPXoxJ8mZcxNQ3XosprCM1ump0Im9R\n3/KWbNqntAndlClTtGzZMk2fPl2O42jBggVatGiRamtrNXnyZM2ePVtXXnmlJOm8885TXV2dampq\ndN1112nGjBkKBoO66667Mg4MAFKhbQLgVbRPAPLJcBzHExM3+pJNx+O2VqyoV1tbTOXlAY0fX533\nIZd+vytJ3dSdz7q9MuzhcGTyOnjtPcvn1gheO3fqpu50P6MQ9OV1sGVrR3C7zMqo7OagRkRH5mzY\nV6a81pvSw4txEVPfFEJMOemh85IVK+q1dOm25MbikjRhQu72dAAAP+vZGkFScgGWCdUs+w8MJDuC\n21Uf3JLY3DiYuHYaGa11OSoA/ckbt2j6qL6+LWUZAPAJtkYAEDFbU5YB+J+vEroDtylg2wIAOLQD\nt0JgawRg4DlwqfRcLp0OwB2+GnI5fny1JPWaQwcAOLjxwxPDK/efQwdgYBkRHSlJMhXV4GgwWQZQ\nOHyV0JmmqQkTajw54dHvehacqa9vU3V1uSsLzgB+1bP4SFuDVG5bOV18JBOmyZw5YKAzZWpktFZV\nKldjlGsnIF96FiTaoajsYG4XJPJVQueFVS4LVc+CM5K0aVOzJBac6S+ZJst8zv3nlW2W7lsdVGtc\nqrCCip0iTRpFIgVkI5+rswJArmwOfqjXS5eoS10qKi3SmR2T9JnosTmpy1cJHatc5g4LzuROpsky\nn3P/eXJDQO+3WLJMaadt6fENDgkdkCVWZwVQCFaVvK7mQLNMGdob2KtVJa+T0EkDN+nIR49NdXV5\nMtnoKaN/ZPq5Haifcz9r7TbUFZNsQzKdRBlAdlidFW7J5xA5FL5OY2/Kcn/yVUI3UJOOfPTY9Cww\ns/+wwP42UOfpZfq5Haifcz8bXmprRdxSTIlGdUSp7XZIgG9Vh51kz1xPGcgH9uxDfxoWG6G2QKts\n2TJkalhsRM7q8lVCN1BXucxHj03PgjO5lI95el5MGs84Y7jeeWeXNm7co2OPHaIzzxye8vHZfM69\neN4DSbEhxeJSly3JTJQBZIfVWeGWJnO3NgU3qludCgWLVRoPa6RI6JCd0d3HaVfgI3WHOhWKFWt0\n93E5q8tXCd1AXeWyUHps8pGYenFxlzfeaFBLS5eqqsrU0tKl119vSBlTNp/z5cu368kn31NbW5fK\ny4tk247OOos/Qvmy7CNLnXYii+u0DS39yHI5IsC/WJ0Vbtkc+kDNgSaZMtQR2KvNoQ90Utepbofl\nOQxN7Zsus1Ojo8eqNFSkjmiXuszOnNXlq4RuoCqUnsl8JKZenH+Wj5heeWWrtm5tkSQ1NXVqyZKt\nJHR5FIkaMozeZQCAv3QbnQo4AUmOTMdQt5G7C3A/Y2hq34TtCrVYTb3KuUJC5wOF0jOZj3l6uU4a\nsxnamJ8e1gPnmDDnJJ9Ghm01dJjaN+JSI8PMoQMAvxkSr1JroFWWDMXlaEi8yu2QPKnNbFGr1aI2\n2XIsUxXxFrdD8qQR0ZGSJFNRDY4Gk+Vc8FVCx/5c/paPeXq5ThqzGdKZj0R20qRaNTRE1NbWrfLy\nkCZN4k5ZPn25Nq73mxPDLotNR1+uZbgYvI/93oDe/q39S5KkSGmzwl2VyTJ66zQ61Wq2KChLUTOu\nTnoyXeerhI79uZBOrpPGbIZP5iORnTChRqZp5jRpxKFVlTr6Ym1M3UZIISemqlJ6SOF97PcG9BZS\nSF9qP1dVpeVqbPfviKhcK3KKVBGvkGSrJG6qyClyOyRPyufQVF8ldF6cH1UovLhKYnd3TIsWrVZ9\nfUTV1WHNnHmKAgF3P7JeXaAmH0kjDq2qxNH6Paaau6XKkKlvHRNzOyQgLfZ7A3pjsY++KbcrJMOQ\nIUOOYSTK+JR8Dk31VULn1YvpQuDF1SEXLVqt55/fqFAooNWrP5IkzZp1mqsx5WP4JPznhc2WtkdM\nOZIi3aae32zp7KPp6YC3sd8b0BuLffSNI0mOI0fOvq84mHwOTfVVQlcoqz16kRd7Pzdu3JOy7AZ6\nwnAw7zdbKg5IlpmYl/R+M9sWwPvY7w3oLWK2piwjocOMqMKuTCS+dpc6zIjbIXlSPoem+iqhK5TV\nHr3Ii72fxx47RBs27OlVBryobkhcm9vMXmXA69jvDegtn8vM+xmvU99U2JVqs1uSiW+FXZmzunyV\n0CF3vDiUcObMUySp1xw6wItuHd8tSdocKdLR4WiyDADwj3wuM+9nvE59w7YFyDsvDiUMBAKaNes0\nemTheaGQdMekblVVFamxkWQOAPzIlKmR0VpVqVyNUa47DoXXqW/y+TqxdA8AAAAA+BQJHQAAAAD4\nFAkdAAAAAPgUCR0AAAAA+BQJHQAAAAD4FAkdAAAAAPgUCR0AAAAA+BQJHQAAAAD4FAkdAAAAAPhU\nIN0DbNvWvHnztH79eoVCIc2fP1+jRo1KHp8/f77efPNNlZWVSZLuvfdetbW16dprr5XjOBo0aJDu\nuusulZSU5O4sAAw4tE0AvIr2CUA+pe2he/HFF9Xd3a0nnnhCs2fP1h133NHr+Lvvvqvf/va3euSR\nR/TII4+ovLxcv//97/Xv//7vevTRR3Xcccfp6aef7pdg43Fby5dv05/+9LaWL98m27b75ecC8B8v\ntU2AF8RtaXm9pafWB7S83hJ/It1D+4RCZsvW9uBWrdYqbQ9ulS0aG7elTehWrlypiRMnSpLGjh2r\nNWvWJI/Ztq0tW7bo5ptv1vTp05ONzwknnKDW1lZJUiQSUSCQtiOwT1asqNfSpdu0ceMeLV26Ta+9\nVt8vPxeA/3ipbQK8YEWDpaU7LG1qNbV0h6XXGiy3QxqwaJ9QyHYEt6s+uEW7tVv1wS3aEdzudkgD\nXtrWIhKJKBwOJ8uWZSkWiykQCKijo0Pf+c539L3vfU/xeFyXXXaZPve5z2nYsGG666679Nxzz6m7\nu1v/9V//1S/B1te3pSwDGDi81DYBXlAfMVKWkT+0TyhkEbM1ZRn5lzahC4fDam9vT5Zt207eNSop\nKdFll12WHOM9fvx4rVu3Tn/4wx90++23a+LEifrnP/+p6667Tg8++GDKegYPLlUgkPpu4pgxQ7Vr\n115JUllZkcaMGaqqqvJ0p9Dv3KiTuql7INXdF15qm/Y3kN+zgXruXql7TLu0a/Mnx8aMlKqq8lN3\nvrn9WU+H9qnvvBiT5M24vBJTRMMUVYckqbSsSCM1TFXyRmySd16n/eU6prQJ3bhx4/Tyyy/r/PPP\n16pVq1RXV5c8tnnzZv3kJz/Rs88+K9u29eabb+ob3/iGKioqVF6eCPyoo45KDiFIpampI+1jjj9+\nsJqbO9TWFlN5eUDHHz9YjY357aWrqirPe53UTd1+rTuXDZiX2qYehfCe+bF+6k44vkRqrrRUHzFU\nHXZ0fElcjY35qTuf+qPuXF9c0T71jRdjkrwZl5diKtUQDQ52yKyMym4OqjQ6RI3yRmxeep16ZBpT\nNu1T2oRuypQpWrZsmaZPny7HcbRgwQItWrRItbW1mjx5si688EJNmzZNwWBQF154oY477jjddNNN\nuvXWW2XbthzH0c0335xxYAfjOP3yYwAUAC+1TYAX8CfSO2ifUMhMmRoZrVWVytUY9Vby5CW2bO0I\nbtcORWUHgxoRHSkzRzvGGY7jjTSpL5nr8uXbtHTpNpWVFam9vUtnnVWjCRNq8hDdJ/x+V5K6qTuf\ndXtx2EOmMr2r5vf3zI/1U3fC8vrEoig9zhoR14TqeF7qzic/9NDli1/ap0PxYkySN+Mipr7xUkzb\ng1tVH9yi0rIidbR3qTo6SiOjtWmfl0375KuNxVkUBQCAg2NRFADwjnwuHuOrhK66ujxlGQCAgao6\n7KQsAwDyJ2xXpCz3J19tcjJ+fLUkJRdF6SkDADDQjR+eGF7ZsyhKTxkAkH8joiMlSaaiGhwNJsu5\n4KuEzjRNTZhQ46nxsQAAeIFpKmdz5gAAmcnn4jG+GnIJAAAAAPgECR0AAAAA+BQJHQAAAAD4FAkd\nAAAAAPgUCR0AAAAA+BQJHQAAAAD4FAkdAAAAAPgUCR0AAAAA+BQJHQAAAAD4FAkdAAAAAPgUCR0A\nAAAA+BQJHQAAAAD4FAkdAAAAAPgUCR0AAAAA+BQJHQAAAAD4FAkdAAAAAPgUCR0AAAAA+BQJHQAA\nAAD4FAkdAAAAAPgUCR0AAAAA+BQJHQAAAAD4FAkdAAAAAPgUCR0AAAAA+BQJHQAAAAD4lOE4juN2\nEAAAAACAzNFDBwAAAAA+RUIHAAAAAD5FQgcAAAAAPkVCBwAAAAA+RUIHAAAAAD5FQgcAAAAAPhVw\nO4BUbNvWvHnztH79eoVCIc2fP1+jRo1KHn/yySf1+OOPKxAI6KqrrtKXvvSlfqk3Go3qxhtvVH19\nvbq7u3XVVVdp8uTJyeO///3v9dRTT2nIkCGSpFtuuUWf+cxn+qXuHt/4xjcUDoclSSNHjtTtt9+e\nPJar85akv/zlL3rmmWckSV1dXVq7dq2WLVumiooKSdL8+fP15ptvqqysTJJ07733qry8/LDrXb16\ntX7xi1/okUce0ZYtW3T99dfLMAwdd9xxmjt3rkzzk3sPnZ2dmjNnjnbv3q2ysjItXLgw+V4cbt1r\n167VbbfdJsuyFAqFtHDhQh155JG9Hp/qvTmcut977z3NmjVLRx99tCRpxowZOv/885OPzeV5X3PN\nNfr4448lSfX19TrllFP0q1/9KvlYx3E0adKkZGxjx47V7Nmzs67b79xqmyT32yfaJtom2iZvc7N9\nyjamXP3+9sX+n7f9vfTSS7rnnnsUCAR00UUXadq0aXmJJ1VM+bj+PFC6vzluvE5u/x08mHg8rp/9\n7GfatGmTDMPQLbfcorq6uuTxnL9Ojoe98MILznXXXec4juO89dZbzg9+8IPksV27djkXXHCB09XV\n5bS2tib/3x+efvppZ/78+Y7jOE5TU5Nz9tln9zo+e/Zs55133umXug6ms7PTufDCCw96LJfnfaB5\n8+Y5jz/+eK/vTZ8+3dm9e3e/1vPggw86F1xwgXPxxRc7juM4s2bNcl577TXHcRznpptucv7+97/3\nevzDDz/s/Pd//7fjOI7z3HPPObfddlu/1X3ppZc67733nuM4jvPnP//ZWbBgQa/Hp3pvDrfuJ598\n0vnd7353yMfn8rx7NDc3O1/72tecnTt39vr+5s2bnVmzZmVdX6Fxq21yHHfbJ9om2qaDoW3yFjfb\np2xicpzc/P72xaE+b93d3c5XvvIVp7m52enq6nK++c1vOo2Nja7G5Di5v/48mFR/c9x6ndy+Tj+Y\nf/zjH87111/vOI7jvPbaa70+4/l4nTw95HLlypWaOHGipMRdtzVr1iSPvf322zr11FMVCoVUXl6u\n2tparVu3rl/qPe+88/TjH/9YUuLun2VZvY6/++67evDBBzVjxgw98MAD/VLn/tatW6e9e/fqiiuu\n0GWXXaZVq1Ylj+XyvPf3zjvvaOPGjbrkkkuS37NtW1u2bNHNN9+s6dOn6+mnn+6Xumpra3X33Xcn\ny++++67OPPNMSdKkSZO0fPnyXo/f/3MxadIkvfrqq/1W9y9/+UudcMIJkhJ3W4qKino9PtV7c7h1\nr1mzRv/85z916aWX6sYbb1QkEun1+Fyed4+7775b3/nOd3TUUUf1+v67776rnTt36rvf/a7+8z//\nUx9++GHWdRcCt9omyd32ibaJtom2yfvcbJ+yiSlXv799cajP2wcffKDa2loNGjRIoVBIp512mt54\n4w1XY5Jyf/15MKn+5rj1Orl9nX4wX/nKV3TbbbdJknbs2JEcPSLl53XydEIXiUSSw0ckybIsxWKx\n5LH9u+PLyso+9UcmW2VlZQqHw4pEIrr66qv1k5/8pNfxqVOnat68efrDH/6glStX6uWXX+6XensU\nFxdr5syZ+t3vfqdbbrlF/+///b+8nPf+HnjgAf3whz/s9b2Ojg595zvf0Z133qnf/va3euyxx/rl\nD8G5556rQOCT0b+O48gwDEmJ82tra+v1+P1fg4MdP5y6ey4W3nzzTf3pT3/S5Zdf3uvxqd6bw637\n5JNP1rXXXqtHH31UNTU1uueee3o9PpfnLUm7d+/Wq6++qm9+85ufenxVVZW+//3v65FHHtGsWbM0\nZ86crOsuBG61TT0/z632ibaJtom2yfvcbJ+yiSlXv799cbDPW0+8brxOqWKScn/9eTCp/ua49Tq5\nfZ1+KIFAQNddd51uu+02ffWrX01+Px+vk6cTunA4rPb29mTZtu3kh/zAY+3t7f063rqhoUGXXXaZ\nLrzwwl5viuM4+o//+A8NGTJEoVBIZ599tt57771+q1eSRo8era997WsyDEOjR49WZWWlGhsbJeX+\nvCWptbVVmzZt0vjx43t9v6SkRJdddplKSkoUDoc1fvz4nDS6+89JaW9v73WXQ+r9Ghzs+OF6/vnn\nNXfuXD344IOfmgeS6r05XFOmTNHnPve55P8P/Fzl+rz/93//VxdccMGn7nRJ0uc+97nk+PTTTz9d\nu3btkuM4/Vq/n7jZNknutU+0TbRNtE3e53b7lGlM+fr9zYRbr1Mq+bj+PJRD/c1x83Vy8zo9lYUL\nF+qFF17QTTfdpI6ODkn5eZ08ndCNGzdOS5YskSStWrWq1+TCk08+WStXrlRXV5fa2tr0wQcf9Dp+\nOD7++GNdccUVmjNnjr71rW/1OhaJRHTBBReovb1djuNoxYoVyT90/eXpp5/WHXfcIUnauXOnIpGI\nqqqqJOX2vHu88cYb+sIXvvCp72/evFkzZsxQPB5XNBrVm2++qTFjxvRr3ZJ04oknasWKFZKkJUuW\n6PTTT+91fNy4cVq8eHHy+GmnndZvdf/1r3/Vn/70Jz3yyCOqqan51PFU783hmjlzpt5++21J0quv\nvvqp1zaX591T56RJkw567De/+Y3+8Ic/SEoM7Ro+fHiyp2Igcqttktxtn2ibaJtom7zPzfYpm5jy\n9fubiWOOOUZbtmxRc3Ozuru79a9//UunnnqqqzHl4/rzYFL9zXHrdXL7Ov1gnn322eTwzpKSEhmG\nkbwJmI/XydOrXE6ZMkXLli3T9OnT5TiOFixYoEWLFqm2tlaTJ0/Wd7/7XX3729+W4zi65pprPjWn\nIFv333+/Wltbde+99+ree++VJF188cXau3evLrnkEl1zzTW67LLLFAqF9IUvfEFnn312v9Tb41vf\n+pZuuOEGzZgxQ4ZhaMGCBXrkkUdyft49Nm3apJEjRybL+7/mF154oaZNm6ZgMKgLL7xQxx13XL/W\nLUnXXXedbrrpJv3yl7/UZz7zGZ177rmSpCuuuEL333+/ZsyYoeuuu04zZsxQMBjUXXfd1S/1xuNx\n/fznP9fw4cP1ox/9SJJ0xhln6Oqrr9a1116rn/zkJwd9bw41NCJT8+bN02233aZgMKgjjzwyORY7\n1+fdY9OmTZ+6UOyp+/vf/77mzJmjxYsXy7Ksw1o9rxC41TZJ7rZPtE20TbRN3udm+5RtTPn4/e2L\nv/3tb+ro6NAll1yi66+/XjNnzpTjOLrooos0dOhQ12PK9fXnwaT7m+PG6+T2dfrBnHPOObrhhht0\n6aWXKhaL6cYbb9Q//vGPvH2eDGegj00AAAAAAJ/y9JBLAAAAAMChkdABAAAAgE+R0AEAAACAT5HQ\nAQAAAIBPkdABAAAAgE+R0AEAAACAT5HQAQAAAIBPkdABAAAAgE+R0AEAAACAT5HQAQAAAIBPkdAB\nAAAAgE+R0AEAAACAT5HQAQAAAIBPkdABAAAAgE+R0AEAAACAT5HQAQAAAIBPkdABAAAAgE+R0AEA\nAACAT5HQAQAAAIBPkdABAAAAgE+R0AEAAACAT5HQ4bC98847uvrqqyVJb7/9tm6++eaMf8af//xn\nPfjgg/0dGgAAAFDQDMdxHLeDQOH4y1/+ohdeeEEPPPCA26EAAAAABY+EDmk9/fTTWrRokUzT1ODB\ng/XNb35TDz/8sEpLS9XR0aE5c+Zo4cKFeuihhzRjxgy1tbXpnHPO0e23366XXnpJ9913n6LRqIqL\ni3Xdddfp1FNP1d13361Vq1Zp165d+uxnP6tRo0apqalJN998s95//33deuutam5ulmEYuuKKK/T1\nr39dK1as0K9+9SvV1NTo/fffV3d3t26++WaNHz/e7ZcIAAAAcEXA7QDgbevWrdMvfvELPfPMMxo+\nfLh+//vf6/7779eWLVv04osvqrq6WitWrJAkDR8+XFdffbVeeOEF3X777dq8ebN+9atf6Y9//KMG\nDx6s999/X9/73vf097//XZJUX1+v5557ToFAQHfffbckKRaL6aqrrtK1116rc845Rzt37tTFF1+s\nUaNGSUoM6Zw7d65OOOEEPfzww/rNb35DQgcAAIABi4QOKb366qs666yzNHz4cEnS5ZdfrhNOOEE3\n3HCDqqurUz532bJl2rVrly6//PLk9wzD0NatWyVJY8eOVSDQ+yO4efNmdXV16ZxzzpEkDR06VOec\nc45eeeUVff7zn9eIESN0wgknSJJOPPFEPfPMM/11qgAAAIDvkNAhJcuyZBhGstzZ2akPP/xQpaWl\naZ9r27a+8IUv6Ne//nXyew0NDTrqqKP0j3/846A/w7btT33PcRzFYjFJUnFxcfL7hmGIEcMAAAAY\nyFjlEil9/vOf16uvvqpdu3ZJkh5//HHdeeedh3y8ZVnJ5Gv8+PFatmyZPvjgA0nS4sWL9bWvfU1d\nXV2HfP7o0aMVDAaTwzJ37typF154QRMmTOivUwIAAAAKBj10SOmzn/2s5syZoyuvvFKSVFVVpVtu\nueWQq1ieeuqp+vWvf60f/vCHuueee3Trrbfqpz/9qRzHUSAQ0H333Zeydy8YDOree+/V/Pnzdffd\ndysej+uHP/yhxo8fn5yrBwAAACCBVS4BAAAAwKcYcgkAAAAAPkVCB6DgrF69Wt/97nc/9f2XXnpJ\nF110kS655BI9+eSTLkQGYKCjfQLQ35hDB6CgPPTQQ/qf//kflZSU9Pp+NBrV7bffrqefflolJSWa\nMWOGvvzlL+vII490KVIAAw3tE4BcoIcOQEGpra1NblS/vw8++EC1tbUaNGiQQqGQTjvtNL3xxhsu\nRAhgoKJ9ApALJHQACsq55577qQ3rJSkSiai8vDxZLisrUyQSyWdoAAY42icAueCZIZeNjW0ZP2fw\n4FI1NXXkIJrcIN7cIt7cyibeqqry9A/Kk3A4rPb29mS5vb291wXUoTiOI8MwchkagAGO9gnA4fBM\nQpeNQMByO4SMEG9uEW9u+S3eAx1zzDHasmWLmpubVVpaqn/961+aOXNm2ucZhpHVDScvqaoq9/05\nSIVxHpyDN3jpZpNE+8Q5uI9z8I5s2idfJ3QAkM7f/vY3dXR06JJLLtH111+vmTNnynEcXXTRRRo6\ndKjb4QEYwGifAPQHz2wsnk1G7bdMnHhzi3hzK5t4vXYXPFt+ep8Oxm+ftUMphPPgHLyhUNomifbJ\nCzgHbyiEc5Cya59YFAUAAAAAfIqEDgAAAAB8ioQOAAAAAHyKhA4AAAAAfIqEDgAAAAB9wjMuAAAg\nAElEQVR8ioQOAAAAAHyKhA4AAAAAfIqEDgAAAAB8ioQOAAAAAHyKhA4AAAAAfIqEDgAAAAB8ioQO\nAAAAAHyKhA4AAAAAfIqEDgAAAAB8ioQOAAAAAHyKhA4AAAAAfCqQ7gG2bWvevHlav369QqGQ5s+f\nr1GjRiWPL168WPfcc48cx9GYMWM0d+5cRSIRXXPNNero6FAoFNKdd96pqqqqnJ4IAAAAAAw0aXvo\nXnzxRXV3d+uJJ57Q7NmzdccddySPRSIR3Xnnnbr//vv11FNPqbq6Wk1NTfrLX/6iuro6PfbYYzr/\n/PP1u9/9LqcnAQAAAAADUdqEbuXKlZo4caIkaezYsVqzZk3y2FtvvaW6ujotXLhQ3/72t3XkkUdq\nyJAhqqurU3t7u6RE0hcIpO0IBAAAAABkKG2mFYlEFA6Hk2XLshSLxRQIBNTU1KQVK1bo2WefVWlp\nqS699FKNHTtWgwcP1rJly3T++eerpaVFjz76aNpABg8uVSBgZXwCVVXlGT/HTcSbW8SbW36LFwAA\noNClTejC4XCyt01KzKnr6XGrrKzUSSedlJwfd/rpp2vt2rV6/vnndeWVV2r69Olat26dfvSjH+lv\nf/tbynqamjoyDr6qqlyNjW0ZP88txJtbxJtb2cRLAggAAJBbaYdcjhs3TkuWLJEkrVq1SnV1dclj\nY8aM0YYNG7Rnzx7FYjGtXr1axx57rCoqKlRenriQO+KII3olhAAAAACA/pG2h27KlClatmyZpk+f\nLsdxtGDBAi1atEi1tbWaPHmyZs+erSuvvFKSdN5556murk4//vGP9bOf/UyPPfaYYrGYbrvttpyf\nCIAcseMKfvyqtGe3grEjFD1ygmSy4wkAAIAXpE3oTNPUrbfe2ut7xxxzTPL/U6dO1dSpU3sdHzp0\nqB566KF+ChGAm4K7lqp4y58lc6+K7RLJthUdNsntsAAAACA2FgeQRnDXYpntW6SuPTLbtyi4659u\nhwQAAIB92E8AQGqOLTPaIsViMp2A5DhuRwQAAIB9SOgApGSHjpTjOJIcOY4jO3SE2yEBAABgHxI6\nACk5RUMUqzxFQWuvYvESOUVD3A4JAAAA+5DQAUjJLh4mc+9OyWmWaVTKLh7hdkgAAADYh0VRAKRk\ntb4nq3unFGuX1b1TVusat0MCAADAPv7soWNfLCBvrJYNvebQWS0b3A4JAAAA+/gyoQt+tFgl6++W\nnGaVGJXSZ2OKjvii22EBhSkak9W6TlJclixFw+PcjggAAAD7+LJbq2jLnxWIbJA6P1YgskFFWx51\nOySgYAV3L5WhuCTJUFzB3a+4HBEAAAB6+LKHzoi2SPFOybYlx5QRbXU7JKBgWV0NKcsAAABwjy97\n6OySkTKcqBTvkuFEZZdUux0SULBshVKWAQAA4B5fJnSOFZYdGCQFEl8dK+x2SEDBso2jUpYBAADg\nHl8OuZRpygmGJaNYjhNghUsghwwjIjn7l9vdCwYAAAC9+DKhs4OVMjt3S85emUaJ7OBgt0MCCpZl\n705ZBgAAgHt8mdAFmt6SEWuTnKgMI6ZA0yq3QwIKWOyActSVKAAAAPBpvhyraLWsTSyKYhgynKis\nlnfdDgkoWHaaMgAAANzjy4ROgXI5hiXJSHwNlLsdEVCwDBWnLAMAAMA9vhxy2VX7DZkffizT6ZRt\nFKur9htuhwQULCdNGQAAAO7xZULXecx/ymrfrEDnRnUXH6vOY77vdkhAwTLVmbLsNbZta968eVq/\nfr1CoZDmz5+vUaNGJY8//PDDeu6552QYhn7wgx9oypQpLkYLYKCgbQKQK75M6II7/6ngzpekaJOC\nwa0K7nxF0ZrJbocFwANefPFFdXd364knntCqVat0xx136L777pMktba26o9//KP+/ve/a+/evfr6\n17/ORROAvKBtApArvpxDV7pqngLtG6Xu3Qq0b1Tpqp+5HRIAj1i5cqUmTpwoSRo7dqzWrFmTPFZS\nUqIRI0Zo79692rt3rwzDcCtMAAMMbROAXPFlD52194MDyh+6FAlQ+BwFZOy3dYHj8WYjEokoHA4n\ny5ZlKRaLKRBIxD18+HBNnTpV8Xhcs2bN6tPPrKry/8JLhXAOUmGcB+cwMOWibZIK473gHLyBc/Av\nb1+ZHdKByzKwkDqQK46KpF4JXZF7wfRBOBxWe3t7smzbdvKCacmSJdq1a5f+7//+T5I0c+ZMjRs3\nTieffHLKn9nY2Ja7gPOgqqrc9+cgFcZ5cA7e4MZFXy7aJon2yQs4B28ohHOQsmuffDnk8sAeAq/3\nGAD+tveAsrcXRRk3bpyWLFkiSVq1apXq6uqSxwYNGqTi4mKFQiEVFRWpvLxcra2tboUKYAChbQKQ\nKz7NhLrTlAH0F/OAHnBTcZci6ZspU6Zo2bJlmj59uhzH0YIFC7Ro0SLV1tZq8uTJWr58uaZNmybT\nNDVu3Dj927/9m9shAxgAaJsA5IrhOI4ntpXKpIt0yJNVMtUlQ4nBl7aKtGdaY85i6y9+6wom3tzy\nS7xHPFkhQ0r+vjmSdk/r253jQhnL7of3KRW/fNbSKYTz4By8oVDaJon2yQs4B28ohHOQBtSQy+KU\nZQAAAAAYCHyZ0MVLRqUsA+g/By45xBJEAAAA3uHLOXRGQNo/FzV8eRaAPxx418eXd4EAAAAKlD9T\nIatIMoJK9BWYiTKAnHBkytivX84hpQMAAPCMtAmdbduaN2+e1q9fr1AopPnz52vUqE+GOC5evFj3\n3HOPHMfRmDFjNHfuXNm2rdtvv11r1qxRd3e3fvSjH+lLX/pSvwUdLx2lQGSDDCcmxwgoXsqQSyBX\nbB0pU7v2K1e5GA0AAAD2l/ZW+4svvqju7m498cQTmj17tu64447ksUgkojvvvFP333+/nnrqKVVX\nV6upqUl//etfFYvF9Pjjj+u+++7Tli1b+jXoWOVJss1iybBkm8WKVabfeBNAdkw1H1BucikSAAAA\nHChtD93KlSs1ceJESdLYsWO1Zs2a5LG33npLdXV1WrhwobZt26aLL75YQ4YM0dKlS3Xcccfp+9//\nvhzH0U033dSvQZtdu2XY3ZLdLUOWzK7d/frzAewvlqYMAAAAt6RN6CKRiMLhcLJsWZZisZgCgYCa\nmpq0YsUKPfvssyotLdWll16qsWPHqqmpSVu3btUDDzygN954QzfccIMeffTRfgs62PhPmbE2yZDM\nWJuCjS/3288GcCAjTRkAAABuSZvQhcNhtbe3J8u2bSsQSDytsrJSJ510kqqqEnNqTj/9dK1du1aV\nlZX64he/KMMwdOaZZ2rz5s1pAxk8uFSBgNW3qGNtkuKS48iQoVCszTebhPolzh7Em1v+iDcgKS4p\nkcpZCvgkbgAAgMKXNqEbN26cXn75ZZ1//vlatWqV6urqksfGjBmjDRs2aM+ePaqoqNDq1as1bdo0\n7dmzR4sXL9a5556rdevWafjw4WkDaWrq6HPQlbahgCRDhhxJMdtQsw92hvfbDvbEm1t+iXewYrKU\nSOYcSXHF1NTHuEn8AAAAcittQjdlyhQtW7ZM06dPl+M4WrBggRYtWqTa2lpNnjxZs2fP1pVXXilJ\nOu+881RXV6ejjz5ac+fO1bRp0+Q4jm655ZZ+DTpeerSszs0yZMuRqXjp6H79+QA+YRwwxPLAMgAA\nANyTNqEzTVO33nprr+8dc8wxyf9PnTpVU6dO7XU8FArp9ttv76cQP82IR/ZdUiYuLY2493s5AP+y\nDyg7rkQBAACAT/PlxuJGrF2OAskeOiPWnv5JALJ0YEIXdyUKAAAAfFrafei8yCk9SgoUS8EyKVCc\nKAPIkQMXK+rj4kUAAADIOV8mdHuP+S/ZgXLJjssOlGvvMT9yOySgYDlW1QFlbqAAAAB4hS8TOqt9\nvZxQpRSulhOqlNW+zu2QgIIVLx6RsgwAAAD3+HIOnRXZmEjoQgE53TFZkY1uhwQULLNz1wHlnS5F\nAgAAgAP5socuXlGXsgyg/xhmV8//DigDAADAbb5M6DprvisnHpf2bJATj6uz5nK3QwIKll1Uo0+2\nFTf2lQEAAOAFvkzoyt67RYHOeikQUqCzXmXv3ex2SEDBskuHSkZAkiUZAdmlw9wOCQAAAPv4MqGz\nWtelLAPoR7FuOY4kxRNfY90uBwQAAIAevkzo4hXHpywD6D9W+wcyFJUkGYrKamcRIgAAAK/w5SqX\n7Z+7TWbHDll7P1A0fIzaPzff7ZCAgmXEO1KWAQAA4B5f9tAFm1+XE6qQhhwvJ1ShYPPrbocEFC7z\ngPs+ZtCdOAAAAPApvuyhCza8qODHr0pOp4JGseyioYoOm+R2WEBBcuLWAWVf3gcCAAAoSL5M6ALN\nq2R17ZTkyJKhQPObbocEFC7LkOKfbFsgy3A7IgAAAOzjz1vtsb37Vt3TvlX39roaDlDInKJB6tlU\nXDLkFFW6GQ4AAAD248uEzi4dJccqk6xSOVaZ7NJRbocEFCzHrDigXO5SJAAAADiQLxO6rppLZAcG\nSZLswCB11cxwOSKggHW1yZEtSYmvXW0uBwQAAIAevkzorI4NMkLFUmmVjFCxrA42FgdyxYrv2m/A\nZaIMAAAAb/BnQtf2/gFlNjoGcsaw9ElTYe4rAwAAwAt8ucqlZMnYu1MybBmOKZ/mpYAvxEtHyur+\nSInfM0Px0pFuhwQAAIB9fJkJOVY4sSiKGZRjlcmxwm6HBBSs2ODPK25VSEZIcatCscGfdzskAAAA\n7OPPHjrLlBMMS2axHDsgWQwBA3KmKCR78IlSKCC7OyYVhdyOCAAAAPv4MqGzrQqZe+uleKdMq1i2\nxb5YQK5Ej/qizI4dkrVX8UCJokd90e2QAAAAsI8vh1yGGl6UGW2T7C6Z0TaFGv7udkhAwYoOOVNO\nICx1tcgJhBUdMt7tkAAAALCPLxM6q/0DSbYSi6jbstpZ5RLIleIPf6vQ7uVSxw6Fdi9X8YcPuh0S\nAAAA9vFlQmcHKtSz4p5k7isDyIXgrqUyuluk2F4Z3S0K7lrqdkgAAADYx5cJXVftdMWtsGQEFLf+\nf3v3HxxVefd9/LNnz25IdgMxZAHFBim4to3whEh7+2iDtpSRijNtcfghFLUt87R12umUjNpxRmAy\nNEJ9mOloUcdxxNYqBesM09qZ9paWkpZyYxsJNN5ARjoiYoUQwo/dJPvrnOePxOXJbbqLNpuz1+b9\nmnGyV67d5HMR8nW/nHOuE1ai9i6vIwEly/VXSJl+KRWTMv0DO8wCAACgKBjZ0GUmzpFb+XEpNEVu\n5ceVmTjH60hAyUpX1Q80db6AXH+F0lX/y+tIAAAAGGRkQxc40yqfm5TscfK5SQXOtHodCShZ7rhq\nZSpnSuGpylTOlDtuoteRAAAAMMjI2xZY8RPy9Z2SfI58riUrfsLrSEDJ8vV3yT7/huT0ybbKlYzc\n6nWknBzH0fr163X06FEFg0Ft2LBB06ZNy87v2bNHW7Zskeu6qqur07p16+Tz+TxMDGAsoDYBKJS8\nR+gcx9HatWu1bNkyrVq1SsePHx8yv2fPHi1dulRLlizR+vXr5bpudu7YsWO64YYblEgkRjZ1JiNl\nElK6f+BjJjOyXx9AVvC938tK9kjpXlnJHgXf2+V1pJx27dqlZDKp7du3q6mpSRs3bszOxWIxPfro\no3rqqaf00ksvaerUqerp6fEwLYCxgtoEoFDyNnQftQDFYjFt2rRJwWBw5FM7rpRJXfrPcUb+ewCQ\nJFm970hyJZ8lyZXVW9xHxNva2tTY2ChJqq+vV0dHR3buwIEDikaj2rRpk1asWKGamhpVV1d7FRXA\nGEJtAlAoeU+5vNwCdOLECS1ZskTV1dVyXVcPP/yw1qxZo/vuu2/EQ1uZHll+nyS/LPlkZfhXLKBQ\nnLIr5fYdl8915MqSU3al15FyisViCofD2bHf71c6nZZt2+rp6dH+/fu1c+dOVVRUaOXKlaqvr9f0\n6dM9TAxgLKA2ASiUvA3dRylAr7zyim655RZ94hOfKEho1y6X4w/J73PkuJZcu7wg3weAlAnPVODc\na5IykixlwjO9jpRTOBxWPB7Pjh3HkW0PlLqqqirNmjVLkUhEkjR37lwdPnw475umSKSycIFHSSms\nQSqNdbCGsakQtUkqjZ8FaygOrMFceRu6j1KAfvWrX2nKlCl6+eWX1dXVpa9//et64YUXcn6fK66o\nkG37Ly/1NfOleKeU7pXfrpD/mi9onCE/QNP+opG3sIzIm+iU/AFJtiz5VJHoVEUR525oaNDu3bt1\n++23q729XdFoNDtXV1enzs5OnT17VuPHj9fBgwe1dOnSvF+zq+tiISMXXCRSafwapNJYB2soDl7U\n3kLUJon6VAxYQ3EohTVIH60+5W3oPkoBevXVV7PP+fznP69nn302b5Cent7LDh2woyovv0Zlzjkl\nrCr12dcqZcAP0LS/aOQtLFPyTkhlZLuS5Ru4fDWdyuj8Zeb24k3TggULtHfvXi1fvlyu66qlpUVb\nt25VbW2t5s+fr6amJq1evVqStHDhwiE1DQAKhdoEoFDyNnTFWICs5HtK13xaZaEypeMJWcn3Cv49\ngbEqPb5O9vlDkpOSfAGlx9d5HSkny7LU3Nw85HMzZszIPl60aJEWLVo02rEAjHHUJgCFkreh+3cL\n0B/+8Id/I97wnIqr5Y/9Y8gYQGH4Uucln1+SI/n88qUueB0JAAAAg4y8sXiq5qaBB3a3UqGJl8YA\nRtzAbQqcwdsWOLJ63/Y6EgAAAAYZ2dBJbv6nABgZ/nL53IwkVz75JD+7ygIAABQLIxu6wOk/q+yt\nbZK/T2WZcslxlJoyz+tYQElyyifJvVAun9JyZcspn+R1JAAAAAwys6F7b7fscwclX1q2a8t5bwoN\nHVAgzrir5Fjlspw+OVa5nHFTvY4EAACAQUY2dFbfSVmpC5LfkpVxZPWd9DoSULJ8Tq98ykh+Wz43\nI58Tz/8iAAAAjAojGzqn4mq5wQmSLy3Xb7PLJVBQjtzyyQO/b64tyfE6EAAAAAYZ2dClJn9O/v5/\nyrb6lHbKlZr8Oa8jASUrE/64dHq35PZLvnHKhGd6HQkAAACDLK8DfBSpqhukxAXp7BEpcUGpqs94\nHQkoXY4kx5Ecd/AjR+gAAACKhZENXejQwwp075f6TivQvV+hQw95HQkoWf7Ym3IDYalsgtxAWP7Y\nm15HAgAAwCAjT7m0u/bJlz6v9++LZXft8zoSULr8wSGbEMkf9DoRAAAABhnZ0PncfvnctCSffHLl\nc/u9jgSUrHR1g/zxY/I755S2qpSubvA6EgAAAAYZ2dBlQjPlT7wnn5uW67OVCbFJA1AoTrhW6Ymf\nVlmoTOl4Qk641utIAAAAGGRkQ5eK3CT7/MHsjY5Tkc96HQkoWamamwYe2N1KhSZeGgMAAMBzRjZ0\nsvySf5zkl6RxkuXzOhFQwlyvAwAAAOBfMLKh88f/ISc0VQracpJp+eP/8DoSULICp/+scce3SVaf\nxjnlkuMoNWWe17EAAAAgQ29bkBkfzTkGMHICp/fIih+XEmdlxY8rcPqPXkcCAADAICOP0PXP+D+S\npGD6LSXsa7JjAAAAABhLjGzoZNvqv+4+VUYq1d910es0QElLTbpVVu+7kr9PGbtcqUm3eh0JAAAA\ng8xs6ACMmlTN/5b/fIeUfktp+xp2uQQAACgiRl5DB2D0BM7ul5U6J1VMlpU6p8DZ//I6EgAAAAaZ\neYTOyShwZp90tluB9OB9sSx6U6AQrPjb8sfflvr65HfKlYmf8DoSAAAABhnZ0AXO7FPgdKsUKlMg\n/oYkKTWJm4sDheBLnJW/55BkpeV3bPkmXO91JAAAAAwysqGzet/JOQYwcqy+0/KlY5LbL59vnKy+\nLq8jAQAAYJCR5yk6FVfnHAMYOVb/P+VzM5K/TD43I6v/Xa8jAQAAYJCRR+iyu+zZ3UqFJrLrHlBA\nTnCylElJ6T7JVy4nOMXrSAAAABhkZEMnyxq4Zi5SqRT3oQMKypeJyef0Sm5GPl+vfBl+5wAAAIqF\nmQ0du1wCo8bq/6dcq0zyOXJdS1b/P72OBAAAgEFGNnTscgmMHjcwXvKPk/yWlHEGxgAAACgKRjZ0\n7HIJjJ7EtJWy+s/I755T2lelxLSVXkcCAADAICMbOmfclQqe+mP2RsepGo7OAYWSmjJPsm2V2d3q\nS7MJEQAAQDExsqGTfHLdgUfvfwRQIGxCBAAAULTyNnSO42j9+vU6evSogsGgNmzYoGnTpmXn9+zZ\noy1btsh1XdXV1WndunWKxWK6//77FYvFlEql9IMf/EBz5swZsdBW/7tywrVSqExOPMF9sYBCYhMi\nAACAopX3XdmuXbuUTCa1fft2NTU1aePGjdm5WCymRx99VE899ZReeuklTZ06VT09Pdq6datuvPFG\n/fznP9cjjzyi5ubmEQ3NjcWB0ZPdhOj8mwqcblXgzF+8jgQAAIBBeY/QtbW1qbGxUZJUX1+vjo6O\n7NyBAwcUjUa1adMmnThxQkuWLFF1dbXuvfdeBYNBSVImk1FZWdmIhubG4sDoYRMiAACA4pW3oYvF\nYgqHw9mx3+9XOp2Wbdvq6enR/v37tXPnTlVUVGjlypWqr6/X9OnTJUldXV26//779dBDD+UNcsUV\nFbJt/+Unn/xFSVLV5b+iKEQilV5H+FDIW1hG5E1GpZMnJUmhUJl0VVQyITcAAMAYkLehC4fDisfj\n2bHjOLLtgZdVVVVp1qxZikQikqS5c+fq8OHDmj59uo4ePao1a9bogQce0Gc+85m8QXp6ei8/9eA1\nPVV2t84ZdE1PJFKpLoM2lSBvYRmT165XINR76ffNrpcuM7cRDSsAAIDB8nZBDQ0Nam1tlSS1t7cr\nGo1m5+rq6tTZ2amzZ88qnU7r4MGDmjlzpt58801973vf0+bNm3XLLbeMeGiu6QFG0fu7XF731YGP\nBvzjCQAAwFiR9wjdggULtHfvXi1fvlyu66qlpUVbt25VbW2t5s+fr6amJq1evVqStHDhQkWjUX37\n299WMpnUD3/4Q0kDR/mefPLJEQvNNT3AKGKXSwAAgKKVt6GzLOsDu1TOmDEj+3jRokVatGjRkPmR\nbN6G41RcLX/sH0PGAAoje0Q8VKZA/A1JGjhSBwAAAM8ZeWNxdrkERg9HxAEAAIqXkQ1d9pqeSKVS\nJmwqARiMI+IAAADFy8wLYZyMAqf/LB39+cBHx/E6EVCyUtX/ISdQJfWekhOoUqr6Rq8j5eQ4jtau\nXatly5Zp1apVOn78+LDPWb16tbZt2+ZBQgBjEbUJQKEY2dCxyyUwegJn/iK755DUe0p2z6Gi/33b\ntWuXksmktm/frqamJm3cuPEDz/nxj3+sCxcueJAOwFhFbQJQKEaecsk1PcDoCZzarcD5Q5IvrYBr\nyzl1pVJT5nkd619qa2tTY2OjJKm+vl4dHR1D5n/729/K5/NlnwMAo4HaBKBQjGzouKYHGD1W/ISs\n/lOSHFmyZMXf9jpSTrFYTOFwODv2+/1Kp9OybVudnZ165ZVX9Nhjj2nLli2X/TVL4QbppbAGqTTW\nwRrGpkLUJqk0fhasoTiwBnMZ2dCxyyUwilzJdSX5Bj+6XgfKLRwOKx6PZ8eO48i2B0rdzp07derU\nKd1zzz06efKkAoGApk6dqnnzch9x7DJ886VIpNL4NUilsQ7WUBy8eNNXiNokUZ+KAWsoDqWwBumj\n1ScjGzp2uQRGjxP6mJyLk+T3peW4tpzQx7yOlFNDQ4N2796t22+/Xe3t7YpGo9m5Bx54IPv48ccf\nV01NzWW9YQKAfxe1CUChmNnQARg1qSmfl9X/ngL+PqUz5UpN+bzXkXJasGCB9u7dq+XLl8t1XbW0\ntGjr1q2qra3V/PnzvY4HYIyiNgEoFBo6ADmlJn1WsiyV291KpIv/FGfLstTc3DzkczNmzPjA8777\n3e+OViQAoDYBKBgaOgC5cYozAABA0TLyPnQAAAAAABo6AAAAADAWDR0AAAAAGIqGDgAAAAAMRUMH\nAAAAAIaioQMAAAAAQ9HQAQAAAIChaOgAAAAAwFA0dAAAAABgKBo6AAAAADAUDR0AAAAAGMr2OgCA\nIudkFDizTzrbrUB6olI1N0kW/xYEAABQDMxs6HiDCYyawJl9CpxulUJlCsTfkCSlJn3W41QAAACQ\nDG3oeIMJjB6r952cYwAAAHjHyMNavMEERo9TcXXOMQAAALxj5BE6Z9yVCp76o9TXJ79TrlQNR+eA\nQknV3DTwwO5WKjTx0hgAAACeM7Khk3xy3YFH738EUCCWNXBKc6RSqa6LXqcBAADA/8fIhs7qf1dO\nuFYKlcmJJ2T1v+t1JAAAAAAYdUZeQ8c1PQAAAABwGUfoHMfR+vXrdfToUQWDQW3YsEHTpk3Lzu/Z\ns0dbtmyR67qqq6vTunXrlEgkdP/996u7u1uhUEibNm1SdXX1iIVOVf+H/D2HpN635ASuUar6xhH7\n2gAAAABgirxH6Hbt2qVkMqnt27erqalJGzduzM7FYjE9+uijeuqpp/TSSy9p6tSp6unp0bZt2xSN\nRvXiiy/qy1/+sp544okRDR04u19W6pxUMVlW6pwCZ/9rRL8+AAAAAJggb0PX1tamxsZGSVJ9fb06\nOjqycwcOHFA0GtWmTZu0YsUK1dTUqLq6eshr5s2bp3379o1saG5bAAAAAAD5T7mMxWIKh8PZsd/v\nVzqdlm3b6unp0f79+7Vz505VVFRo5cqVqq+vVywWU2VlpSQpFArp4sX8O+NdcUWFbNt/eamTUenk\nycGvXyZdFZUilZf3Wo9FDMn5PvIWFnkBAADw78jb0IXDYcXj8ezYcRzZ9sDLqqqqNGvWLEUiEUnS\n3Llzdfjw4SGvicfjGj9+fN4gPT29HyJ1vQKhXlXZ3TqXnqiUXS8ZsJ16JFKpLgNyvo+8hTUW8tIA\nAgAAFFbeUy4bGhrU2toqSWpvb1c0Gs3O1dXVqbOzU2fPnlU6ndbBgwc1c+ZMNRJ9IgQAAA/qSURB\nVDQ0aM+ePZKk1tZW3XDDDSOcevC+WNd9deCjZeRmnQAAAADwb8l7hG7BggXau3evli9fLtd11dLS\noq1bt6q2tlbz589XU1OTVq9eLUlauHChotGoPvaxj+nBBx/UXXfdpUAgoM2bN49saiejwJl90tlu\nBdITlaq5iaYOAAAAwJiTt6GzLEvNzc1DPjdjxozs40WLFmnRokVD5svLy/XYY4+NUMQPCpzZp8Dp\nVilUpkD8DUkaOFIHAAAAAGOIkYe12OUSAAAAAAxt6JyKq3OOAQAAAGAsyHvKZTFK1dw08MDuVio0\n8dIYAAAAAMYQI4/QSa7XAQAAAADAc0YeoWNTFGAUsassAABA0TKyoWNTFGD0BN7bo/Kjj0vuOZX7\nqqTr0kpddavXsQAAACBDT7l0xl0pf/xt6cwh+eNvyxl3ldeRgJJVdnyb7Fin1H9GdqxTZcdf8DoS\nAAAABhl5hE7yyR28jM7lcjqgoHyp81KmX3IcybXkS13wOhIAAAAGGXmEzuo98T/GnHIJFIpTcbV8\nvoHHPp/kVEz1NhAAAACyjGzofMkeBc4fknoOK3D+kHzJs15HAkpWOny9MtYEyZUy1gSlw7O9jgQA\nAIBBRjZ0VqJ7yCmXVqLb20BACbNj/y1LCSkYlqWE7FiH15EAAAAwyMxr6Hw+ucEJUtCWm0wrez4Y\ngJHnpgd+33xpuX5bctNeJwIAAMAgI4/QpWoa5frGScmYXN84pWrmeR0JKFmZypk5xwAAAPCOmUfo\nLL/cismSNV6uU85NjoECyoy/XpngZNnuOWV8VcqMv97rSDk5jqP169fr6NGjCgaD2rBhg6ZNm5ad\nf+655/Sb3/xGknTLLbfoO9/5jldRAYwh1CYAhWJkJ2T1v6tMqFaqma1MqFZW/7teRwJKlpV8T+nI\np6Xpdygd+bSs5HteR8pp165dSiaT2r59u5qamrRx48bs3IkTJ/SrX/1Kv/jFL7Rjxw79+c9/1pEj\nRzxMC2CsoDYBKBQjGzqn4uqcYwAjxwlEFHz3P6XDP1Xw3f+UE5jsdaSc2tra1NjYKEmqr69XR8el\nTVymTJmiZ555Rn6/Xz6fT+l0WmVlZV5FBTCGUJsAFIqRp1ymam4aeGB3KxWaeGkMYMQFT/5GVvyE\n5HNkJeIKnvy1UlM/53WsfykWiykcDmfHfr9f6XRatm0rEAiourparuvqRz/6kT71qU9p+vTpeb9m\nJFJZyMijohTWIJXGOljD2FSI2iSVxs+CNRQH1mAuIxs6WZZSkz4rRSqV6rrodRqgpPljnZI9TvJb\nUsYZGBexcDiseDyeHTuOI9u+VOoSiYQeeughhUIhrVu37rK+ZpfhdSYSqTR+DVJprIM1FAcv3vQV\nojZJ1KdiwBqKQymsQfpo9cnIUy4BjJ7M+E/kHBebhoYGtba2SpLa29sVjUazc67r6r777tN1112n\n5uZm+f1+r2ICGGOoTQAKxcwjdABGTXx2iySpov9NJcbNzI6L1YIFC7R3714tX75cruuqpaVFW7du\nVW1trRzH0WuvvaZkMqk//elPkqQ1a9Zozpw5HqcGUOqoTQAKhYYOQG7BoOJz/68qIpWKG3Aqg2VZ\nam5uHvK5GTNmZB///e9/H+1IAEBtAlAwnHIJAAAAAIaioQMAAAAAQ9HQAQAAAIChaOgAAAAAwFA0\ndAAAAABgKBo6AAAAADAUDR0AAAAAGIqGDgAAAAAMRUMHAAAAAIay8z3BcRytX79eR48eVTAY1IYN\nGzRt2rTs/IYNG/T6668rFApJkp544gldvHhRDzzwgFzX1YQJE7R582aVl5cXbhUACsfJKHBmn3S2\nW4H0RKVqbpIs/i0IAACgGOR9V7Zr1y4lk0lt375dTU1N2rhx45D5N954Q88884yef/55Pf/886qs\nrNRzzz2nL37xi3rhhRd07bXX6pe//GXBFgCgsAJn9ilwulU6/6YCp1sVOPMXryMBAABgUN6Grq2t\nTY2NjZKk+vp6dXR0ZOccx9Hx48e1du1aLV++PNu4ffKTn9SFCxckSbFYTLad90AggCJl9b6TcwwA\nAADv5O20YrGYwuFwduz3+5VOp2Xbtnp7e/XVr35VX/va15TJZHT33Xfr+uuv15QpU7R582a98sor\nSiaT+s53vpM3yBVXVMi2/R96AZFI5Yd+jZfIW1jkLYBkVDp5UpIUCpVJV0UlE3IDAACMAXkbunA4\nrHg8nh07jpM94lZeXq677747e33cjTfeqCNHjuinP/2pHnnkETU2NuqPf/yjHnzwQT399NM5v09P\nT++HDh+JVKqr6+KHfp1XyFtY5C0Qu16BUK+q7G6dS09Uyq6XLjO3EQ0rAACAwfKectnQ0KDW1lZJ\nUnt7u6LRaHburbfe0l133aVMJqNUKqXXX39ddXV1Gj9+vCorB97ITZo0KXv6JQATuV4HAAAAwL+Q\n9wjdggULtHfvXi1fvlyu66qlpUVbt25VbW2t5s+fry996UtaunSpAoGAvvSlL+naa6/Vww8/rObm\nZjmOI9d1tXbt2tFYC4ACyG6KEipTIP6GJCk16bMepwIAAIB0GQ2dZVlqbm4e8rkZM2ZkH69evVqr\nV68eMj9z5kz97Gc/G6GIALzEpigAAADFi5tJAcjJqbg65xgAAADe4X4CAHJK1dw08MDuVio08dIY\nAAAAnqOhA5CbZQ1cMxepVMqEXTkBAADGEE65BAAAAABD0dABAAAAgKFo6AAAAADAUDR0AAAAAGAo\nGjoAAAAAMBQNHQAAAAAYioYOAAAAAAxFQwcAAAAAhqKhAwAAAABD0dABAAAAgKFo6AAAAADAUDR0\nAAAAAGAoGjoAAAAAMBQNHQAAAAAYioYOAAAAAAxFQwcAAAAAhqKhAwAAAABD0dABAAAAgKFo6AAA\nAADAUDR0AAAAAGAoGjoAAAAAMBQNHYCS4jiO1q5dq2XLlmnVqlU6fvz4kPkdO3Zo8eLFWrp0qXbv\n3u1RSgBjDbUJQKHYXgcAgJG0a9cuJZNJbd++Xe3t7dq4caOefPJJSVJXV5eef/55vfzyy0okElqx\nYoVuvvlmBYNBj1MDKHXUJgCFwhE6ACWlra1NjY2NkqT6+np1dHRk5w4dOqQ5c+YoGAyqsrJStbW1\nOnLkiFdRAYwh1CYAhUJDB6CkxGIxhcPh7Njv9yudTmfnKisrs3OhUEixWGzUMwIYe6hNAAqlaE65\njEQq8z9pBF/nFfIWFnkLy4S84XBY8Xg8O3YcR7ZtDzsXj8eHvIn6V0xYdz6lsAapNNbBGsamQtQm\nqTR+FqyhOLAGc3GEDkBJaWhoUGtrqySpvb1d0Wg0Ozd79my1tbUpkUjo4sWLOnbs2JB5ACgUahOA\nQvG5rut6HQIARorjOFq/fr06Ozvluq5aWlrU2tqq2tpazZ8/Xzt27ND27dvluq6++c1v6rbbbvM6\nMoAxgNoEoFBo6AAAAADAUJxyCQAAAACGoqEDAAAAAEPR0AEAAACAoYq+oXMcR2vXrtWyZcu0atUq\nHT9+fMj8jh07tHjxYi1dulS7d+/2KOUl+fI+99xzWrJkiZYsWaKf/OQnHqW8JF/e95+zevVqbdu2\nzYOEH8ySK++ePXu0dOlSLVmyROvXr5fXl4jmy/vss89q8eLFuvPOO/Xqq696lPKDDh48qFWrVn3g\n83/4wx905513atmyZdqxY4cHyQrHtFozHNPqz3BMq0nDMa1ODcfU2jUc0+sZtak4lEJtkqhPxWbE\n6pNb5H73u9+5Dz74oOu6rnvgwAH3W9/6Vnbu9OnT7h133OEmEgn3woUL2cdeypX37bffdr/yla+4\n6XTadRzHXbZsmXv48GGvorqumzvv+zZv3uwuWbLEffHFF0c73gfkynvx4kV30aJFbnd3t+u6rvv0\n009nH3slV97z58+7t9xyi5tIJNxz5865t956q1cxh3j66afdO+64w12yZMmQzyeTSfcLX/iCe+7c\nOTeRSLiLFy92u7q6PEo58kyrNcMxrf4Mx7SaNBzT6tRwTKxdwymFekZtKg6lUJtcl/pUTEayPhX9\nEbq2tjY1NjZKkurr69XR0ZGdO3TokObMmaNgMKjKykrV1tbqyJEjXkWVlDvvlClT9Mwzz8jv98vn\n8ymdTqusrMyrqJJy55Wk3/72t/L5fNnneC1X3gMHDigajWrTpk1asWKFampqVF1d7VVUSbnzlpeX\n66qrrlJfX5/6+vrk8/m8ijlEbW2tHn/88Q98/tixY6qtrdWECRMUDAZ1ww036K9//asHCQvDtFoz\nHNPqz3BMq0nDMa1ODcfE2jWcUqhn1KbiUAq1SaI+FZORrE92oUKOlFgspnA4nB37/X6l02nZtq1Y\nLKbKykt3hA+FQorFYl7EzMqVNxAIqLq6Wq7r6kc/+pE+9alPafr06R6mzZ23s7NTr7zyih577DFt\n2bLFw5SX5Mrb09Oj/fv3a+fOnaqoqNDKlStVX1/v6Z9xrrySdOWVV2rRokXKZDL65je/6VXMIW67\n7Ta98847H/h8Mf6+jSTTas1wTKs/wzGtJg3HtDo1HBNr13BKoZ5Rm4pDKdQmifpUTEayPhV9QxcO\nhxWPx7Njx3GyP7D/ORePx4f8AXghV15JSiQSeuihhxQKhbRu3TovIg6RK+/OnTt16tQp3XPPPTp5\n8qQCgYCmTp2qefPmeRU3Z96qqirNmjVLkUhEkjR37lwdPnzY00KUK29ra6tOnz6t3//+95Kkb3zj\nG2poaNDs2bM9yZpPMf6+jSTTas1wTKs/wzGtJg3HtDo1nFKqXcMx5XdaojYVi1KoTRL1yQQf5fe6\n6E+5bGhoUGtrqySpvb1d0Wg0Ozd79my1tbUpkUjo4sWLOnbs2JB5L+TK67qu7rvvPl133XVqbm6W\n3+/3KmZWrrwPPPCAXnrpJT3//PP6yle+onvvvdfz4pQrb11dnTo7O3X27Fml02kdPHhQM2fO9Cqq\npNx5J0yYoHHjxikYDKqsrEyVlZW6cOGCV1HzmjFjho4fP65z584pmUzqb3/7m+bMmeN1rBFjWq0Z\njmn1Zzim1aThmFanhlNKtWs4JtUzalNxKIXaJFGfTPBR6lPRH6FbsGCB9u7dq+XLl8t1XbW0tGjr\n1q2qra3V/PnztWrVKq1YsUKu6+r73/++5+dd58rrOI5ee+01JZNJ/elPf5IkrVmzxtP/ieT78y02\n+fI2NTVp9erVkqSFCxd6/j+2fHn/8pe/aOnSpbIsSw0NDbr55ps9zTucX//61+rt7dWyZcv0gx/8\nQN/4xjfkuq7uvPNOTZ482et4I8a0WjMc0+rPcEyrScMxrU4NpxRq13BMrGfUpuJQCrVJoj4Vs3+n\nPvlctwj3IwUAAAAA5FX0p1wCAAAAAIZHQwcAAAAAhqKhAwAAAABD0dABAAAAgKFo6AAAAADAUDR0\nAAAAAGAoGjoAAAAAMBQNHQAAAAAY6v8BOppj4/BWqZsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a5ef5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "parameters = ['n_estimators', 'max_depth', 'max_features', 'criterion']\n",
    "f, axes = plt.subplots(nrows=2, ncols=3, figsize=(15,10))\n",
    "\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "for i, val in enumerate(parameters):\n",
    "    print i, val\n",
    "    xs = np.array([t['misc']['vals'][val] for t in trials.trials]).ravel()\n",
    "    ys = [-t['result']['loss'] for t in trials.trials]\n",
    "    #xs, ys = zip(sorted(zip(xs, ys)))\n",
    "    ys = np.array(ys)\n",
    "    axes[i/3,i%3].scatter(xs, ys, s=20, linewidth=0.01, alpha=0.5, c=cmap(float(i)/len(parameters)))\n",
    "    axes[i/3,i%3].set_title(val)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic steps to hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into a development and evaluation set\n",
    "X_dev,X_eval, y_dev,y_eval = train_test_split(df_X, df_y,\n",
    "                                              test_size=0.33, random_state=seed)\n",
    "# Split development set into a train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.33,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Handle model fit and feature importance\n",
    "\n",
    "def modelfit(alg, y, X, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    \n",
    "    # Customize the major grid\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid(which='major', linestyle='-', linewidth='0.2', color='gray')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    #Extract sample weights from datasest\n",
    "    sample_weights = X[\"globalTimesEventWeight\"].values\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X.drop('globalTimesEventWeight', axis=1, inplace=False), y)\n",
    "        \n",
    "    #Predict training set:\n",
    "    predictions = alg.predict(X.drop('globalTimesEventWeight', axis=1, inplace=False))\n",
    "    predprob = alg.predict_proba(X.drop('globalTimesEventWeight', axis=1, inplace=False))[:,1]\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(alg, \n",
    "                                   X.drop('globalTimesEventWeight', axis=1, inplace=False), \n",
    "                                   y, cv=cv_folds, scoring='roc_auc',\n",
    "                                   fit_params={'sample_weight': sample_weights})\n",
    "    \n",
    "    #Print model report:\n",
    "    print \"\\nModel Report\"\n",
    "    print \"Accuracy : %.4g\" % accuracy_score(y, predictions, sample_weight=sample_weights)\n",
    "    print \"AUC Score (Train): %f\" % roc_auc_score(y, predprob, sample_weight=sample_weights)\n",
    "    \n",
    "    if performCV:\n",
    "        print \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),\n",
    "                                                                                 np.std(cv_score),\n",
    "                                                                                 np.min(cv_score),\n",
    "                                                                                 np.max(cv_score))\n",
    "\n",
    "    # Extract features set\n",
    "    features_set = X.drop('globalTimesEventWeight', axis=1, inplace=False).columns.values\n",
    "    \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_, features_set).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Out-of-the-box model\n",
    "\n",
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "\n",
    "modelfit(gbm0, y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "\n",
    "param_test1 = {'n_estimators':range(20, 200, 10)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=200,\n",
    "                                                               min_samples_leaf=50, max_depth=8, \n",
    "                                                               max_features='sqrt', subsample=0.8,random_state=10), \n",
    "                        param_grid = param_test1, scoring='roc_auc',n_jobs=-1, iid=False, cv=5)\n",
    "gsearch1.fit(X_train.drop('globalTimesEventWeight', axis=1, inplace=False), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Tuning tree-specific parameters\n",
    "\n",
    "param_test2 = {'max_depth':range(2, 10, 2), 'min_samples_split':range(100, 201, 20)}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=30, max_features='sqrt',\n",
    "                                                               min_samples_leaf=50, subsample=0.8, random_state=10), \n",
    "                        param_grid = param_test2, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "gsearch2.fit(X_train.drop('globalTimesEventWeight', axis=1, inplace=False), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take the max_depth of 7 as optimum and not try different values for higher min_samples_split. \n",
    "# It might not be the best idea always but here if you observe the output closely, \n",
    "# max_depth of 7 works better in most of the cases. \n",
    "# Also, we can test for 5 values of min_samples_leaf, from 30 to 70 in steps of 10, \n",
    "# along with higher min_samples_split.\n",
    "\n",
    "param_test3 = {'min_samples_split':range(100, 201, 20), 'min_samples_leaf':range(25,201,25)}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=30,\n",
    "                                                               max_depth=6,max_features='sqrt', subsample=0.8, \n",
    "                                                               random_state=10), \n",
    "                        param_grid = param_test3, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "gsearch3.fit(X_train.drop('globalTimesEventWeight', axis=1, inplace=False), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we get the optimum values as 100 for min_samples_split and 25 for min_samples_leaf. \n",
    "# Also, we can see the CV score increasing to 0.73264671016122429 now.\n",
    "# Let's fit the model again on this and have a look at the feature importance.\n",
    "\n",
    "modelfit(gsearch3.best_estimator_,  y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_test4 = {'max_features':range(2, 12, 1)}\n",
    "gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=30,max_depth=6, \n",
    "                                                               min_samples_split=100, min_samples_leaf=25, \n",
    "                                                               subsample=0.8, random_state=10),\n",
    "                        param_grid = param_test4, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "gsearch4.fit(X_train.drop('globalTimesEventWeight', axis=1, inplace=False), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tuning subsample and making models with lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The next step would be try different subsample values. Lets take values 0.6,0.7,0.75,0.8,0.85,0.9.\n",
    "\n",
    "param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n",
    "gsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=30, max_depth=6,\n",
    "                                                               min_samples_split=100, min_samples_leaf=25, \n",
    "                                                               subsample=0.8, random_state=10, max_features=3),\n",
    "param_grid = param_test5, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "gsearch5.fit(X_train.drop('globalTimesEventWeight', axis=1, inplace=False), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Lets decrease the learning rate to half, i.e. 0.05 with twice (30) the number of trees. \n",
    "\n",
    "gbm_tuned_1 = GradientBoostingClassifier(learning_rate=0.05, n_estimators=60,\n",
    "                                         max_depth=6, min_samples_split=100,\n",
    "                                         min_samples_leaf=25, subsample=0.8, \n",
    "                                         random_state=10, max_features=3)\n",
    "modelfit(gbm_tuned_1, y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Now lets reduce to one-tenth of the original value, i.e. 0.01 for 30 trees.\n",
    "\n",
    "gbm_tuned_2 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=30,\n",
    "                                         max_depth=6, min_samples_split=100,\n",
    "                                         min_samples_leaf=25, subsample=0.80, \n",
    "                                         random_state=10, max_features=3)\n",
    "\n",
    "modelfit(gbm_tuned_2,  y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Lets decrease to one-twentieth of the original value, i.e. 0.005 for 1200 trees.\n",
    "\n",
    "gbm_tuned_3 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=60,\n",
    "                                         max_depth=6, min_samples_split=100, \n",
    "                                         min_samples_leaf=25, subsample=0.80, \n",
    "                                         random_state=10, max_features=3,\n",
    "                                         warm_start=True)\n",
    "\n",
    "modelfit(gbm_tuned_3, y_train, X_train, performCV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Here we see that the score reduced very slightly. So lets run for 1500 trees.\n",
    "\n",
    "gbm_tuned_4 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=100,\n",
    "                                         max_depth=6, min_samples_split=100, \n",
    "                                         min_samples_leaf=25, subsample=0.80, \n",
    "                                         random_state=10, max_features=3,\n",
    "                                         warm_start=True)\n",
    "modelfit(gbm_tuned_4,  y_train, X_train, performCV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
